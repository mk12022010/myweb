[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Teaching\n\n\n\nCourses\nResearch Tools\nOffice Programs\n\n\n\n\n\nResearch methodology\nBasic statistics using R\nIntermediate statistics using R\nAdvanced statistics using R\nMachine learning using R\nData analysis using Python/SPSS/Minitab\n\n\nData collection tools and techniques\nTechniques for data processing\nOptimization of data analysis techniques\nUse of AI in research\n\n\nOffice program Use of MS Word\nUse of MS Excel\nUse of MS PowerPoint"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RUEN Research",
    "section": "",
    "text": "This platform is initiated by Professor Dr. Md. Kamrul Hasan. At the moment, Professor Hasan holds a full time position as an academic employee in the Department of Agricultural Extension and Rural Development, Faculty of Agriculture, Patuakhali Science and Technology University, Dumki, Patuakhali 8602, Bangladesh.\nAboutBiographyPublicationsResearch Supervision\n\n\nProfessor Hasan has a PhD in Ecosystem Management from the School of Environmental and Rural Science, University of New England, Australia. After graduation from Bangladesh Agricultural University in Bachelor of Science in Agriculture and Master of Science in Agricultural Extension Education, he completed an International Master of Science in Rural Development (IMRD) from Bioscience Engineering in Ghent University, Belgium. He studied human-environment interactions and climate science in Humboldt University, Germany followed by a case study in Slovakia to understand the community-led development activities under common agricultural policy of European Union. Economics, Econometrics, Advanced Statistics, and Policy Analysis were some of the major focus of his IMRD program.\n\nDetails at PSTU Website\n\n\nDr. Md. Kamrul Hasan is a professor of Agricultural Extension and Rural Development. His early childhood started in a rural area (Telipara village, Bashtoil, Mirzapur, Tangail). Spending time with friends was remarkable during his study period in Pekua Govt. Primary School (1987-1991) and Bashtoil M. M. High School (1992-1997). As a carrom and chess lover he enjoyed playing with diversified opponents and has earned a number of champion’s trophies. Swimming and cycling hobby also makes him nostalgic as he was used to cross rivers by swimming and visit distant tourist attractions with cyclists. Since his HSC (Science group, 1999) at Govt. M. M. Ali College (Tangail), he was almost away from his family that supported his educational expenses until B.Sc.Ag. (Hons.) at Bangladesh Agricultural University (BAU, 2005).\nStrong mathematical problem-solving capacity opened his way to become a big fan of statistics subject. He started using PC for academic purposes in 2003 and became expert in Microsoft Office programs, operating systems, networking, video and audio editing and statistical software (MSTAT, SPSS) while completing his Master of Science (MS) degree in Agricultural Extension Education (BAU, 2006). Further improvement in using statistical and programming language (R, GAMS, SAS) occurred during his International Master of Science in Rural Development (IMRD, Ghent University, Humboldt University and Slovak University of Agriculture) in Belgium, Germany and Slovakia. Application of machine learning algorithm and econometric analysis using Python and R programming language along with the use of GIS software was flourished in Australia during his PhD in Ecosystem Management (School of Environmental and Rural Science) at the University of New England.\nAs a graduate in agricultural sciences, Professor Hasan studied a wide range of subjects related to production, protection, extension, marketing and policy. Environmental Science and Development Economics attracted him while he was in Europe studying under Erasmus Mundus program. His research focused on Climate change, food security and farming systems. He is supervising Master’s and PhD students related to such fields of interests. Besides, research his taught courses include Research Methodology, Training Management, Communication, Learning Theories and Methods, Leadership and Groups, Gender Development, and Technology Transfer.\nCurrently, Professor Hasan is the Chairman of Agricultural Extension and Rural Development and also worked as the Director of Institutional Quality Assurance Cell (IQAC) of Patuakhali Science and Technology University. As a part of this job, he organized and delivered training on quality related topics, e.g., Curriculum Development, National Integrity System, Right to Information, Grievance Redress, Citizen Charter, Public Procurement, and Pedagogy (teaching-tearning, active learning, blended learning). He also acts as an active team member for organizing regular extension field tours for undergraduate students.\n\n\n\nHasan, M.K. and Kumar, L. (2024). Determining adequate sample size for social survey research. Journal of Bangladesh Agricultural University, 22(2), 146-157. https://doi.org/10.3329/jbau.v22i2.74547\nMaiti, A., Hasan, M.K., Sannigrahi, S., et al. (2024). Optimal rainfall threshold for monsoon rice production in India varies across space and time. Communications Earth & Environment, 5, 302. https://doi.org/10.1038/s43247-024-01414-7\nSiddik, M. A., Hasan, M. K., Islam, A. R. M. T., & Islam, M. S. (2023). Coastal Community Valorization through Patuakhali Science and Technology University: Policy Support and Way Forwards. Journal of Planning Education and Research. https://doi.org/10.1177/0739456X231195620\nHasan, M. K., and Kumar, L. (2022). Changes in coastal farming systems in a changing climate. Regional Environmental Change, 2(4). https://doi.org/10.1007/s10113-022-01962-8\nKogo, B. K., Kumar, L., Koech, R., and Hasan, M. K. (2022). Response to climate change in a rain-fed crop production system: Insights from maize farmers of western Kenya. Mitigation and Adaptation Strategies for Global Change, 27(8). https://doi.org/10.1007/s11027-022-10023-8\nKogo, B. K., Kumar, L., Koech, R., & Hasan, K. (2021). Climatic and non-climatic risks in rainfed crop production systems: insights from maize farmers of western Kenya. Climate and Development, 13(10), 869–878. https://doi.org/10.1080/17565529.2020.1867043\nHasan, M. K., & Kumar, L. (2021). Yield trends and variabilities explained by climatic change in coastal and non-coastal areas of Bangladesh. Science of the Total Environment, 795, 148814. https://doi.org/10.1016/j.scitotenv.2021.148814\nHasan, M. K., & Kumar, L. (2020). Discriminated perceptions of climatic impacts on coastal farm management practices. Journal of Environmental Management, 278, 111550. https://doi.org/10.1016/j.jenvman.2020.111550\nGopalakrishnan, T., Kumar, L., & Hasan, M. K. (2020). Coastal settlement patterns and exposure to sea-level rise in the Jaffna Peninsula, Sri Lanka. Population and Environment, 42, 129-145. https://doi.org/10.1007/s11111-020-00350-w\nHasan, M. K., & Kumar, L. (2020). Perceived farm-level climatic impacts on coastal agricultural productivity in Bangladesh. Climatic Change, 161, 617-636. https://doi.org/10.1007/s10584-020-02708-3\nJayasinghe, S. L., Kumar, L., & Hasan, M. K. (2020). Relationship between Environmental Covariates and Ceylon Tea Cultivation in Sri Lanka. Agronomy, 10(4), 476. https://doi.org/10.3390/agronomy10040476\nHasan, M. K., & Kumar, L. (2020). Meteorological data and farmers’ perception of coastal climate in Bangladesh. Science of the Total Environment, 704, 135384. https://doi.org/10.1016/j.scitotenv.2019.135384\nChhogyel, N., Kumar, L., Bajgai, Y., & Hasan, M. K. (2020). Perception of farmers on climate change and its impacts on agriculture across various altitudinal zones of Bhutan Himalayas. International Journal of Environmental Science and Technology, 17, 3607-3620. https://doi.org/10.1007/s13762-020-02662-8\nHasan, M. K., Kumar, L., & Gopalakrishnan, T. (2020). Inundation modelling for Bangladeshi coasts using downscaled and bias-corrected temperature. Climate Risk Management, 27, 100207. https://doi.org/https://doi.org/10.1016/j.crm.2019.100207\nGopalakrishnan, T., Hasan, M. K., Haque, A. T. M. S., Jayasinghe, S. L., & Kumar, L. (2019). Sustainability of coastal agriculture under climate change. Sustainability, 11(24), 7200. https://doi.org/10.3390/su11247200\nHasan, M. K., and Kumar, L. (2019). Comparison between meteorological data and farmer perceptions of climate change and vulnerability in relation to adaptation. Journal of Environmental Management, 237, 54-62. https://doi.org/10.1016/j.jenvman.2019.02.028\nHasan, M. K., Desiere, S., D’Haese, M., & Kumar, L. (2018). Impact of climate-smart agriculture adoption on the food security of coastal farmers in Bangladesh. Food Security, 10, 1073-1088. https://doi.org/10.1007/s12571-018-0824-1\n\n\n\n\nOngoingCompleted"
  },
  {
    "objectID": "index.html#welcome-to-ruen-research",
    "href": "index.html#welcome-to-ruen-research",
    "title": "RUEN Research",
    "section": "",
    "text": "This platform is initiated by Professor Dr. Md. Kamrul Hasan. At the moment, Professor Hasan holds a full time position as an academic employee in the Department of Agricultural Extension and Rural Development, Faculty of Agriculture, Patuakhali Science and Technology University, Dumki, Patuakhali 8602, Bangladesh.\nAboutBiographyPublicationsResearch Supervision\n\n\nProfessor Hasan has a PhD in Ecosystem Management from the School of Environmental and Rural Science, University of New England, Australia. After graduation from Bangladesh Agricultural University in Bachelor of Science in Agriculture and Master of Science in Agricultural Extension Education, he completed an International Master of Science in Rural Development (IMRD) from Bioscience Engineering in Ghent University, Belgium. He studied human-environment interactions and climate science in Humboldt University, Germany followed by a case study in Slovakia to understand the community-led development activities under common agricultural policy of European Union. Economics, Econometrics, Advanced Statistics, and Policy Analysis were some of the major focus of his IMRD program.\n\nDetails at PSTU Website\n\n\nDr. Md. Kamrul Hasan is a professor of Agricultural Extension and Rural Development. His early childhood started in a rural area (Telipara village, Bashtoil, Mirzapur, Tangail). Spending time with friends was remarkable during his study period in Pekua Govt. Primary School (1987-1991) and Bashtoil M. M. High School (1992-1997). As a carrom and chess lover he enjoyed playing with diversified opponents and has earned a number of champion’s trophies. Swimming and cycling hobby also makes him nostalgic as he was used to cross rivers by swimming and visit distant tourist attractions with cyclists. Since his HSC (Science group, 1999) at Govt. M. M. Ali College (Tangail), he was almost away from his family that supported his educational expenses until B.Sc.Ag. (Hons.) at Bangladesh Agricultural University (BAU, 2005).\nStrong mathematical problem-solving capacity opened his way to become a big fan of statistics subject. He started using PC for academic purposes in 2003 and became expert in Microsoft Office programs, operating systems, networking, video and audio editing and statistical software (MSTAT, SPSS) while completing his Master of Science (MS) degree in Agricultural Extension Education (BAU, 2006). Further improvement in using statistical and programming language (R, GAMS, SAS) occurred during his International Master of Science in Rural Development (IMRD, Ghent University, Humboldt University and Slovak University of Agriculture) in Belgium, Germany and Slovakia. Application of machine learning algorithm and econometric analysis using Python and R programming language along with the use of GIS software was flourished in Australia during his PhD in Ecosystem Management (School of Environmental and Rural Science) at the University of New England.\nAs a graduate in agricultural sciences, Professor Hasan studied a wide range of subjects related to production, protection, extension, marketing and policy. Environmental Science and Development Economics attracted him while he was in Europe studying under Erasmus Mundus program. His research focused on Climate change, food security and farming systems. He is supervising Master’s and PhD students related to such fields of interests. Besides, research his taught courses include Research Methodology, Training Management, Communication, Learning Theories and Methods, Leadership and Groups, Gender Development, and Technology Transfer.\nCurrently, Professor Hasan is the Chairman of Agricultural Extension and Rural Development and also worked as the Director of Institutional Quality Assurance Cell (IQAC) of Patuakhali Science and Technology University. As a part of this job, he organized and delivered training on quality related topics, e.g., Curriculum Development, National Integrity System, Right to Information, Grievance Redress, Citizen Charter, Public Procurement, and Pedagogy (teaching-tearning, active learning, blended learning). He also acts as an active team member for organizing regular extension field tours for undergraduate students.\n\n\n\nHasan, M.K. and Kumar, L. (2024). Determining adequate sample size for social survey research. Journal of Bangladesh Agricultural University, 22(2), 146-157. https://doi.org/10.3329/jbau.v22i2.74547\nMaiti, A., Hasan, M.K., Sannigrahi, S., et al. (2024). Optimal rainfall threshold for monsoon rice production in India varies across space and time. Communications Earth & Environment, 5, 302. https://doi.org/10.1038/s43247-024-01414-7\nSiddik, M. A., Hasan, M. K., Islam, A. R. M. T., & Islam, M. S. (2023). Coastal Community Valorization through Patuakhali Science and Technology University: Policy Support and Way Forwards. Journal of Planning Education and Research. https://doi.org/10.1177/0739456X231195620\nHasan, M. K., and Kumar, L. (2022). Changes in coastal farming systems in a changing climate. Regional Environmental Change, 2(4). https://doi.org/10.1007/s10113-022-01962-8\nKogo, B. K., Kumar, L., Koech, R., and Hasan, M. K. (2022). Response to climate change in a rain-fed crop production system: Insights from maize farmers of western Kenya. Mitigation and Adaptation Strategies for Global Change, 27(8). https://doi.org/10.1007/s11027-022-10023-8\nKogo, B. K., Kumar, L., Koech, R., & Hasan, K. (2021). Climatic and non-climatic risks in rainfed crop production systems: insights from maize farmers of western Kenya. Climate and Development, 13(10), 869–878. https://doi.org/10.1080/17565529.2020.1867043\nHasan, M. K., & Kumar, L. (2021). Yield trends and variabilities explained by climatic change in coastal and non-coastal areas of Bangladesh. Science of the Total Environment, 795, 148814. https://doi.org/10.1016/j.scitotenv.2021.148814\nHasan, M. K., & Kumar, L. (2020). Discriminated perceptions of climatic impacts on coastal farm management practices. Journal of Environmental Management, 278, 111550. https://doi.org/10.1016/j.jenvman.2020.111550\nGopalakrishnan, T., Kumar, L., & Hasan, M. K. (2020). Coastal settlement patterns and exposure to sea-level rise in the Jaffna Peninsula, Sri Lanka. Population and Environment, 42, 129-145. https://doi.org/10.1007/s11111-020-00350-w\nHasan, M. K., & Kumar, L. (2020). Perceived farm-level climatic impacts on coastal agricultural productivity in Bangladesh. Climatic Change, 161, 617-636. https://doi.org/10.1007/s10584-020-02708-3\nJayasinghe, S. L., Kumar, L., & Hasan, M. K. (2020). Relationship between Environmental Covariates and Ceylon Tea Cultivation in Sri Lanka. Agronomy, 10(4), 476. https://doi.org/10.3390/agronomy10040476\nHasan, M. K., & Kumar, L. (2020). Meteorological data and farmers’ perception of coastal climate in Bangladesh. Science of the Total Environment, 704, 135384. https://doi.org/10.1016/j.scitotenv.2019.135384\nChhogyel, N., Kumar, L., Bajgai, Y., & Hasan, M. K. (2020). Perception of farmers on climate change and its impacts on agriculture across various altitudinal zones of Bhutan Himalayas. International Journal of Environmental Science and Technology, 17, 3607-3620. https://doi.org/10.1007/s13762-020-02662-8\nHasan, M. K., Kumar, L., & Gopalakrishnan, T. (2020). Inundation modelling for Bangladeshi coasts using downscaled and bias-corrected temperature. Climate Risk Management, 27, 100207. https://doi.org/https://doi.org/10.1016/j.crm.2019.100207\nGopalakrishnan, T., Hasan, M. K., Haque, A. T. M. S., Jayasinghe, S. L., & Kumar, L. (2019). Sustainability of coastal agriculture under climate change. Sustainability, 11(24), 7200. https://doi.org/10.3390/su11247200\nHasan, M. K., and Kumar, L. (2019). Comparison between meteorological data and farmer perceptions of climate change and vulnerability in relation to adaptation. Journal of Environmental Management, 237, 54-62. https://doi.org/10.1016/j.jenvman.2019.02.028\nHasan, M. K., Desiere, S., D’Haese, M., & Kumar, L. (2018). Impact of climate-smart agriculture adoption on the food security of coastal farmers in Bangladesh. Food Security, 10, 1073-1088. https://doi.org/10.1007/s12571-018-0824-1\n\n\n\n\nOngoingCompleted"
  },
  {
    "objectID": "courses/research-methodology.html",
    "href": "courses/research-methodology.html",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "To ensure the validity and reproducibility of any research work, all actions must be justified with appropriate reasoning, for which Research Methodology serves as the only acceptable foundation(M. K. Hasan).\n\n\n\n\n\n\n\n\n\n\nResearch methodology &gt;\n&gt; Research method &gt;\n&gt; Research design &gt;\n&gt; Experimental design\n\n\nResearch methodology deals with how to conduct a scientific study. As an academic course, it discusses all the theories, methods and procedures that are required to carry out a research work.\nResearch methods typically include study location, population, sampling, research design, experimental design, variables, data collection tools, and analytical procedures.\nResearch approach (qualitative, quantitative, mixed) that also includes experimental design.\nThe intended settings of experimental units, treatment application, randomization, data collection period.\n\n\nThis provides necessary explanations of choosing a particular location and methods for a specific research work.\nResearch method is a part of research methodology.\nResearch design is a part of research methods.\nExperimental design is a part of research design.\n\n\n\n\n\n\nKnowledge is the stored information in our brain, books or internet that can be recalled when needed. If we cannot recall or retrieve the information that is not considered as knowledge. Knowledge can be gathered randomly from anywhere and by any means. However, if the knowledge is generated using systematic process (problem identification, data collection, analysis and interpretation) then this systematic knowledge is called science. Science is seen through its invention (device - pen, idea - planting date, or practice - row planting), which is called technology. When a technology is perceived as being new to an individual, the technology is termed as innovation (new technology).\nResearch is the process of generating the knowledge, i.e. the systematic process by which knowledge or technology is generated, in other words, a problem is solved. This process has to be reproducible. It is a systematic progress from the known to the unknown. Research is a crucial tool for making adjustment to the ever changing situation of our universe. Research can be fundamental/basic, applied, and action research depending on their nature.\n\nFundamental or basic research: This type of research involves experimental and theoretical work aimed at gaining new knowledge without any immediate practical applications, focusing solely on advancing understanding.\nApplied research: This research aims to address practical problems, with the goal of improving products or processes, rather than simply gaining knowledge for its own sake.\nAction research: This methodology combines action and research to explore specific questions, issues, or phenomena through observation, reflection, and intentional interventions, with a focus on immediate application in a local context. This is more participatory than applied research.\nBehavioral research: Behavioral research is a scientific field that investigates the actions and interactions of individuals and groups. It seeks to understand, describe, and predict behavior through systematic observation, experimentation, and analysis. This type of research encompasses a wide range of activities, including studying how people think, feel, and act in various situations, and examining the underlying psychological, social, and environmental factors influencing these behaviors.\n\nBehavioral research employs various methodologies, such as experiments, surveys, observations, and case studies. Experimental methods involve manipulating variables to observe their effects on behavior, often in controlled settings, to establish cause-and-effect relationships. Surveys and questionnaires gather data on attitudes, beliefs, and behaviors from large groups, providing insights into trends and correlations. Observational studies involve systematically recording behaviors in natural or laboratory settings without interference, while case studies offer in-depth analyses of individual or group behaviors over time.\nApplications of behavioral research are vast, spanning fields like psychology, sociology, education, healthcare, marketing, and public policy. Insights gained from behavioral research inform interventions, treatments, and strategies to address social issues, improve mental health, enhance educational outcomes, and optimize organizational performance. Ultimately, behavioral research contributes to a deeper understanding of human behavior, helping to create more effective solutions for individual and societal challenges.\nSome other types of research can be mentioned.\n\nHistorical research: This type of research aims to describe past events to uncover generalizations that aid in understanding both the past and present and, to some extent, predicting the future.\nDescriptive research: This research focuses on describing the current state of affairs to identify relationships between variables that are not manipulated.\nExperimental research: This research investigates what will happen when specific variables are carefully controlled or manipulated to determine relationships between these manipulated variables and other variables.\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nData\nRaw, unprocessed observations (numbers, text, etc.).\n“Temperature readings: 25°C, 30°C, 22°C.”\n\n\nInformation\nProcessed data with context or meaning.\n“Average temperature this week: 26°C.”\n\n\nFact\nAn objective, verifiable observation.\n“Water boils at 100°C at sea level.”\n\n\nHypothesis\nA testable prediction about a phenomenon.\n“Increasing light exposure speeds up plant growth.”\n\n\nTheory\nA well-substantiated explanation of natural phenomena.\n“Darwin’s theory of evolution by natural selection.”\n\n\nLaw\nA consistent, universal description of how (not why) phenomena behave.\n“Newton’s law of universal gravitation.”\n\n\n\n\n\n\nData → Information: Adding meaning turns data into information.\n\nHypothesis → Theory: A hypothesis becomes a theory after extensive evidence.\n\nTheory vs. Law: Theories explain why; laws describe what happens (e.g., gravity law describes attraction; general relativity theory explains it).\n\nA fact is a statement that can be proven true or false based on empirical evidence. It is objective and verifiable, often derived from observation or measurement. Facts are not always necessarily true. While facts are statements that can be proven true or false, they are considered true based on the best available evidence at the time. However, new evidence or better understanding can sometimes change what is considered a fact. For example, it was once considered a fact that the Earth was the center of the universe, but this was later disproved by scientific advancements. Therefore, facts are true as long as they are supported by current evidence, but they can be revised in light of new information. A theory, on the other hand, is a well-substantiated explanation of some aspect of the natural world, based on a body of evidence and reasoning. Theories are broader in scope than facts and can provide a framework for understanding and predicting phenomena. For example, the theory of evolution explains the diversity of life based on evidence from various scientific fields.\nA theory is formed based on observation through a systematic process. Initially, observations and empirical data are gathered from experiments or natural occurrences. Scientists then analyze this data to identify patterns, relationships, and regularities. Hypotheses are proposed as tentative explanations for these observations. These hypotheses are rigorously tested through further experimentation and data collection. If a hypothesis consistently explains the observations and withstands repeated testing and scrutiny, it may be integrated into a broader theoretical framework. Over time, as more evidence accumulates and the theory proves robust, it becomes widely accepted as a reliable explanation of the observed phenomena.\n\n\n\n\nA research problem is a specific issue, difficulty, contradiction, or gap in knowledge that a researcher aims to address through systematic investigation. It is the foundation of any research project, guiding the direction of study and focusing efforts on finding solutions or answers. In essence, a research problem sets the stage for the entire research process by outlining what needs to be investigated, why it is significant, and how it can be explored.\nIdentifying a research problem is a critical step in the research process. It requires careful observation, comprehensive literature review, and engagement with the field’s practical and theoretical aspects. In agricultural extension, addressing the right research problem can lead to significant improvements in how knowledge and technologies are disseminated to farmers, ultimately contributing to enhanced agricultural productivity and rural development. By focusing on real-world issues and gaps in existing knowledge, researchers can ensure that their work is relevant, impactful, and valuable to stakeholders.\nA research problem should not be too vague (e.g. effect of climate change on agriculture), not too narrow (e.g. name of the first president of the Malaysia.). It has be new with novelty that has not been done before but consistent with previous research.\n\n\n\nGuides the Research Process: A clearly defined research problem provides a clear direction for the research process. It helps in formulating research questions, hypotheses, and objectives, ensuring that the study remains focused and relevant.\nDetermines Research Design: The nature of the research problem influences the choice of research design, methods, and procedures. It dictates whether the study will be qualitative, quantitative, or mixed-methods.\nEnsures Relevance: A well-articulated research problem ensures that the research addresses real-world issues or gaps in knowledge, making the findings valuable to stakeholders, practitioners, and policymakers.\nFacilitates Resource Allocation: By identifying a specific problem, researchers can better allocate time, funding, and other resources to areas that require attention, optimizing the efficiency and impact of the study.\nEnhances Research Quality: A precise research problem helps in maintaining the study’s rigor and coherence, contributing to the validity and reliability of the research findings.\n\n\n\n\n\nLiterature Review: Conducting a thorough review of existing literature helps in identifying gaps, inconsistencies, and unresolved questions in the current body of knowledge. This can highlight areas that require further investigation.\nPractical Observations: Real-world observations and experiences can reveal pressing issues or challenges that need to be addressed. Engaging with practitioners, stakeholders, and communities can provide insights into practical problems.\nExpert Consultation: Discussing with experts, mentors, and colleagues in the field can help in identifying significant research problems. Their experience and perspective can shed light on critical areas that may not be immediately apparent.\nPrevious Research: Analyzing findings and recommendations from previous research studies can reveal new problems or questions that have emerged as a result of earlier work.\nPolicy Analysis: Reviewing policies, regulations, and strategic plans can uncover gaps or areas needing evaluation, particularly in applied fields like agricultural extension.\n\n\n\n\n\nContext: Agricultural extension involves the dissemination of knowledge and technologies to farmers to improve their productivity, income, and livelihoods. In Bangladesh, agricultural extension services are crucial for enhancing food security and rural development.\nObservation: Despite extensive efforts, many farmers in Bangladesh are not adopting modern agricultural practices and technologies. This results in lower productivity and inefficiencies in the agricultural sector.\nLiterature Review: A review of literature reveals that while several studies have focused on the development and efficacy of extension methods, there is limited research on the barriers to adoption from the farmers’ perspective.\nIdentified Research Problem: “What are the key barriers to the adoption of modern agricultural practices among smallholder farmers in Bangladesh, and how can agricultural extension services be improved to address these barriers?”\nImportance: Understanding these barriers is crucial for designing effective extension programs that are responsive to the needs and constraints of farmers. Addressing this problem can lead to increased adoption of beneficial practices, ultimately enhancing agricultural productivity and sustainability\n\n\n\n\n\nIdentify Potential Problems: Compile a comprehensive list of potential research problems based on literature reviews, expert consultations, observations, and stakeholder input.\nDefine Criteria for Prioritization: Establish criteria to evaluate and rank the identified research problems. Common criteria include relevance, feasibility, urgency, potential impact, and alignment with strategic national and international goals.\nEvaluate Each Problem: Assess each research problem against the defined criteria. This can be done through qualitative assessment, scoring, or ranking methods.\nEngage Stakeholders: Involve key stakeholders, including practitioners, policymakers, funders, and the target population, in the prioritization process to ensure the selected problems are relevant and impactful.\nAnalyze and Decide: Analyze the evaluation results and make decisions on which research problems to prioritize. This might involve selecting the top-ranked problems or those that meet multiple criteria strongly.\nReview and Adjust: Periodically review the prioritization process and adjust based on new information, changing contexts, or emerging issues.\n\n\n\n\n\nA research proposal is a detailed plan outlining a proposed study, aiming to communicate the significance, methodology, and anticipated outcomes of the research. All the components and methods of a research work chosen by the researcher must have acceptable reasons based on literature and appropriate arguments. Here are the key components of a research proposal:\n\nTitle: The title provides a concise summary of the research topic. It should be clear, specific, and informative. It includes what (purpose), who (scope) and sometimes how. It is better to keep around 10-15 words (not exceeding 16 words). You can watch this video to create a smart title https://www.youtube.com/watch?v=LdiJ8qUclQE. You can also take help from ChatGPT for phrasing a smart title.\nAbstract: The abstract summarizes the key elements of the proposal. It is a brief overview of the research problem, objectives, methodology, and expected outcomes. In a journal article, it is composed of usually 200-300 words. However, in a thesis it can be extended as required and suggested by the supervisors.\nIntroduction: The introduction introduces the research topic and establishes the context. It contains background information, statement of the problem, significance of the study, and research questions.\nLiterature Review: This section reviews existing research and theories related to the topic. It summarizes the relevant studies, identification of gaps in the current knowledge, and justification for the proposed research. A conceptual framework can be drawn based on the facts and theories to visualize the components and connections between various variables in a study. In a journal article, this section is integrated in the introduction section.\nResearch Objectives: This section clearly defines what the study aims to achieve. An objective should be specific, measurable, achievable, relevant, and time-bound (SMART). Objectives are written usually using ‘to + verb’. This section is also merged with the introduction section in a scientific report and article.\nResearch Hypotheses: A hypothesis is an intellectual guess based on literature and context that a researcher wants to evaluate and test. This section specifies the clear and focused hypotheses to be tested. Example: training has a positive effect on food security.\nResearch methods: This section describes the research design and methods to be used. It contains:\n\nLocation: Study location where the study will take place.\nResearch Design: Qualitative, quantitative, or mixed methods.\nPopulation and Sample: Description of the study population, sampling methods, and sample size.\nData Collection Methods: Techniques for gathering data (e.g., surveys, interviews, observations).\nData Analysis: Methods for analyzing the collected data.\nEthical Considerations: Ethical issues and how they will be addressed. For example, informed consent and application of treatments ethically.\n\nWork Plan and Timeline: It provides a time schedule for the research activities and key milestones. It is often presented as a Gantt chart. It is a non-financial budget that allocates time for different activities.\nBudget: It outlines the financial requirements of the study. Itemized budget including costs for personnel, equipment, materials, travel, and other expenses.\nExpected Outcomes: This section describes the anticipated results and their implications. It contains potential findings, contributions to knowledge, and practical applications.\nReferences: The references include a list of all the sources cited in the proposal. It contains properly formatted bibliography following a standard citation style, e.g. APA, MLA, and EEE. The citations and references must match with each other.\nAppendices (not always required): this section provides supplementary information and additional materials such as questionnaires, consent forms, detailed methodology, or supporting documents.\n\n\n\nDeveloping a conceptual framework from a literature review involves identifying key concepts, understanding their relationships, formulating hypotheses or research questions, and visualizing these elements in a coherent structure. This process helps in organizing and guiding the research, ensuring that it builds on existing knowledge and addresses relevant gaps. In behavioral research, such as agricultural extension, a well-developed conceptual framework can illuminate the factors influencing behaviors and provide a clear pathway for investigation. Developing a conceptual framework from a literature review for a behavioral research work involves several key steps. The conceptual framework helps in organizing and visualizing the key concepts, variables, and their relationships that will be studied. Here’s a step-by-step guide to developing a conceptual framework:\n1. Identify Key Concepts from the Literature Review:\n\nRead and analyze a wide range of studies related to your research topic.\nIdentify recurring themes, variables, and constructs that are important in the field.\nExtract relevant concepts, theories, and models from the existing literature.\n\n2. Define Relationships Between Concepts from the Literature Review:\n\nLook for empirical studies that examine relationships between the identified concepts.\nNote the direction and nature (positive, negative, causal, correlational) of these relationships.\nUnderstand how different concepts are related based on previous research.\n\n3. Develop Hypotheses or Research Questions based on Literature Insights:\n\nUse insights from the literature to develop testable hypotheses or focused research questions.\nFormulate specific hypotheses or research questions that your study will address.\nEnsure that these are logically derived from the identified relationships.\nHypotheses based on literature might include:\n\nH1: Higher access to agricultural information positively influences the adoption of modern practices.\nH2: Perceived risks negatively affect the adoption of modern agricultural practices.\nH3: Socio-economic factors such as income and education level significantly impact the adoption behavior.\n\n\n4. Organize the Concepts and Relationships Visually:\n\nDraw a diagram or flowchart that includes all key concepts (variables) and illustrates the hypothesized relationships among them.\nUse arrows to indicate the direction of relationships and labels to describe the nature of these relationships.\nVisualize the structure of the concepts and their relationships.\n\n5. Refine the Framework:\n\nReview and refine the framework to ensure all relevant concepts and relationships are included.\nSimplify where possible to make the framework easy to understand and interpret.\nEnsure the conceptual framework is clear, coherent, and comprehensive.\n\n\n\n\n\nResearch design means the strategy which is planned to answer the research questions. It essentially has the theories and models related to the research questions, sampling frame, data collection methods and analytical approaches. There are two broad types of research design or approaches: quantitative and qualitative. Although research approach is a broader term, still research design and approach are used interchangeably. There are two philosophical approaches of research – inductive and deductive. Inductive approach formulates theory from observations, and deductive approach test hypothesis based on observations using the theoretical framework.\nMixed method research answers research questions by combining both quantitative and qualitative research methods where one complements another. This a triangulation of different methods to clearly understand a research problem. The information revealed by quantitative method can also be validated using the information obtained using qualitative method.\nDifferences between quantitative and qualitative research design/approach\n\n\n\n\n\n\n\n\nAspect\nQuantitative\nQualitative\n\n\nObjective\nQuantify data, generalize results to a population\nUnderstand concepts, thoughts, or experience\n\n\nData Collection Methods\nSurveys, experiments, structured observational studies\nInterviews, focus groups, participant observation, document analysis\n\n\nData Type\nNumerical data\nNon-numerical data (text, audio, video)\n\n\nNature of Data\nStructured, quantifiable\nUnstructured or semi-structured, not easily quantifiable\n\n\nAnalysis\nStatistical analysis (e.g., regression, ANOVA)\nThematic, content, narrative analysis, coding\n\n\nGeneralizability\nHigh, applicable to larger populations\nLow, specific to particular contexts\n\n\nDepth vs. Breadth\nBreadth across large number of cases\nDepth within a smaller number of cases\n\n\nApproach\nDeductive, testing hypotheses\nInductive, generating hypotheses\n\n\nExamples\nMeasuring effect of a teaching method on performance\nExploring experiences of farmers with disasters\n\n\n\n\n\n\nExperimental design is a part of a research design. In order to establish a cause-and-effect relationship, manipulation of variable(s) is done with specific settings of sample(s) or experimental units. Such specific settings or approaches are called research design. Research design is the blue-print of research. It helps to control variance and minimize errors in addition to the accurate conduction of study, efficient allocation of resources and well defined reproducible work procedures. It is required for yielding maximal reliable and valid information with minimal efforts. It includes hypothesis formulation, selecting samples, random assignment of treatments, replications and local control. Research design is done with utmost care in advance since any error in it can ruin the entire research efforts.\nEssential Components of Experimental Designs\nA good experimental design maximizes reliability of information and minimize experimental errors, which is ensured by the following three components:\n\nRandomization: Treatments must be assigned randomly to experimental units. Any bias can result in variance in result that can falsely be attributed to the treatments.\nReplication: To improve accuracy and minimize error, identical treatments are applied to more than one experimental units. Thus, the number of experimental units with the identical treatments are called replication.\nLocal control: A researcher must have a high level of control over the variables. To attribute the variance truly to the treatments, factors and conditions other than the treatments must be kept homogeneous for all experimental units. This process of keeping homogeneity among the experimental units is called local control. If a researcher fails to maintain homogeneity, the effect of replication will be significant and they cannot claim that the resulted variance in the dependent variable is due to the effect of the treatments.\n\nTypes of experimental design in social research\n\nPre-experimental or non-experimental design: Randomization is not practical to be used in this case. This design can take different forms, some of which are explained here.\n\nOne group post-test design: Data is collected after certain time of applying treatments. In this design, no pretest/benchmark/initial data is available, and therefore, exact effect of the treatment is not possible.\nOne group pretest-post-test design: Data is collected twice – firstly before applying the treatment and secondly after certain period of treatment application. Effect is measured by deducting the initial quantity from the final quantity.\nStatic group comparison (Control group post-test) design: Data is collected after certain time of applying treatments from the treatment groups. Data is collected from another control group that has not received the treatment. Two groups should be as similar as possible to make them comparable. However, the groups should also be as distant as possible to avoid spill-over/triple effect of the treatment.\n\nTrue experimental design: Randomization is properly done and at least a control group is included.\n\nPost-test control group design (Randomized control trial – RCT): Participants are randomly selected from control and treatment groups. Observations are made after applying the treatments.\nPretest-post-test control group design: Similar to the RCT design, participants are randomly assigned treatments, but data is collected before the treatment application and after certain period of the treatment application.\n\nQuasi-experimental design: Randomization is not possible, such as in psychological study. For example, we cannot assign a school going kid in the drop out group.\n\nNon-equivalent control group design: Similar to the pretest-post-test control group design, it has both control and treatment groups but without randomization. It also similar to the static group comparison design because both have control groups and do not have randomization, but different because the static group comparison design does not have the pretest step.\nInterrupted/multiple time series design: Similar to the non-equivalent control group design but several observations are taken before assigning the treatment and several observations are taken after assigning the treatment. By analyzing data points over time, both before and after the intervention, a time series design allows researchers to observe trends that existed prior to the intervention. This helps in distinguishing the actual effect of the intervention from pre-existing trends, reducing the risk of selection bias. Using lagged variables (past values of the dependent variable) can help account for autocorrelation (if present value is correlated with past values) and control for past influences, making it easier to isolate the effect of the intervention. Thus, it helps to minimize selection bias and endogeneity problems.\n\n\n\n\nNatural sciences, such as Agronomy, Horticulture, Pathology, Fisheries, and Anatomy, three basic (one factor) and two advance research designs (more than one factor) are usually used. The basic designs are: CRD, RCBD and LSD. The advance designs are factorial design and split-plot design. All of these designs must have randomization, replication and local control.\nCRD – Completely Randomized Design: One factor design where all units are homogeneous, i.e. experimental field is homogeneous in both direction. Treatments are applied randomly to all units.\n\nRCBD – Randomized Complete Block Design: One factor design where field is homogeneous in one direction but heterogeneous in another direction. The heterogeneous direction is divided into a number of blocks equal to the number of replications. Randomization of treatment is done within each block.\n\nLSD – Latin Square Design: One factor design where the field is heterogeneous in both directions. Therefore, treatments are applied randomly but restricted from both directions. Each row or column receives a treatment only once. It is a restricted design, so degree of freedom is less compared to the CRD and RCBD.\n\nFactorial Design: Two factors or treatments and both are easy to manage or manipulate. For example, effect of N and P on rice yield. This design can follow any of the basic designs.\n\n\nSplit-plot design: Two factors where one is easy to manager (fertilizer) and another is difficult to mange (irrigation). For example, effect of N and irrigation on rice yield. Firstly, the factor which is difficult to manage is randomly assigned to different whole plot (first stage randomization). Secondly, the factor which is easy to manage is randomly assigned to the split plots within each of the whole plots (second stage randomization). This design can also follow any of the basic designs.\n\n\n\n\n\n\nA Scale means a measuring tool that puts a value, score, word, or symbol against a response obtained from a respondent. The method of assigning a number for the value, score, word, or symbol is called measurement. Examples of the scale include tape, balance, measuring cup, thermometer etc. In social and behavioral science, data is collected using Thurstone scale, Likert scale, Guttman scale, semantic differential, rating scale, multidimensional scaling and so on. However, there are many other ways of collecting qualitative information from the respondents, e.g. alternate response items, sentence completion items, ranking items, open ended questions, multiple choice questions, observation checklists, situation tests, and projective techniques.\nFour levels of measurement (nominal, ordinal, interval, and ratio) are possible to obtain from various scales of measurement that are explained here.\nNominal level of measurement: What is your gender? Answer: Male = 1, Female = 2; Are you a student? Answer: Yes = 1, No = 2. This type of assigning number to responses is called nominal level of measurement. Here the number is used just to identify the response, not to quantify it. In this level, true zero point is absent and the responses cannot be ordered based on practical values. Therefore, mathematical operations are not allowed for this nominal values. Permissible statistical operations are: frequency, percentage, chi-square test etc.\nOrdinal level of measurement: What is your age group? Answer: Adolescence = 1, Young = 2, Old = 3; How much do you believe that ghost exists? Not at all = 0, Low = 1, Medium = 2, High = 3. Here, the categories can be ordered but the intervals are not equal and true zero point is absent. Therefore, mathematical operations are still not allowed. Permissible statistical operations are: frequency, percentage, chi-square test, correlation coefficient based on rankings etc.\nInterval level of measurement: What is the outside temperature now? Answer: 25°C, What is your monthly income group? Answer: BDT 20,000 or less = 1, BDT &gt;20,000 to 40,000 = 2, BDT &gt;40,000 to 60,000 = 3, BDT &gt;60,000 = 4. Here, the intervals are equal but true zero point is absent, i.e. zero does not mean absence of attribute. Example, 0°C does not mean absence of temperature. permissible statistical operations are: mean, standard deviation, Pearson product moment correlation coefficient, t-test, F-test etc.\nRatio level of measurement: What is your age? Answer: 25 years; What is your weight? Answer 60 kg; What is your family size? Answer: 6. This is the highest level of measurement which has a true zero point and intervals are equal. All types of mathematical and statistical operations are allowed.\nDevelopment of test for measurement\n1. Multiple Choice Questions (MCQ): More than one, usually four to five, probable options are provided for answers. One option is typically true and other alternatives are distractors which are prepared with care so that the distractors are related to the correct option but not correct. Good distractors should receive at least 2% of the responses. Item analysis (difficulty index and discrimination index) based on pretest is done to select the items for the tests. The best 27% and worst 27% of the items are retained for the questionnaire. The formula is similar to the Kudar-Richardson (KR-20) coefficient of split-half reliability.\n\nPossible range of the difficulty index is 0.00 to 0.99, and that the discrimination index is -1.00 to +1.00. Items to be retained for the test have difficulty index from 0.50 and 0.70 and discrimination index from 0.20 to 0.29 (twenties). The difficulty index is inversely correlated with the discrimination index.\n2. Alternate response items: The responses include only two possible options and either one is intended to be selected by the respondents. Example: Where would you prefer to live? Options: a) City, b) Rural area.\n3. Open ended questions without any given answers. Example: What would be the consequences if population growth is allowed to continue unabated?\n4. Sentence completion items: Unfinished statements. Example: Our ancestors had large families because _________________.\n5. Ranking items: Respondents are asked to rank the items according to their importance, values or preferences.\n6. Pairwise ranking: Respondents are asked to rank two items from a list of more than two items in all possible combinations. The scores are noted in a matrix format. It is particularly useful for less educated people. It reduces the cognitive burden of the respondents. To interpret the results, you can count the number of times each item is preferred.\n\n\n\n\n\n\n\n\n\n\n\n\n\nApple (A)\nBanana (B)\nCherry (C)\nDate (D)\nRanks (R)\n\n\nApple (A)\n–\nA\nA\nA\nR1: Apple: 3\n\n\nBanana (B)\n–\n–\nB\nB\nR2: Banana: 2\n\n\nCherry (C)\n–\n–\n–\nC\nR3: Cherry: 1\n\n\nDate (D)\n–\n–\n–\n–\nR4: Date: 0\n\n\n\n7. Projective technique: Respondents are asked to draw something (e.g. happy and unhappy family) or write a paragraph telling their thoughts before/after a given situation. This technique unknowingly reveals their attitudes, values and beliefs.\n8. Multidimensional scaling: Respondents are asked to point their position in a quadrant in a two dimensional aspects.\n\n9. Rating scale: Respondents are asked to rate the statements against scores, percentage, or descriptions. Example: To what extent did the cyclone Remal affect your standing crops? Options: Low | Medium | High.\n\n\n\nIn behavioral research, a researcher may not find any existing scales to measure attitudes, feelings, perceptions, aspiration, fatalism, interest, perception, personality traits or behavior of respondents. In this case, they must develop a scale that can measure such psychological constructs with reliability and validity. This is done to quantify the mysterious mental world of an individual. Four major ways to develop scales are:\n\nThurstone (differential or equal appearing interval) scales,\nLikert (summated) scales,\nGuttman (cumulative) scales, and\nSemantic differential scales (asking participants to place a mark along a line between two opposite adjectives, e.g. good —— bad).\n\nLikert scales\nLikert scales are widely used to measure attitudes and perceptions in social research. In a summated rating scale, individuals express their agreement or disagreement with each statement, and their scores for all statements are summed to determine their overall characteristic. Likert scales, developed in 1932, use a familiar five-point bipolar response format. These scales typically ask respondents to indicate their level of agreement or disagreement, approval or disapproval, or belief in the truth or falsehood of statements. The key aspect of a Likert scale is having at least five response categories to ensure ordinal data can be treated as interval data.\nHere are some examples of Likert scale responses:\n\nFrequency: Never – Seldom – Sometimes – Often – Always\nAgreement: Strongly Agree – Agree – About 50/50 – Disagree – Strongly Disagree – Don’t Know\nApproval: Strongly Approve – Approve – Need more information – Disapprove – Strongly Disapprove\nOpposition: Strongly Opposed – Definitely Opposed – A bit of both – Definitely Unopposed – Strongly Unopposed\n\nWhile “Don’t Know” is optional, responses like “About 50/50,” “Need more information,” or “A bit of both” are preferable. Seven-point scales can be created by adding “very” to the extremes, enhancing reliability. It’s best to use a wide scale, as responses can always be condensed during analysis.\nSteps to Construct a Likert Scale\n\nAssemble Statements: Gather a large number (e.g., 50) of clear, favorable, or unfavorable statements relevant to the attitude under study.\nAdminister to Subjects: Present these statements to a representative group.\nScore Responses: Score the responses for positive and negative statements according to the rules.\nTotal Scores: Sum the scores for all statements to get each individual’s total score.\nAnalyze Scores: Evaluate the reliability of the items for the study using the following formula:\n\n\nA t-value of 1.75 or higher indicates significant differentiation between high and low groups. Statements with t-values equal to or greater than 1.75 are selected for the final investigation.\nCriteria for Constructing Statements\nWhen creating statements for Likert or Thurstone scales, follow these criteria set by Edwards (1957):\n\nAvoid past-tense statements; focus on the present.\nAvoid factual statements or those that could be interpreted as such.\nAvoid statements that can be interpreted in multiple ways.\nEnsure statements are relevant to the psychological object under study.\nAvoid statements that nearly everyone or no one would endorse.\nSelect statements covering the entire range of the affective scale.\nUse simple, clear, and direct language.\nKeep statements short, ideally under 20 words.\nEach statement should contain a single thought.\nAvoid universal terms like “all,” “always,” “none,” and “never.”\nUse words like “only,” “just,” and “merely” sparingly.\nPrefer simple sentences over compound or complex ones.\nAvoid complex vocabulary that may not be understood by respondents.\nAvoid double negatives.\n\n\n\n\nIn research across social, agricultural, and natural sciences, the concepts of population and sample are fundamental for gathering data and drawing conclusions. Understanding the distinction between population and sample is crucial because it influences research design, data collection, and the validity of conclusions. A well-chosen sample allows researchers to make inferences about the population without studying every member, which is often impractical.\nPopulation: A population is the entire group of individuals, objects, or phenomena that a researcher aims to study. It encompasses all members that fit the criteria for inclusion in the research.\n\nSocial Science: The population might be all teenagers in a country when studying social media usage.\nAgricultural Science: It could be all the corn plants in a specific region when assessing crop yield.\nNatural Science: It might include all the lakes in a region when examining water quality.\n\nExample:\n\nSocial Science: If studying the voting behavior of adults in the United States, the population would be all eligible voters in the country.\nAgricultural Science: For a study on pest resistance in wheat, the population would be all wheat plants in the area of interest.\nNatural Science: In researching the migration patterns of monarch butterflies, the population would be all monarch butterflies across North America.\n\nSample: A sample is a subset of the population selected for actual study. It is meant to be representative of the population to ensure that the results can be generalized.\n\nSocial Science: Researchers might select a sample using random or stratified sampling to ensure diversity and representativeness.\nAgricultural Science: Sampling might involve selecting specific plots of land or groups of plants to study soil health or crop yields.\nNatural Science: Researchers might sample a specific number of lakes or animal individuals to study broader environmental patterns.\n\nExample:\n\nSocial Science: To study voting behavior, researchers might survey 1,000 voters from various demographics and regions.\nAgricultural Science: For a study on wheat pest resistance, a sample could be 50 wheat fields randomly selected across the region.\nNatural Science: In a study of water quality, the sample might include 30 lakes from different parts of the region.\n\n\n\n\nThe process of obtaining representative samples from a population (sampling frame) is called sampling. Without proper sampling, conclusions will not be valid for the population. In a research report, the population and sample size must be mentioned along with their formula, confidence interval, margin of error and reference. For more details on sample size, you can read this article – Determining Adequate Sample Size for Social Survey Research. For determining the required sample size you can use this web applet – https://kamrulext.shinyapps.io/sample/. However, sample size is seen smaller than the required number due to limited time and resources. In this case, the confidence interval and margin of error will tell the reader about the strength of the conclusion.\nBroadly sampling techniques are of two types: probabilistic and non-probabilistic sampling. Different types of sampling techniques are discussed here.\nProbability/Probabilistic sampling: It is based on the randomization or random selection where every unit of a population has the same chance of being selected as a sample. Randomization can be ensured in five ways:\n\nSimple random sampling: Select randomly (using lottery or computer software) required number of samples from a population.\nSystematic random sampling: Select every 5th, 10th or any other systematic unit as sample from an ordered list of population.\nStratified sampling: The population is divided into homogeneous strata and samples are then drawn from each of the strata proportionately to the sizes of the different strata or disproportionately. Randomization can be done using simple or systematic way. Thus, it can be stratified proportionate/disproportionate simple random/systematic random sampling.\nMulti-stage or double sampling: The population is divided into primary sampling units from where required number of units are selected. Each of the selected primary units are again divided into secondary sampling units from where required number of units are again selected. This can go even tertiary or further levels. In this way, we can select divisions, districts, sub-districts, unions, villages and households. In one or more of these stages, there may have several strata. In this way, we can form several types of sampling techniques:\n\nMulti-stage stratified proportionate\n\nsimple random sampling\nsystematic random sampling\n\nMulti-stage stratified disproportionate\n\nsimple random sampling\nsystematic random sampling\n\n\nCluster or area sampling: Cluster sampling is a probability sampling method used to improve efficiency when dealing with large, dispersed populations. In this approach, the population is divided into clusters, often based on geographical areas or other natural groupings (e.g. fishermen, crop farmers, and dairy farmers). Some entire clusters are then randomly selected, and all individuals within these chosen clusters are included in the sample. This method reduces costs and simplifies data collection, as it focuses on specific locations or groups rather than individuals scattered across a wide area. However, cluster sampling can introduce cluster bias if the selected clusters do not accurately represent the diversity of the entire population.\n\nNon-probability/non-probabilistic sampling: Randomization may not be suitable in many cases, for example, in psychological or medical studies. We cannot willingly put a school-going kid as a drop out or we cannot inoculate a pathogen in a healthy body that will be a violation of research ethics. Therefore, non-probability sampling is suitable in this case where each of the units does not have the same opportunity for being selected as a sample. This type of sampling helps exploration of a subject matter but lacks generalization ability.\n\nAccidental/convenience/haphazard sampling: Sample is taken based on the who-comes-first basis. For example, interviewing some people from a tea stall, interviewing households located near the roadside.\nPurposive sampling: A case is selected based on the judgment of the researcher that the case is assumed to serve as a useful sample.\nSnowball/network/chain/reputational sampling: First arbitrarily a sample is chosen and subsequent samples are drawn by requesting the first sample about potential further samples. This process goes until sufficient samples are selected. This process mimics the snowball that increases in size when rolling.\nQuota sampling: Quota sampling is a non-probability sampling method used in research to ensure that specific subgroups are adequately represented within the sample. Researchers divide the population into exclusive subgroups (e.g., age, gender, income level) and then determine a target quota for each subgroup. Participants are selected non-randomly until these quotas are met. This method helps in reflecting the characteristics of the overall population within the sample. Although it ensures diversity and can be more practical and cost-effective than random sampling, quota sampling may introduce selection bias as the sample might not be truly representative of the population.\n\nDifference between quota and cluster sampling\n\n\n\n\n\n\n\n\nAspect\nQuota Sampling\nCluster Sampling\n\n\n\n\nType\nNon-probability sampling\nProbability sampling\n\n\nPurpose\nEnsure representation of specific subgroups\nImprove efficiency and cost-effectiveness for large, dispersed populations\n\n\nMethod\nPopulation divided into subgroups based on certain characteristics; non-random selection until quotas (specific number of samples) are met\nPopulation divided into clusters; entire clusters are randomly selected and all individuals within selected clusters are included\n\n\nSelection Basis\nNon-random\nRandom\n\n\nAdvantage\nEnsures diversity and representation of key subgroups\nMore practical and cost-effective for large populations\n\n\nDisadvantage\nCan introduce selection bias and may not be truly representative of the population\nMay introduce cluster bias if selected clusters are not representative of the population\n\n\n\n\n\n\nInterview schedule and questionnaire are widely used for collecting social data. Many people unknowingly use these two terms interchangeably, but they are not the same although the both are printed set of questions. These can be structured (all questions are set in advance), non-structured (questions are framed during interview by expert interviewers), or semi-structured (some questions are prepared in advance and some are not). The differences are given here.\n\nInterview Schedule: Used in qualitative research or in-depth studies where understanding context, emotions, and detailed responses are important. For instance, a researcher studying job satisfaction might use an interview schedule to gather detailed insights from employees.\nQuestionnaire: Used in large-scale surveys or quantitative research where standardized data is needed. For example, a market research firm might use a questionnaire to gather feedback from thousands of customers about a new product. When a questionnaire is sent by mail, it is called mailed questionnaire. The examination question is also an example of questionnaire.\n\n\n\n\n\n\n\n\n\nAspect\nInterview Schedule\nQuestionnaire\n\n\n\n\nDefinition\nA structured set of questions used by an interviewer to guide a face-to-face or telephone interview.\nTo be structured, the interview schedule should be self-directed, i.e. instructions to fill the schedule should be clearly included. This also applies for the questionnaire.\nA written set of questions provided to respondents to fill out on their own\n\n\nMethod of Data Collection\nAdministered by an interviewer\nSelf-administered by the respondent\n\n\nQuestion type\nOpen type can be included.\nMostly closed form questions, i.e., answers are mentioned in the questionnaire in different forms (MCQ, or rating statements).\n\n\nPronouns used in the questions\nI, we, us, our\nYou, your, yours\n\n\nInteraction Level\nHigh interaction between interviewer and respondent\nNo interaction; respondent completes it independently\n\n\nFlexibility\nAllows for probing and follow-up questions\nLimited to predefined questions; no room for probing\n\n\nClarification\nInterviewer can clarify questions if needed\nNo immediate clarification; instructions must be clear\n\n\nResponse Rate\nGenerally higher due to personal interaction\nCan vary; often lower due to lack of personal engagement\n\n\nCost\nHigher, due to interviewer training and time\nLower, as it doesn’t require interviewer involvement\n\n\nAnonymity\nLess anonymity, as interaction is personal\nGreater anonymity, which may encourage honest responses\n\n\nBias\nPotential for interviewer bias\nReduced bias, as there is no interviewer influence\n\n\nData Consistency\nMay vary due to different interviewers’ styles\nMore consistent, as all respondents receive the same questions in the same format\n\n\n\nSteps in preparing a questionnaire or interview schedule\nPreparing a good questionnaire or interview schedule involves several key steps to ensure that it effectively gathers the necessary information while being clear and easy for respondents to complete. Here are the steps:\n\nDefine the Objectives: Clearly articulate the purpose of the questionnaire and what you hope to achieve. Define the specific information you need to collect.\nIdentify the Target Audience: Determine who will be responding to the questionnaire. Understand their background, language, and any specific characteristics relevant to the study.\nChoose the Mode of Administration: Decide whether the questionnaire will be administered online, by mail, in person, or over the phone. Each mode has its own implications for design and delivery.\nDevelop a List of Information Needed: Break down your objectives into specific information requirements. This will guide the content and structure of your questions.\nDraft Questions: Write clear, concise questions. Avoid leading, ambiguous, or complex questions. Use simple language and be specific.\nChoose the Question Type: Decide on the types of questions to use (e.g., multiple-choice, Likert scale, open-ended, dichotomous). Mix question types as appropriate to gather a range of data.\nSequence the Questions: Organize questions logically, starting with easy, engaging questions to build interest. Group similar topics together and place sensitive or difficult questions towards the end.\nUse Clear Instructions: Provide clear instructions on how to answer the questions, especially if the format changes (e.g., from multiple-choice to open-ended).\nPilot Test the Questionnaire: Test the questionnaire with a small, representative sample of your target audience. Gather feedback on question clarity, length, and overall usability.\nRevise and Refine: Use the feedback from the pilot test to make necessary adjustments. Clarify any confusing questions, remove redundant ones, and ensure that the questionnaire flows smoothly.\nPre-Test Again (if necessary): Conduct another round of testing if significant changes were made after the initial pilot test. This ensures that the revised questionnaire is effective.\nFinalize the Questionnaire: Review the final version to ensure it meets all objectives, is free of errors, and is ready for distribution.\n\nPrinciples of question phrasing and wording\nEffective question phrasing and wording are crucial for gathering accurate and meaningful responses in a questionnaire. Here are the key principles to consider:\n\nClarity and Simplicity: Use simple, clear language that is easy to understand. Avoid jargon, technical terms, and complex sentence structures.\nSpecificity: Be specific in your questions to avoid ambiguity. Make sure respondents know exactly what is being asked.\nRelevance: Ensure that every question is relevant to the research objectives. Avoid asking unnecessary or irrelevant questions.\nNeutrality: Phrase questions in a neutral manner to avoid leading or biasing respondents. Ensure that questions do not suggest a “correct” answer.\nSingle-Concept: Each question should address only one concept or issue at a time to avoid confusion (avoid double-barreled questions).\nBalanced Response Options: Provide balanced response options for closed-ended questions, including a range of choices that cover all possible answers (e.g., agree/disagree scales).\nAvoid Negative Wording: Avoid using negative wording, which can be confusing. Instead of asking “Don’t you think…?” ask “Do you think…?” .\nAvoid Double Negatives: Double negatives can confuse respondents. For example, “Do you not agree that…?” should be rephrased for clarity.\nConsistent Terminology: Use consistent terminology throughout the questionnaire to avoid confusion.\nCultural Sensitivity: Be mindful of cultural differences and avoid terms or phrases that might be offensive or misunderstood by different demographic groups.\nPre-test Questions: Pre-test questions with a small, representative sample to identify any issues with phrasing or wording before full deployment.\nLogical Flow: Arrange questions in a logical order that flows naturally from one topic to the next.\n\nExamples\n\nUnclear: “How often do you exercise?”\nClear: “How many days per week do you exercise for at least 30 minutes?”\nAmbiguous: “Do you support government programs?”\nSpecific: “Do you support the government providing free healthcare to all citizens?”\nLeading: “Don’t you agree that reducing taxes is beneficial?”\nNeutral: “What is your opinion on reducing taxes?”\nDouble-barreled: “How satisfied are you with your job and salary?”\nSingle-concept: “How satisfied are you with your job?” and “How satisfied are you with your salary?”\n\nBy adhering to these principles, you can create questions that are clear, precise, and unbiased, leading to more reliable and valid data collection.\nSteps of Interview\nBefore starting the interview the date, time and place of meeting should be agreed by both the interviewer and interviewee. While meeting, the following steps should be properly maintained. Remember: Never guess any answer and never miss any questions.\n\nBuild Rapport: Start with a brief introduction and small talk to make the participant comfortable.\nExplain Purpose and Confidentiality: Reiterate the interview’s purpose, how the data will be used, and assure confidentiality.\nInformed Consent: Ensure participants provide informed consent before the interview.\nCultural Sensitivity: Be aware of and respect cultural differences and sensitivities during the interview.\nUse Open-Ended Questions: Begin with broad questions and gradually narrow down to more specific topics.\nActive Listening: Listen attentively, show interest, and avoid interrupting the participant.\nProbing: Use follow-up questions and probes to elicit more detailed responses (e.g., “Can you tell me more about that?”).\nNeutrality: Maintain a neutral demeanor, avoiding any reactions that might influence the participant’s responses.\nRecord Responses: Use audio recording (with permission) and take notes to capture the interview accurately.\nSummarize: Briefly summarize key points discussed to ensure understanding and accuracy.\nFinal Questions: Ask if the participant has anything else to add or if they have any questions for you.\nThank the Participant: Express gratitude for their time and insights.\n\n\n\nA comparison between Rapid Rural Appraisal (RRA) and Participatory Rural Appraisal (PRA):\n\n\n\n\n\n\n\n\nAspect\nRapid Rural Appraisal (RRA)\nParticipatory Rural Appraisal (PRA)\n\n\n\n\nDefinition\nRRA is a set of techniques for quickly and systematically collecting data and gaining insights from rural communities.\nPRA is an approach that involves local people in the analysis and planning of their own development activities.\n\n\nPurpose\nTo gather quick, reliable data for decision-making by outsiders (researchers, development practitioners).\nTo empower local communities to analyze their own situation and make informed decisions about their development.\n\n\nApproach\nExtractive – information is gathered by outsiders.\nParticipatory – information is generated and analyzed by the community members themselves.\n\n\nDuration\nShort-term, often a few days to a couple of weeks.\nLonger-term, often several weeks to months, depending on the scope of participation and activities.\n\n\nRole of Outsiders\nOutsiders are the main analysts and decision-makers.\nOutsiders act as facilitators, guiding the process but not leading it.\n\n\nCommunity Involvement\nLimited to providing information.\nExtensive, with community members actively participating in the entire process.\n\n\nTechniques Used\nSemi-structured interviews, transect walks, direct observation, secondary data review, focus group discussions.\nMapping, modeling, ranking, scoring, seasonal calendars, Venn diagrams, participatory mapping, and other visual tools.\n\n\nData Collection\nRapid, relying on a mix of qualitative and quantitative methods.\nIn-depth, primarily qualitative, using visual and interactive methods.\n\n\nAnalysis\nConducted primarily by outsiders.\nConducted jointly by community members and facilitators.\n\n\nFocus\nOften problem-oriented, focusing on specific issues or areas.\nHolistic, encompassing a wide range of community issues and perspectives.\n\n\nOutcome\nReports and recommendations for development projects.\nCommunity-driven plans and actions for local development.\n\n\nEmpowerment\nLimited, as the focus is on data extraction.\nHigh, as the process enhances local capacity and self-reliance.\n\n\nSustainability\nOften project-based with limited long-term sustainability.\nAims for sustainable development through local ownership and capacity building.\n\n\nExample\nAn NGO conducts an RRA to quickly assess the agricultural needs of a village to design an intervention program.\nA development agency facilitates a PRA process in a village where community members create detailed maps and plans for managing their natural resources.\n\n\n\n\n\n\n\nA researcher needs to answer two questions:\n\nIs the research data reliable? Or will the instrument consistently produce similar scores on repeated measurements?\nDoes the instrument accurately reflect the research objective? Or does the instrument measure what it is designed to measure?\n\nTo answer these questions, the researcher must first ensure that the research instrument is reliable and valid. It is important to remember that a test can be reliable but not valid and vice versa.\nThe reliability refers to the degree to which a research instrument or procedure measures consistently over time. It refers to the consistency or reproducibility of scores. The reliability of a test can be improved by including more items of equal quality as the other items. Carefully designed test directions will also improve the testing instrument’s reliability.\nThe validity of a research instrument refers to the degree to which it accurately measures what it is intended to measure. It indicates the extent to which the instrument’s results are truthful and reflect the real characteristics or phenomena under study.\n\nHigh bias, high variance: neither reliable nor valid\nHigh variance, low bias: not reliable but valid\nLow variance, high bias: reliable but not valid\nLow variance, low bias: reliable and valid\nEstimation of reliability\nEstimating the reliability of a research instrument involves several methods, each with its own formula. Here are some common methods:\n1. Test-retest reliability (temporal stability): This method measures the stability of the instrument over time (typically 15 days gap) by administering the same test to the same group of respondents at two different points in time. The reliability coefficient is the correlation between the two sets of scores. Reliability is satisfied if r &gt; 0.7.\n2. Parallel-forms reliability (equivalent or stability over item samples): This method involves creating two different forms of the same test that are equivalent in content and difficulty. Both forms are administered to the same group, and the scores are used to calculate the correlation coefficient to express the reliability. Reliability is satisfied if r &gt; 0.7.\n3. Inter-rater reliability (stability over scores): This method assesses the consistency of scores assigned by different raters. It is often used in qualitative research or subjective assessments. The reliability is calculated by correlating the scores given by different raters. Cohen’s Kappa values less than or equal to 0 indicate no agreement, 0.01–0.20 denote none to slight agreement, 0.21–0.40 indicate fair agreement, 0.41–0.60 signify moderate agreement, 0.61–0.80 represent substantial agreement, and 0.81–1.00 denote almost perfect agreement.\n\n4. Split-half or Internal consistency reliability (stability of items):\nThis method evaluates the homogeneity or consistency of results across items within a test. In this method, the test items are divided into two (odd numbered and even numbered). If the correlation coefficient between these two sets &gt; 0.7, the scale is said to be reliable. Besides, we can measure the internal consistency of the scale is measured using Cronbach’s alpha, which assesses the average correlation among all items. However, split-half reliability is measured by correlation coefficient, Spearman-Brown Prophecy or Kudar-Richardson formula where at least 0.7 value is considered as reliable.\n\n\n\n\n\n\n\n\n\n\nEstimation of Validity\nContent validity: Content validity ensures the instrument covers the entire range of the concept being measured. It is usually evaluated qualitatively by experts in the field, who review the instrument’s items for relevance and completeness. There is no specific formula for content validity, as it relies on expert judgment rather than statistical analysis.\nFace validity: Face validity refers to the extent to which an instrument appears to measure what it is supposed to measure, based on subjective judgment by a colleague or expert in the field. Like content validity, it does not have a specific formula and relies on expert or user feedback.\nCriterion-related validity: Criterion validity evaluates how well one measure predicts an outcome based on another measure (the criterion). It includes predictive validity and concurrent validity. Predictive validity correlates actual job success (job performance) with previous activities (exam performance). Concurrent validity correlates new test scores (viva-voce) with previous standard test scores (paper-based exam). Pearson correlation coefficient is calculated to measure the validity.\nConstruct validity: A construct is a theoretical concept that is being measured or assessed. It represents an abstract idea or characteristic that cannot be directly observed but can be inferred from observable behaviors, responses, or indicators. Examples of constructs include intelligence, satisfaction, anxiety, and motivation. Construct validity assesses whether the instrument truly measures the theoretical construct it is intended to measure. It involves both convergent and discriminant validity, often evaluated through factor analysis. Convergent validity assesses whether two measures that are supposed to be related are actually related. Discriminant validity checks that measures supposed to be unrelated are indeed unrelated.\n\nEigenvalues and eigenvectors: These are computed to determine the principal components.\nFactor loadings: Represent the correlations between observed variables and the latent factors.\n\nAverage variance extracted (AVE) is calculated to estimate the convergent and discriminant validity with the following formula:\n\nAn Average Variance Extracted (AVE) value of 0.50 or higher is generally considered acceptable for demonstrating the validity of a research instrument. This threshold indicates that the construct explains at least 50% of the variance in its indicators, suggesting adequate convergent validity. For discriminant validity, the AVE of a construct (e.g. customer satisfaction) should be greater than the highest squared correlation with any other construct (e.g. customer loyalty) in the model. This means that the AVE for each construct should exceed the squared inter-construct correlations, indicating that the construct shares more variance with its own indicators than with other constructs. The goal is to demonstrate that each construct measures a unique aspect of the theoretical framework, therefore, they should not be closely related.\n\n\n\nData refers to raw, unprocessed facts and figures without context, such as numbers, text, or images. It is the basic input that can be collected and stored but lacks meaning on its own. Information, on the other hand, is data that has been processed, organized, or structured to provide context and meaning. It is useful for decision-making, understanding, and knowledge creation. For instance, a list of numbers is data, but when these numbers are analyzed and interpreted as sales figures over a month, they become information that can inform business strategies. Research data refers to the collected observations or measurements gathered during a research study. It serves as the foundation for analysis and conclusions, supporting the validation of hypotheses, theories, or findings.\nTypes of data\n\nQualitative data – express description. Example: It is beneficial, the flower color is red, the family is a large family.\nQuantitative data – numerical information\n\nDiscrete data – It is counted, can take only full integers. Example: Number of farmers, family size, number of crops.\nContinuous data – It is measured, can take any values including decimals). Example: Height, weight, distance.\n\n\n\n\nQualitative data\n\nIt’s color is black.\nIt has long hair.\nIt has lots of energy.\n\n\n\n\nQuantitative data\n\nDiscrete:\n\nIt has 4 legs.\nIt has 2 eyes.\n\nContinuous:\n\nIt weighs 7 kg.\nIt is 30 cm tall.\n\n\n\n\n\n\nFeatures refer to the distinctive characteristics or properties of the subjects under study. In social science, a feature could be the educational level of individuals in a population. In agricultural science, it might be the soil type in different farming regions. In natural science, a feature could be the temperature range of a habitat.\nVariables are elements that can change or be changed within an experiment or study. A variable in a research study is any factor, trait, or condition that can exist in differing amounts or types. They are often categorized as independent, dependent, or controlled. In social science, an independent variable could be the teaching method used in schools, while the dependent variable might be student performance. In agricultural science, the amount of fertilizer applied (independent variable) could affect crop yield (dependent variable). In natural science, sunlight exposure (independent variable) might impact plant growth (dependent variable).\nControlled variables, also known as constants, are kept unchanged to ensure that the effect on the dependent variable is due to the manipulation of the independent variable. For example, in a study on the effect of fertilizer on plant growth, the fertilizer amount is the independent variable, plant growth is the dependent variable, and factors like soil type and watering frequency are controlled variables.\nTypes of variables\nVariables that can only exist in two opposite states are known as attributes. Variables refer to the quantifiable traits or properties of the subjects being examined. Besides dependent and independent variables, there are some other types of variables. Similar to the types of data, variables are classified as qualitative, quantitative (discrete or continuous). The variables having only two values, e.g. male/female, yes/no, are termed as dichotomous or binary variable. The major jargon of variables are listed here:\n\nDependent/target/response/criterion/predicted variable: the phenomenon hypothesized to be the outcome, effect, consequence or output of some input variables. e.g. knowledge on IPM\nIndependent/regressor/input/stimulus/determinant/predictor variable: the phenomenon hypothesized to be the input or antecedent of the effect or outcome. e.g. educational status\nIntervening/ hidden variable: immeasurable variable which is hypothesized to exist and can help to explain the relationship between the dependent and independent variables. e.g. intelligence\nExtraneous variable: those measurable independent variables that are not related to the purpose of the study but may affect the dependent variable. e.g. cosmopolitanism\nEndogenous variable: An independent variable is called an endogenous variable when it can be be affected by the dependent variable. In this case, the endogenous variable shows correlation with the residuals.\nInstrumental variable: an alternative variable used instead of another variable that shows endogeneity issues with the dependent variable in a regression analysis.\n\n\n\n\nA hypothesis is a intelligent guess or testable statement predicting the relationship between variables. It proposes a potential outcome based on prior knowledge or theories.\nFor example, a researcher might hypothesize that “Applying nitrogen fertilizer will increase the yield of maize crops.” This hypothesis can be tested through experiments by applying nitrogen fertilizer to some maize crops (the experimental group) and not to others (the control group), and then measuring the yield differences. The results will either support or refute the hypothesis, contributing to the understanding of the effects of nitrogen fertilizer on maize production.\nIn social science, a researcher might hypothesize that “Students who participate in extracurricular activities have higher academic performance than those who do not.” This hypothesis can be tested by collecting data on students’ involvement in extracurricular activities and their academic grades, then analyzing the correlation between these variables. The findings will either support or refute the hypothesis, thereby contributing to the understanding of the impact of extracurricular activities on academic success.\n\n\n\nStates the Expected Relationship Between Variables: Clearly defines the interaction or association between the variables involved.\nTestable with Operationally Defined Variables: Specifies variables in measurable terms, ensuring they can be empirically tested.\nClear Implications for Testing Stated Relations: Provides explicit directions for how the relationship between variables can be examined and tested.\nExplanatory Power: Offers insights or explanations about the phenomena, contributing to the understanding of the subject matter.\nConsistency with Existing Knowledge: Aligns with and builds upon the current body of knowledge and theories in the field.\nSimplicity and Conciseness: Articulates the hypothesis in a straightforward and succinct manner.\nAppropriate Scope: Balances specificity and generality; not too broad to be untestable nor too narrow to be insignificant.\n\n\n\n\n\nProvides a Tentative Explanation of Phenomena: A hypothesis offers an initial explanation for observed phenomena, serving as a starting point for further investigation. By proposing potential relationships or effects, it facilitates the extension of knowledge in a specific area.\nOffers a Directly Testable Relational Statement: A hypothesis presents a clear, testable statement that defines the relationship between variables. This allows researchers to design experiments and studies that can empirically test these relationships, providing concrete evidence for or against the hypothesis.\nGuides the Research: Hypotheses give direction to the research process by focusing on specific variables and their potential interactions. This helps in formulating research questions, selecting methodologies, and determining the scope of the study.\nFramework for Reporting Conclusions: Hypotheses provide a structured framework for interpreting and reporting the results of a study. Researchers can compare their findings against the hypothesis to draw conclusions, making the research process more systematic and coherent.\nFirst Level of Theory Development: Hypothesis formulation is a crucial step in theory development. By proposing testable statements, hypotheses lay the groundwork for building broader theoretical frameworks that explain and predict phenomena in a given field.\nObjectivity in Testing: Hypotheses allow for objective testing that is independent of personal values and opinions. They can be empirically evaluated and shown to be supported or not supported based on evidence, ensuring that conclusions are based on data rather than subjective beliefs.\nEnhances Critical Thinking and Innovation: Formulating and testing hypotheses encourage critical thinking and innovation. Researchers must think creatively and analytically to develop plausible hypotheses and design experiments to test them, fostering scientific advancement.\n\n\n\n\nHypotheses can be classified into two main types:\n\nNull Hypothesis\nResearch Hypothesis\n\nNull Hypothesis: A null hypothesis is a formal negative statement predicting no relationship between two or more variables, typically denoted by H0​ or HN​. For example, H0​​: There is no relationship between anxiety and IQ levels of children. The null hypothesis states the opposite of what the researcher expects or predicts. The final conclusion of the study will either retain the null hypothesis or reject it in favor of an alternative hypothesis. Not rejecting H0​​ does not necessarily mean that H0​​ is true; it may simply indicate insufficient evidence against it.\nResearch Hypothesis: A research hypothesis is a formal affirmative statement predicting a relationship between two or more variables, also known as the scientific or alternative hypothesis, denoted by Ha​. For example, Ha: There is a relationship between anxiety and IQ levels of children. This hypothesis often arises from prior literature or studies and is established when the null hypothesis is rejected. The research hypothesis is usually the desired conclusion of the investigator.\nHypotheses can further be categorized as directional or non-directional.\nDirectional Hypothesis:\n\nSpecifies the expected direction of the findings.\nOften used to examine relationships among variables rather than comparing groups.\nExample: “Children with high IQ will exhibit more anxiety than children with low IQ.”\n\nNon-directional Hypothesis:\n\nDoes not specify the expected direction of the findings.\nUsed when the researcher is unsure of what to predict based on past literature.\nExample: “There is a difference in the anxiety levels of children with high IQ and those with low IQ.”\n\n\n\n\nHypothesis testing is a method for making rational decisions about the presence of effects based on empirical data. Hypotheses are tested using observed facts. When the test statistic falls within the acceptance region, the hypothesis is accepted; if it falls within the rejection region, the hypothesis is rejected. Before testing a hypothesis, we must select a significance level based on which we will reject the null hypothesis. Then we have select appropriate statistical tests based on the distribution and nature of the variables of interest.\nThe decision regarding hypothesis testing is based on the calculated value of the statistic in relation to the null hypothesis. The strength of the sample data is evaluated to determine if the null hypothesis should be rejected. If the calculated value exceeds the critical table value, the result is significant, and the null hypothesis is rejected. Conversely, if the calculated value is less than the table value, the result is non-significant, and the null hypothesis is not rejected.\nEven if hypotheses are not confirmed, they hold value (Kerlinger, 1956). Negative findings are as significant as positive ones, as they reduce ignorance and can highlight new hypotheses and avenues for research. Hypotheses cannot be conclusively proved or disproved but can be supported or not supported.\nErrors in Hypothesis Testing\nAt the conclusion of hypothesis testing, decisions can be correct or erroneous. Errors in hypothesis testing are categorized into two types:\n\nType I Error: Rejecting a null hypothesis when it is true.\nType II Error: Not rejecting a null hypothesis when it is false.\n\n\n\n\nDecision\nH0​ is True\nH0​ is False\n\n\n\n\nReject\nType I Error\nCorrect Decision\n\n\nAccept\nCorrect Decision\nType II Error\n\n\n\nSignificance Levels and Sample Size\nSetting a higher level of significance reduces the risk of Type I error but increases the risk of Type II error. The researcher must set the level of significance based on the relative severity of making a Type I or Type II error. Increasing the sample size can reduce the probability of both types of errors.\n\n\n\n\n\n\n\n\nAspect\nType I Error\nType II Error\n\n\nDefinition\nRejecting a true null hypothesis (false positive).\nFailing to reject a false null hypothesis (false negative).\n\n\nExample\nConcluding a new drug is effective when it is not.\nConcluding a new drug is ineffective when it is effective.\n\n\nSymbol\nα (alpha)\nβ (beta)\n\n\nConsequence\nTaking action based on incorrect assumptions.\nMissing out on a potentially beneficial action or intervention.\n\n\nRisk Perception\nOften seen as less dangerous because it leads to further scrutiny and testing.\nOften seen as more dangerous because it can result in missed opportunities and prolonged issues.\n\n\nReal-world Impact\nApproving a harmful or ineffective treatment.\nNot approving a beneficial treatment, leading to continued suffering.\n\n\nDanger Comparison\nType I errors can be mitigated by further research and validation.\nType II errors can prolong the problem, leading to significant negative consequences.\n\n\nMinimization option\nBy increasing alpha level.\nBy increasing sample size.\n\n\n\nType II error is often more dangerous. This type of error occurs when a researcher fails to reject a false null hypothesis, meaning that a potentially effective agricultural practice or intervention is dismissed as ineffective. Imagine a study evaluating the effectiveness of a new drought-resistant crop variety. The null hypothesis (H0) is that the new crop variety is no more effective than the existing varieties in terms of drought resistance. A Type II error in this context would mean concluding that the new crop variety is not more effective in resisting drought when it actually is. Farmers continue using the less effective existing varieties, leading to lower yields and higher susceptibility to drought conditions. This results in economic losses due to reduced productivity. The broader agricultural sector might miss out on improvements in crop yields and resilience, which could have strengthened food security and reduced economic losses during drought periods. Lower crop yields during droughts can lead to food shortages, impacting food security for communities that rely heavily on agriculture.\nOne-tailed vs two-tailed test\nIn hypothesis testing, the concepts of one-tailed and two-tailed tests are crucial. A one-tailed test is used when the rejection area is located in only one tail of the distribution, either on the left or right side. Conversely, a two-tailed test is used when the rejection areas are on both tails of the distribution. The choice between a one-tailed and a two-tailed test depends on how the null hypothesis is formulated. A one-tailed test is appropriate for testing a directional hypothesis, which indicates a specific direction of difference, such as superiority or inferiority between two groups. For example, a one-tailed hypothesis could be: H0​: Athletes do not have higher IQs than non-athletes. You can see the dynamics of probability (p-value: area below the quantile) and quantile (x) from this link https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html.\n\n\n\n\n\n\n\n\n\nFeature\nOne-Tailed Test\nTwo-Tailed Test\n\n\nDefinition\nExamines the direction of an effect\nExamines if there is any significant difference in either direction\n\n\nUse\nApplied when the hypothesis predicts a specific direction of the effect\nApplied when the hypothesis does not predict the direction of the effect\n\n\nHypotheses Example\nH0​: The new fertilizer does not increase yield (yield ≤ current yield)\nHa​: The new fertilizer increases yield (yield &gt; current yield)\nH0​​: The new teaching method does not affect performance (performance = current method)\nHa​​: The new teaching method affects performance (performance ≠ current method)\n\n\nTesting Direction\nOnly one direction (greater than or less than)\nBoth directions (either greater or less)\n\n\nPower\nMore powerful for detecting an effect in a specific direction\nMore conservative, allows for detection of effects in both directions\n\n\n\n\n\n\n\n\n\nData editing is done in two steps: field editing and central editing. Field editing is done at the field immediately after interviews to clarify any queries, complete answers, input missed answers, or correct wrongly written notes if necessary. Central editing is done after completing all the data collection during data entry and compilation if any inconsistency is noticed. The collected data is entered in an Excel sheet or in a paper-based master sheet. This process is called data tabulation. During data tabulation numerical values are assigned to qualitative data, which is called coding. The coding helps apply mathematical operations and counting easier. For example, 1 for yes, 2 for no; 1 for male, 2 for female; 1 for low, 2 for medium, 3 for high.\nCategorization means classification of data based on their class intervals. This reduces the number of levels in discrete or continuous data. Categorization must fulfill two conditions: mutually exclusive and completely exhaustive. Mutually exclusive means an observation will fall one and only once category. Completely exhaustive means all the categories will accommodate all the data and no observations will remain unclassified.\nExample of categorization: Data was collected on a group of 50 farmers’ age in years. The 50 observations are classified as follows where each farmer falls in only one of the category (mutually exclusive) and all 50 farmers are classified (completely exhaustive).\n\n\n\nCategory\nRange (years)\nFrequency\n\n\nYoung\nUp to 35\n15\n\n\nMiddle-aged\n&gt; 35 to 50\n25\n\n\nOld\n&gt; 50\n10\n\n\n\n\n\n\nSelection of suitable statistical tests and analyses depends on the purpose, data types and distribution properties. A brief overview of the selection criteria and related assumptions are stated here.\nPurpose 1: Exploration of data or cases or respondents is done using descriptive statistics, e.g. mean, standard deviation, standard error, coefficient of variation, frequency or percentage distribution, bar graph, line graph, boxplot, and group boxplot.\nPurpose 2: Difference testing between groups:\nDependent variable ——&gt; continuous\n\ngroup = 2: t-test (data normal, equal variance)\ngroup = 2: Welch test (data normal, unequal variance)\ngroup = 2: Wilcoxon test (non-parametric test, data not normal, n&lt;10)\ngroup &gt; 2: ANOVA (data normal, equal variance)\ngroup &gt; 2: Kruskal-Wallis test (data not normal\n\nDependent variable ——&gt; categorical\n\nChi-square test: cell frequency &gt; 5 for &gt; 20% cells\nFisher’s exact test: cell frequency assumptions not required\n\nPurpose 3: Testing relationship between variables is can be done by either parametric or non-parametric tests. Parametric analysis requires the data follows specific distribution otherwise the result will not be reliable. Non-parametric tests do not require such assumptions.\nPearson correlation coefficient (r) is calculated as a parametric test. In this case, the assumptions are: data should be normally distributed and both variables are continuous or equal interval (not ordinal, not nominal). If the data does not meet these criteria or the data is rank data, Spearman rank correlation (ρ) is calculated. However, for categorical variables Chi-square or Fisher’s exact test is used.\n\nChi-square test: data categorical, cell frequency &gt;5 for &gt;20% cells\nFisher’s exact test: data categorical, cell frequency &lt;5 for &gt;20% cells\nCramer’s V is calculated to understand the strength of association in Chi-square or Fisher’s exact test\nCramer’s V is a scaled version (values ranges 0 – 1) similar to r values\n\nPurpose 4: Causal factor identification:\n\nSimple regression: one independent variable\nMultiple regression: multiple independent variable\nPoisson regression: dependent variable is frequency\nCensored dependent variable: Tobit regression, censored Poisson regression\nStepwise regression: variable selection\nLogistic regression (logit/probit): dependent variable binary\nMultinomial logit: dependent variable has &gt; 2 categories\nPolynomial regression: relationship is not linear\nMachine learning: decision making for mainly categorical variables\n\n\n\n\n\n1. R\n\nOverview: R is a free, open-source programming language and software environment used for statistical computing and graphics. It is highly extensible, with a vast repository of packages available via CRAN (Comprehensive R Archive Network).\nStrengths:\n\nFlexibility: R excels in statistical analysis and graphical models. Users can perform a wide range of statistical tests and create publication-quality plots with packages like ggplot2.\nCommunity Support: Being open-source, R benefits from a large, active community of users and developers, constantly contributing new packages and updates.\nReproducibility: Scripts and markdown documents ensure that analyses can be replicated and shared easily.\n\nWeaknesses:\n\nLearning Curve: R can be challenging for beginners due to its steep learning curve and the necessity of understanding programming concepts.\nPerformance: For very large datasets, R can be slower compared to some other statistical software.\n\n\n2. Python\n\nOverview: Python is a versatile, high-level programming language known for its simplicity and readability. In the context of statistics, libraries like Pandas, NumPy, SciPy, and statsmodels, along with visualization tools like Matplotlib and Seaborn, make Python a powerful tool.\nStrengths:\n\nEase of Use: Python’s syntax is intuitive and user-friendly, making it accessible for beginners and those familiar with programming.\nIntegration: Python integrates well with other programming languages and software, making it suitable for a wide range of applications beyond statistics, such as machine learning (via scikit-learn) and data science.\nCommunity and Resources: Python has a robust community and extensive documentation, tutorials, and forums to support users.\n\nWeaknesses:\n\nStatistical Packages: While Python is strong overall, its statistical libraries are not as specialized or comprehensive as those available in R.\nPerformance: Like R, Python can struggle with performance issues when handling extremely large datasets without the use of specialized libraries or frameworks.\n\n\n3. SPSS (Statistical Package for the Social Sciences)\n\nOverview: SPSS is a software package used for interactive, or batched, statistical analysis. It is widely used in social sciences, health sciences, and market research.\nStrengths:\n\nUser-Friendly Interface: SPSS is known for its easy-to-use, menu-driven interface, which is ideal for users without a programming background.\nComprehensive Features: SPSS offers a wide range of statistical procedures, from basic descriptive statistics to complex multivariate analyses.\nIntegration: It integrates well with other IBM products and can handle large datasets efficiently.\n\nWeaknesses:\n\nCost: SPSS is a commercial product with a significant cost, which can be a barrier for some users.\nFlexibility: While powerful, SPSS is less flexible than programming languages like R and Python, particularly for custom analyses.\n\n\n4. STATA\n\nOverview: STATA is a powerful statistical software used for data management, statistical analysis, and graphical representation. It is popular in economics, sociology, and political science.\nStrengths:\n\nEase of Learning: STATA’s command syntax is straightforward and easier to learn for those new to programming.\nComprehensive Data Management: STATA excels in handling large datasets and complex data management tasks.\nAdvanced Statistical Procedures: It offers advanced statistical techniques, particularly in econometrics.\n\nWeaknesses:\n\nCost: Like SPSS, STATA is a commercial software with substantial licensing fees.\nLess Community Support: While it has a dedicated user base, STATA’s community is smaller compared to R and Python.\n\n\n5. SAS (Statistical Analysis System)\n\nOverview: SAS is a software suite developed for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\nStrengths:\n\nIndustry Standard: Widely used in corporate and governmental settings for its reliability and robustness in handling complex analyses.\nComprehensive Capabilities: SAS provides a broad range of statistical procedures and has strong data handling capabilities.\nSupport and Documentation: SAS offers extensive support and thorough documentation, which is beneficial for enterprise environments.\n\nWeaknesses:\n\nCost: SAS is one of the most expensive statistical software packages, limiting its accessibility to larger organizations.\nSteep Learning Curve: Despite its power, SAS can be difficult to learn, especially for users without a strong background in statistics or programming.\n\n\nEach of these statistical software tools has its unique strengths and weaknesses. R and Python offer flexibility and powerful community support, making them ideal for academic and research settings. SPSS and STATA are user-friendly and highly efficient for social sciences and econometrics, respectively, though their cost can be prohibitive. SAS, while expensive and complex, is a powerhouse in enterprise environments for handling large-scale, complex analyses. The choice of software often depends on the specific needs, budget, and expertise of the user.\n\n\n\nAfter analyzing the data, results are presented using texts, tables and figures without repetition. It means that if any information is presented in a table that should not be presented in a figure in the same report. However, in the texts can repeat the information for the sake of interpretation. Results include texts, tables, figures and their related interpretation. In the discussion step, the results are explained as why they are so (reasons) and how they are related to other studies and real word. When we ask the question ‘SO WHAT?’ to the results and discussion, we get conclusions. Conclusion includes implications of the results to the study areas and population. Finally, suggestions are included in a report in the form of recommendations. One golden rule is that all the sections in a report – introduction, methods, results, discussion and conclusions – are closely linked and consistent. Never add conclusions and recommendations those are not part of your research. In other words, never add any conclusions and recommendations those are not backed up by your data.\nGuidelines of including tables in a report: Table title should be placed on the top of the table. A table must be referred in the text before the table.\n\nNumbering and Titles: Number tables sequentially (e.g., Table 1, Table 2) and provide a clear, descriptive title for each table.\nSelf-Contained: Ensure tables are self-explanatory, containing all necessary information to understand the data without referring to the text.\nHeaders: Label all columns (caption) and rows (stub) clearly with descriptive headers and indicate units of measurement.\nFootnotes: Use footnotes to explain abbreviations, symbols, and any additional information necessary for understanding the table.\nConsistency: Maintain a consistent format and style throughout all tables, including font size and type, borders, and alignment.\nData Presentation: Present data accurately and succinctly, avoiding unnecessary details. Align numerical data to the right for readability.\nReferencing: Reference all tables in the text and discuss key findings presented in the tables within the report.\nJournal Guidelines: Adhere to specific guidelines provided by the journal or institution, which may include formatting rules, size limits, and submission requirements.\nSimplicity: Keep tables simple and uncluttered, avoiding excessive use of lines and shading.\nSoftware Tools: Use appropriate software tools to create tables, ensuring they are professionally formatted and easy to read.\nAvoid ditto marks for the identical texts in the next rows.\nKeep contrasting columns side by side.\nAdd the sum/total column at the right-most column or bottom-most row.\nKeep the table in a single page, if not possible repeat the header row in the following pages.\n\nGuidelines of including figures in a report: Figure title should be placed at the bottom of the figure. A figure must be referred in the text before the figure.\n\nNumbering and Titles: Number figures sequentially (e.g., Figure 1, Figure 2) and provide a concise, descriptive caption beneath each figure.\nReferencing: Refer to all figures in the main text, and ensure the discussion highlights the key information each figure presents.\nClarity and Quality: Ensure figures are of high resolution and clarity. Avoid unnecessary details that may clutter the figure. Use standard fonts and sizes.\nSelf-Contained: Each figure should be self-explanatory. Include legends and labels to explain symbols, lines, or colors used in the figure.\nUnits and Scales: Clearly indicate units of measurement and scales on axes. Use appropriate and consistent scales.\nConsistency: Maintain a consistent style and format across all figures. This includes color schemes, font sizes, and line thicknesses.\nRelevance: Only include figures that are relevant and add value to the report. Each figure should contribute to the understanding of the data and support the main findings.\nPlacement: Place figures close to where they are referenced in the text to aid readability and flow.\nEthical Considerations: Ensure figures are not manipulated in a way that misrepresents the data. Credit the source if you use figures from other works.\nSupplementary Material: If figures are complex or numerous, consider placing detailed versions in supplementary material while keeping simplified versions in the main report.\n\nTypes of charts, graphs and diagrams used as figures in a report\nIn a thesis or scientific article, various types of figures are used to present data and illustrate key concepts. Here are some common types:\n\nGraphs and Charts:\n\nLine Graphs: Show trends over time or continuous data.\nBar Charts: Compare quantities across different categories.\nPie Charts: Display proportions of a whole.\nScatter Plots: Illustrate relationships between two variables.\nHistograms: Show frequency distributions of continuous data.\n\nDiagrams and Schematics:\n\nFlowcharts: Outline processes or workflows.\nCircuit Diagrams: Represent electrical circuits.\nBlock Diagrams: Illustrate system structures or processes.\n\nPhotographs:\n\nShow real-life images of specimens, equipment, or experimental setups.\n\nMaps:\n\nProvide geographical context or spatial distribution of data.\n\nMolecular Structures:\n\nDisplay chemical structures or biological macromolecules.\n\nHeatmaps:\n\nShow intensity of data points over a two-dimensional space, often used in genomics and other biological data.\n\nBox Plots:\n\nSummarize a set of data showing its distribution, median, and outliers.\n\n3D Plots:\n\nRepresent three-dimensional data for better spatial understanding.\n\nInfographics:\n\nCombine images, charts, and minimal text to explain a concept or process in an engaging way.\n\n\nEach type of figure serves a specific purpose, helping to clarify complex information and support the narrative of the scientific work.\n\n\n\n\nScientific report is a document that contains the justification, investigation techniques and implications of a scientific inquiry. Examples: thesis, scientific articles/notes/commentary in journals. Scientific reports are reviewed by experts before publication to ensure that the policy makers and users will get reliable information about the topic of interest. Style and format of a scientific report can widely vary depending on the academic degrees, institutions, publishers and disciplines. The general format of a scientific reports contains the following five chapters (body of the report) and a few extra sections without sections numbers (abstract, acknowledgments and references, appendix/annex for interview schedule, additional related information):\n\n Abstract\nIntroduction\nMethodology\nResults\nDiscussion\nConclusions\nAcknowledgments\nReferences\nAppendix (if any)\n\nIn a thesis there is a section called preliminaries that contain\n\nCover page (Research title, name of the researcher, degree and institution, submission date)\nDeclaration (The thesis contents and supervisors’ approval)\nAcknowledgments (Thanks to those who are not author but helped in the research work)\nContents (List of chapters, sections, subsections; List of tables; List of figures)\nAbstract (Summary of the study, 200 – 300 words)\n\nThere is a separate chapter in thesis after the introduction, which is ‘Review of Literature’. This is embedded in the introduction of other types of scientific reports.\nIntroduction of a scientific report\nIntroduction means introducing the research problem and objectives. There are some well accepted and widely practices principles of writing an introduction. In a thesis, subheadings of the introduction are mentioned but in scientific journals those are implicit (hidden). Contents of a good introduction are:\n1. General background or context\n\nStart with general or international perspectives and end with specific or local perspectives towards your research topic.\n\n2. Research problem\n\n   Importance of the topic\n   Existing research/literature about the topic\n   Research gap\n   Conceptual framework\n\n3. Research objectives\n\nResearch questions or objectives specific to the study. These must be SMART (specific, measurable, achievable, rational and timely) and sufficient to fill the research gap.\n\n4. Research contributions\n\nAnticipated research contribution to the community, policymakers, science and existing body of knowledge.\n\nReview of literature of a thesis\nOutput of review of literature is to design a conceptual framework of a research work. It contains related theories and existing research outputs in relation to the study variables.\nMethodology\n\nLocation of the study where the respondents and problems are located.\nMethodological framework\nSampling frame (part of population) and techniques\nVariables and their measurements\nData collection methods\nData processing and analytical frameworks\nSoftware used for the analysis.\n\nResults/findings and discussions\nResults/findings are the output of research. This section objectively present results/findings using text, figures and tables. What has been found is the result and why this has been found is the discussion.\nConclusions\nConclusions are the implications of the results. So what is explained in this section with a generalization over the population based on the sample study. What could be the impact of the findings are concisely written in conclusion without any references. Recommendations are merged with conclusions. Recommendations include what to do to overcome the problems and what should be included in further research.\n\n\n\nThere are numerous styles of citations, footnotes, references and bibliography. Examples\nAPA: American Psychological Association (APA), mostly followed by social sciences. Please visit the linkfor detail APA updates and quick guides.\nHarvard: Author-date style, mostly followed by Elsevier, Springer and many other journals.\nEEE: Electronic and Electrical Engineering and computer science (EEE), numbered citations\nCitations are placed in the texts (body of the article). Example: Climatic impacts on agriculture are inevitable. El Niño Southern Oscillation itself, through droughts and floods, can cause 15 to 35% variation in global yield in wheat, oilseeds and coarse grains (Howden et al. 2007). Habiba et al. (2012) and Roy et al. (2018) conducted research in the drought-prone areas of Bangladesh.\nReferences are the detailed sources placed at the end of the report. These are alphabetically arranged, except EEE where references are arranged a sequence as they in the text. Example:\n\nHabiba, U., Shaw, R., & Takeuchi, Y. (2012). Farmer’s perception and adaptation practices to cope with drought: perspectives from northwestern Bangladesh. International Journal of Disaster Risk Reduction, 1, 72–84. https://doi.org/10.1016/j. ijdrr.2012.05.004\nHowden, S.M., Soussana, J.F., Tubiello, F.N., Chhetri, N., Dunlop, M., & Meinke, H. (2007). Adapting agriculture to climate change. PNAS 104, 19691–19696. https://doi.org/10.1073/pnas.0701890104\nRoy, D, Kowsari, M.S., Nath, T.D., Taiyebi, K.A., & Rashid, M.M. (2018). Smallholder farmers’ perception to climate change impact on crop production: case from drought prone areas of Bangladesh. International Journal of Agricultural Technology, 14, 1813–1828\n\nNote: References must contain only those sources that have been used in the text. References must include all the sources used in the text. Bibliography may contain additional related sources.\nUse Mendeley(free), Zotero(free), or Endnote(paid) reference manager for managing and changing styles. Create an account at Mendeley website and watch in YouTube for more information.\nSources and components of literature and information\n\nBooks: author(s), year, title, publisher, place\nJournal articles: author(s), year, title, journal, volume (issue): page, doi (digital object identifier) number\nNewspaper articles: author(s), year, title, newspaper, publication date\nConference proceedings: authors(s), year, title, conference, date, page\nChapter of an edited book: author(s), year, chapter title, editors, book title, page, publisher, place\nWebpage: authors(s), year, title, access date, web address\nAnonymous (n.d.) when author and year are unavailable.\nAnonymous (2021) when only author is unavailable.\nHasan et al. (n.d.) when only date is unavailable.\n\n‘et al.’ is a Latin phrase et alia (and others), which is usually used when there are more than two authors.\nA footnote is a note placed at the bottom of a page in a document that provides additional information or citations related to the text on that page. Footnotes are used to:\n\nCite sources of information, data, or quotations used in the text.\nProvide additional context or explanations that would be too lengthy or distracting to include in the main text.\nInclude comments or references that are relevant to the text but do not fit smoothly into the main narrative.\n\nFootnotes are typically indicated by a superscript number or symbol in the text, which corresponds to a matching number or symbol at the bottom of the page where the footnote is provided. Here is an example of how footnotes are used:\nIn the main text: “According to recent studies, the population of tigers in the wild has decreased significantly over the past decade.^1”\nAt the bottom of the page: “1. Smith, John. The Decline of Tigers in the Wild. Wildlife Conservation Journal, 2023, pp. 45-50.”\nFootnotes help keep the main text concise and focused while still providing necessary citations and additional information for readers who want to delve deeper into the topic.\n\n\n\nPlagiarism is the inclusion of someone else ideas, text and results without acknowledgment is unethical and punishable academic crime. Use quote or paraphrased statements with proper citations of the original authors (even for your own previous works) to avoid plagiarism. The Turnitin software is used to detect plagiarism. Our PSTU has its subscription and you can use it with the help from the library.\nIn the age of the 4th Industrial Revolution, we must be familiar with AI. At the same time we must be aware of its dangerous sides. It is a wolf in sheep’s cloths. So, my suggestion is to use AI cautiously and creatively. Some AI tools include ChatGPT(general purpose tool), Elicit(review of literature tool), Quillbot(paraphrasing tool). If you use AI in your research, you need to acknowledge it by mentioning why you have used it. For example AI tools have been used to improve grammar and paraphrasing.\nWe have also to be technologically sound in the academic and research world. Proficiency in MS Word, Excel, PowerPoint, R Program, Canva, Slido, Miroboard, Kahoot, Zoom, Google Meet, Microsoft Team, Google Drive, ResearchGate, Google Scholar, Scopus Researcher ID, Web of Science, ORCIDand the similar software and online platforms are some additional capital assets for a researcher.\n\n\n\n\nKothari, C. R. (2013). Research Methodology: Methods and Techniques. New Delhi: New Age International.\nEdwards, A. L. (1957). Techniques of Attitude Scale Construction. Appleton-Century-Crofts.\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate Data Analysis. 7th ed. Prentice Hall.\n\n\n\n\nHasan, M. K. (2024). Understanding Research Methodology: A Comprehensive Guide. Accessed [date of access] from www.ruenresearch.com/courses/research-methodology"
  },
  {
    "objectID": "courses/research-methodology.html#knowledge-science-technology-research",
    "href": "courses/research-methodology.html#knowledge-science-technology-research",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Knowledge is the stored information in our brain, books or internet that can be recalled when needed. If we cannot recall or retrieve the information that is not considered as knowledge. Knowledge can be gathered randomly from anywhere and by any means. However, if the knowledge is generated using systematic process (problem identification, data collection, analysis and interpretation) then this systematic knowledge is called science. Science is seen through its invention (device - pen, idea - planting date, or practice - row planting), which is called technology. When a technology is perceived as being new to an individual, the technology is termed as innovation (new technology).\nResearch is the process of generating the knowledge, i.e. the systematic process by which knowledge or technology is generated, in other words, a problem is solved. This process has to be reproducible. It is a systematic progress from the known to the unknown. Research is a crucial tool for making adjustment to the ever changing situation of our universe. Research can be fundamental/basic, applied, and action research depending on their nature.\n\nFundamental or basic research: This type of research involves experimental and theoretical work aimed at gaining new knowledge without any immediate practical applications, focusing solely on advancing understanding.\nApplied research: This research aims to address practical problems, with the goal of improving products or processes, rather than simply gaining knowledge for its own sake.\nAction research: This methodology combines action and research to explore specific questions, issues, or phenomena through observation, reflection, and intentional interventions, with a focus on immediate application in a local context. This is more participatory than applied research.\nBehavioral research: Behavioral research is a scientific field that investigates the actions and interactions of individuals and groups. It seeks to understand, describe, and predict behavior through systematic observation, experimentation, and analysis. This type of research encompasses a wide range of activities, including studying how people think, feel, and act in various situations, and examining the underlying psychological, social, and environmental factors influencing these behaviors.\n\nBehavioral research employs various methodologies, such as experiments, surveys, observations, and case studies. Experimental methods involve manipulating variables to observe their effects on behavior, often in controlled settings, to establish cause-and-effect relationships. Surveys and questionnaires gather data on attitudes, beliefs, and behaviors from large groups, providing insights into trends and correlations. Observational studies involve systematically recording behaviors in natural or laboratory settings without interference, while case studies offer in-depth analyses of individual or group behaviors over time.\nApplications of behavioral research are vast, spanning fields like psychology, sociology, education, healthcare, marketing, and public policy. Insights gained from behavioral research inform interventions, treatments, and strategies to address social issues, improve mental health, enhance educational outcomes, and optimize organizational performance. Ultimately, behavioral research contributes to a deeper understanding of human behavior, helping to create more effective solutions for individual and societal challenges.\nSome other types of research can be mentioned.\n\nHistorical research: This type of research aims to describe past events to uncover generalizations that aid in understanding both the past and present and, to some extent, predicting the future.\nDescriptive research: This research focuses on describing the current state of affairs to identify relationships between variables that are not manipulated.\nExperimental research: This research investigates what will happen when specific variables are carefully controlled or manipulated to determine relationships between these manipulated variables and other variables."
  },
  {
    "objectID": "courses/research-methodology.html#data-informatin-fact-hypothesis-theory-and-law",
    "href": "courses/research-methodology.html#data-informatin-fact-hypothesis-theory-and-law",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Term\nDefinition\nExample\n\n\n\n\nData\nRaw, unprocessed observations (numbers, text, etc.).\n“Temperature readings: 25°C, 30°C, 22°C.”\n\n\nInformation\nProcessed data with context or meaning.\n“Average temperature this week: 26°C.”\n\n\nFact\nAn objective, verifiable observation.\n“Water boils at 100°C at sea level.”\n\n\nHypothesis\nA testable prediction about a phenomenon.\n“Increasing light exposure speeds up plant growth.”\n\n\nTheory\nA well-substantiated explanation of natural phenomena.\n“Darwin’s theory of evolution by natural selection.”\n\n\nLaw\nA consistent, universal description of how (not why) phenomena behave.\n“Newton’s law of universal gravitation.”\n\n\n\n\n\n\nData → Information: Adding meaning turns data into information.\n\nHypothesis → Theory: A hypothesis becomes a theory after extensive evidence.\n\nTheory vs. Law: Theories explain why; laws describe what happens (e.g., gravity law describes attraction; general relativity theory explains it).\n\nA fact is a statement that can be proven true or false based on empirical evidence. It is objective and verifiable, often derived from observation or measurement. Facts are not always necessarily true. While facts are statements that can be proven true or false, they are considered true based on the best available evidence at the time. However, new evidence or better understanding can sometimes change what is considered a fact. For example, it was once considered a fact that the Earth was the center of the universe, but this was later disproved by scientific advancements. Therefore, facts are true as long as they are supported by current evidence, but they can be revised in light of new information. A theory, on the other hand, is a well-substantiated explanation of some aspect of the natural world, based on a body of evidence and reasoning. Theories are broader in scope than facts and can provide a framework for understanding and predicting phenomena. For example, the theory of evolution explains the diversity of life based on evidence from various scientific fields.\nA theory is formed based on observation through a systematic process. Initially, observations and empirical data are gathered from experiments or natural occurrences. Scientists then analyze this data to identify patterns, relationships, and regularities. Hypotheses are proposed as tentative explanations for these observations. These hypotheses are rigorously tested through further experimentation and data collection. If a hypothesis consistently explains the observations and withstands repeated testing and scrutiny, it may be integrated into a broader theoretical framework. Over time, as more evidence accumulates and the theory proves robust, it becomes widely accepted as a reliable explanation of the observed phenomena."
  },
  {
    "objectID": "courses/research-methodology.html#research-problem",
    "href": "courses/research-methodology.html#research-problem",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "A research problem is a specific issue, difficulty, contradiction, or gap in knowledge that a researcher aims to address through systematic investigation. It is the foundation of any research project, guiding the direction of study and focusing efforts on finding solutions or answers. In essence, a research problem sets the stage for the entire research process by outlining what needs to be investigated, why it is significant, and how it can be explored.\nIdentifying a research problem is a critical step in the research process. It requires careful observation, comprehensive literature review, and engagement with the field’s practical and theoretical aspects. In agricultural extension, addressing the right research problem can lead to significant improvements in how knowledge and technologies are disseminated to farmers, ultimately contributing to enhanced agricultural productivity and rural development. By focusing on real-world issues and gaps in existing knowledge, researchers can ensure that their work is relevant, impactful, and valuable to stakeholders.\nA research problem should not be too vague (e.g. effect of climate change on agriculture), not too narrow (e.g. name of the first president of the Malaysia.). It has be new with novelty that has not been done before but consistent with previous research.\n\n\n\nGuides the Research Process: A clearly defined research problem provides a clear direction for the research process. It helps in formulating research questions, hypotheses, and objectives, ensuring that the study remains focused and relevant.\nDetermines Research Design: The nature of the research problem influences the choice of research design, methods, and procedures. It dictates whether the study will be qualitative, quantitative, or mixed-methods.\nEnsures Relevance: A well-articulated research problem ensures that the research addresses real-world issues or gaps in knowledge, making the findings valuable to stakeholders, practitioners, and policymakers.\nFacilitates Resource Allocation: By identifying a specific problem, researchers can better allocate time, funding, and other resources to areas that require attention, optimizing the efficiency and impact of the study.\nEnhances Research Quality: A precise research problem helps in maintaining the study’s rigor and coherence, contributing to the validity and reliability of the research findings.\n\n\n\n\n\nLiterature Review: Conducting a thorough review of existing literature helps in identifying gaps, inconsistencies, and unresolved questions in the current body of knowledge. This can highlight areas that require further investigation.\nPractical Observations: Real-world observations and experiences can reveal pressing issues or challenges that need to be addressed. Engaging with practitioners, stakeholders, and communities can provide insights into practical problems.\nExpert Consultation: Discussing with experts, mentors, and colleagues in the field can help in identifying significant research problems. Their experience and perspective can shed light on critical areas that may not be immediately apparent.\nPrevious Research: Analyzing findings and recommendations from previous research studies can reveal new problems or questions that have emerged as a result of earlier work.\nPolicy Analysis: Reviewing policies, regulations, and strategic plans can uncover gaps or areas needing evaluation, particularly in applied fields like agricultural extension.\n\n\n\n\n\nContext: Agricultural extension involves the dissemination of knowledge and technologies to farmers to improve their productivity, income, and livelihoods. In Bangladesh, agricultural extension services are crucial for enhancing food security and rural development.\nObservation: Despite extensive efforts, many farmers in Bangladesh are not adopting modern agricultural practices and technologies. This results in lower productivity and inefficiencies in the agricultural sector.\nLiterature Review: A review of literature reveals that while several studies have focused on the development and efficacy of extension methods, there is limited research on the barriers to adoption from the farmers’ perspective.\nIdentified Research Problem: “What are the key barriers to the adoption of modern agricultural practices among smallholder farmers in Bangladesh, and how can agricultural extension services be improved to address these barriers?”\nImportance: Understanding these barriers is crucial for designing effective extension programs that are responsive to the needs and constraints of farmers. Addressing this problem can lead to increased adoption of beneficial practices, ultimately enhancing agricultural productivity and sustainability\n\n\n\n\n\nIdentify Potential Problems: Compile a comprehensive list of potential research problems based on literature reviews, expert consultations, observations, and stakeholder input.\nDefine Criteria for Prioritization: Establish criteria to evaluate and rank the identified research problems. Common criteria include relevance, feasibility, urgency, potential impact, and alignment with strategic national and international goals.\nEvaluate Each Problem: Assess each research problem against the defined criteria. This can be done through qualitative assessment, scoring, or ranking methods.\nEngage Stakeholders: Involve key stakeholders, including practitioners, policymakers, funders, and the target population, in the prioritization process to ensure the selected problems are relevant and impactful.\nAnalyze and Decide: Analyze the evaluation results and make decisions on which research problems to prioritize. This might involve selecting the top-ranked problems or those that meet multiple criteria strongly.\nReview and Adjust: Periodically review the prioritization process and adjust based on new information, changing contexts, or emerging issues."
  },
  {
    "objectID": "courses/research-methodology.html#research-proposal",
    "href": "courses/research-methodology.html#research-proposal",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "A research proposal is a detailed plan outlining a proposed study, aiming to communicate the significance, methodology, and anticipated outcomes of the research. All the components and methods of a research work chosen by the researcher must have acceptable reasons based on literature and appropriate arguments. Here are the key components of a research proposal:\n\nTitle: The title provides a concise summary of the research topic. It should be clear, specific, and informative. It includes what (purpose), who (scope) and sometimes how. It is better to keep around 10-15 words (not exceeding 16 words). You can watch this video to create a smart title https://www.youtube.com/watch?v=LdiJ8qUclQE. You can also take help from ChatGPT for phrasing a smart title.\nAbstract: The abstract summarizes the key elements of the proposal. It is a brief overview of the research problem, objectives, methodology, and expected outcomes. In a journal article, it is composed of usually 200-300 words. However, in a thesis it can be extended as required and suggested by the supervisors.\nIntroduction: The introduction introduces the research topic and establishes the context. It contains background information, statement of the problem, significance of the study, and research questions.\nLiterature Review: This section reviews existing research and theories related to the topic. It summarizes the relevant studies, identification of gaps in the current knowledge, and justification for the proposed research. A conceptual framework can be drawn based on the facts and theories to visualize the components and connections between various variables in a study. In a journal article, this section is integrated in the introduction section.\nResearch Objectives: This section clearly defines what the study aims to achieve. An objective should be specific, measurable, achievable, relevant, and time-bound (SMART). Objectives are written usually using ‘to + verb’. This section is also merged with the introduction section in a scientific report and article.\nResearch Hypotheses: A hypothesis is an intellectual guess based on literature and context that a researcher wants to evaluate and test. This section specifies the clear and focused hypotheses to be tested. Example: training has a positive effect on food security.\nResearch methods: This section describes the research design and methods to be used. It contains:\n\nLocation: Study location where the study will take place.\nResearch Design: Qualitative, quantitative, or mixed methods.\nPopulation and Sample: Description of the study population, sampling methods, and sample size.\nData Collection Methods: Techniques for gathering data (e.g., surveys, interviews, observations).\nData Analysis: Methods for analyzing the collected data.\nEthical Considerations: Ethical issues and how they will be addressed. For example, informed consent and application of treatments ethically.\n\nWork Plan and Timeline: It provides a time schedule for the research activities and key milestones. It is often presented as a Gantt chart. It is a non-financial budget that allocates time for different activities.\nBudget: It outlines the financial requirements of the study. Itemized budget including costs for personnel, equipment, materials, travel, and other expenses.\nExpected Outcomes: This section describes the anticipated results and their implications. It contains potential findings, contributions to knowledge, and practical applications.\nReferences: The references include a list of all the sources cited in the proposal. It contains properly formatted bibliography following a standard citation style, e.g. APA, MLA, and EEE. The citations and references must match with each other.\nAppendices (not always required): this section provides supplementary information and additional materials such as questionnaires, consent forms, detailed methodology, or supporting documents.\n\n\n\nDeveloping a conceptual framework from a literature review involves identifying key concepts, understanding their relationships, formulating hypotheses or research questions, and visualizing these elements in a coherent structure. This process helps in organizing and guiding the research, ensuring that it builds on existing knowledge and addresses relevant gaps. In behavioral research, such as agricultural extension, a well-developed conceptual framework can illuminate the factors influencing behaviors and provide a clear pathway for investigation. Developing a conceptual framework from a literature review for a behavioral research work involves several key steps. The conceptual framework helps in organizing and visualizing the key concepts, variables, and their relationships that will be studied. Here’s a step-by-step guide to developing a conceptual framework:\n1. Identify Key Concepts from the Literature Review:\n\nRead and analyze a wide range of studies related to your research topic.\nIdentify recurring themes, variables, and constructs that are important in the field.\nExtract relevant concepts, theories, and models from the existing literature.\n\n2. Define Relationships Between Concepts from the Literature Review:\n\nLook for empirical studies that examine relationships between the identified concepts.\nNote the direction and nature (positive, negative, causal, correlational) of these relationships.\nUnderstand how different concepts are related based on previous research.\n\n3. Develop Hypotheses or Research Questions based on Literature Insights:\n\nUse insights from the literature to develop testable hypotheses or focused research questions.\nFormulate specific hypotheses or research questions that your study will address.\nEnsure that these are logically derived from the identified relationships.\nHypotheses based on literature might include:\n\nH1: Higher access to agricultural information positively influences the adoption of modern practices.\nH2: Perceived risks negatively affect the adoption of modern agricultural practices.\nH3: Socio-economic factors such as income and education level significantly impact the adoption behavior.\n\n\n4. Organize the Concepts and Relationships Visually:\n\nDraw a diagram or flowchart that includes all key concepts (variables) and illustrates the hypothesized relationships among them.\nUse arrows to indicate the direction of relationships and labels to describe the nature of these relationships.\nVisualize the structure of the concepts and their relationships.\n\n5. Refine the Framework:\n\nReview and refine the framework to ensure all relevant concepts and relationships are included.\nSimplify where possible to make the framework easy to understand and interpret.\nEnsure the conceptual framework is clear, coherent, and comprehensive."
  },
  {
    "objectID": "courses/research-methodology.html#research-design",
    "href": "courses/research-methodology.html#research-design",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Research design means the strategy which is planned to answer the research questions. It essentially has the theories and models related to the research questions, sampling frame, data collection methods and analytical approaches. There are two broad types of research design or approaches: quantitative and qualitative. Although research approach is a broader term, still research design and approach are used interchangeably. There are two philosophical approaches of research – inductive and deductive. Inductive approach formulates theory from observations, and deductive approach test hypothesis based on observations using the theoretical framework.\nMixed method research answers research questions by combining both quantitative and qualitative research methods where one complements another. This a triangulation of different methods to clearly understand a research problem. The information revealed by quantitative method can also be validated using the information obtained using qualitative method.\nDifferences between quantitative and qualitative research design/approach\n\n\n\n\n\n\n\n\nAspect\nQuantitative\nQualitative\n\n\nObjective\nQuantify data, generalize results to a population\nUnderstand concepts, thoughts, or experience\n\n\nData Collection Methods\nSurveys, experiments, structured observational studies\nInterviews, focus groups, participant observation, document analysis\n\n\nData Type\nNumerical data\nNon-numerical data (text, audio, video)\n\n\nNature of Data\nStructured, quantifiable\nUnstructured or semi-structured, not easily quantifiable\n\n\nAnalysis\nStatistical analysis (e.g., regression, ANOVA)\nThematic, content, narrative analysis, coding\n\n\nGeneralizability\nHigh, applicable to larger populations\nLow, specific to particular contexts\n\n\nDepth vs. Breadth\nBreadth across large number of cases\nDepth within a smaller number of cases\n\n\nApproach\nDeductive, testing hypotheses\nInductive, generating hypotheses\n\n\nExamples\nMeasuring effect of a teaching method on performance\nExploring experiences of farmers with disasters"
  },
  {
    "objectID": "courses/research-methodology.html#experimental-design",
    "href": "courses/research-methodology.html#experimental-design",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Experimental design is a part of a research design. In order to establish a cause-and-effect relationship, manipulation of variable(s) is done with specific settings of sample(s) or experimental units. Such specific settings or approaches are called research design. Research design is the blue-print of research. It helps to control variance and minimize errors in addition to the accurate conduction of study, efficient allocation of resources and well defined reproducible work procedures. It is required for yielding maximal reliable and valid information with minimal efforts. It includes hypothesis formulation, selecting samples, random assignment of treatments, replications and local control. Research design is done with utmost care in advance since any error in it can ruin the entire research efforts.\nEssential Components of Experimental Designs\nA good experimental design maximizes reliability of information and minimize experimental errors, which is ensured by the following three components:\n\nRandomization: Treatments must be assigned randomly to experimental units. Any bias can result in variance in result that can falsely be attributed to the treatments.\nReplication: To improve accuracy and minimize error, identical treatments are applied to more than one experimental units. Thus, the number of experimental units with the identical treatments are called replication.\nLocal control: A researcher must have a high level of control over the variables. To attribute the variance truly to the treatments, factors and conditions other than the treatments must be kept homogeneous for all experimental units. This process of keeping homogeneity among the experimental units is called local control. If a researcher fails to maintain homogeneity, the effect of replication will be significant and they cannot claim that the resulted variance in the dependent variable is due to the effect of the treatments.\n\nTypes of experimental design in social research\n\nPre-experimental or non-experimental design: Randomization is not practical to be used in this case. This design can take different forms, some of which are explained here.\n\nOne group post-test design: Data is collected after certain time of applying treatments. In this design, no pretest/benchmark/initial data is available, and therefore, exact effect of the treatment is not possible.\nOne group pretest-post-test design: Data is collected twice – firstly before applying the treatment and secondly after certain period of treatment application. Effect is measured by deducting the initial quantity from the final quantity.\nStatic group comparison (Control group post-test) design: Data is collected after certain time of applying treatments from the treatment groups. Data is collected from another control group that has not received the treatment. Two groups should be as similar as possible to make them comparable. However, the groups should also be as distant as possible to avoid spill-over/triple effect of the treatment.\n\nTrue experimental design: Randomization is properly done and at least a control group is included.\n\nPost-test control group design (Randomized control trial – RCT): Participants are randomly selected from control and treatment groups. Observations are made after applying the treatments.\nPretest-post-test control group design: Similar to the RCT design, participants are randomly assigned treatments, but data is collected before the treatment application and after certain period of the treatment application.\n\nQuasi-experimental design: Randomization is not possible, such as in psychological study. For example, we cannot assign a school going kid in the drop out group.\n\nNon-equivalent control group design: Similar to the pretest-post-test control group design, it has both control and treatment groups but without randomization. It also similar to the static group comparison design because both have control groups and do not have randomization, but different because the static group comparison design does not have the pretest step.\nInterrupted/multiple time series design: Similar to the non-equivalent control group design but several observations are taken before assigning the treatment and several observations are taken after assigning the treatment. By analyzing data points over time, both before and after the intervention, a time series design allows researchers to observe trends that existed prior to the intervention. This helps in distinguishing the actual effect of the intervention from pre-existing trends, reducing the risk of selection bias. Using lagged variables (past values of the dependent variable) can help account for autocorrelation (if present value is correlated with past values) and control for past influences, making it easier to isolate the effect of the intervention. Thus, it helps to minimize selection bias and endogeneity problems.\n\n\n\n\nNatural sciences, such as Agronomy, Horticulture, Pathology, Fisheries, and Anatomy, three basic (one factor) and two advance research designs (more than one factor) are usually used. The basic designs are: CRD, RCBD and LSD. The advance designs are factorial design and split-plot design. All of these designs must have randomization, replication and local control.\nCRD – Completely Randomized Design: One factor design where all units are homogeneous, i.e. experimental field is homogeneous in both direction. Treatments are applied randomly to all units.\n\nRCBD – Randomized Complete Block Design: One factor design where field is homogeneous in one direction but heterogeneous in another direction. The heterogeneous direction is divided into a number of blocks equal to the number of replications. Randomization of treatment is done within each block.\n\nLSD – Latin Square Design: One factor design where the field is heterogeneous in both directions. Therefore, treatments are applied randomly but restricted from both directions. Each row or column receives a treatment only once. It is a restricted design, so degree of freedom is less compared to the CRD and RCBD.\n\nFactorial Design: Two factors or treatments and both are easy to manage or manipulate. For example, effect of N and P on rice yield. This design can follow any of the basic designs.\n\n\nSplit-plot design: Two factors where one is easy to manager (fertilizer) and another is difficult to mange (irrigation). For example, effect of N and irrigation on rice yield. Firstly, the factor which is difficult to manage is randomly assigned to different whole plot (first stage randomization). Secondly, the factor which is easy to manage is randomly assigned to the split plots within each of the whole plots (second stage randomization). This design can also follow any of the basic designs."
  },
  {
    "objectID": "courses/research-methodology.html#scale-and-measurement",
    "href": "courses/research-methodology.html#scale-and-measurement",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "A Scale means a measuring tool that puts a value, score, word, or symbol against a response obtained from a respondent. The method of assigning a number for the value, score, word, or symbol is called measurement. Examples of the scale include tape, balance, measuring cup, thermometer etc. In social and behavioral science, data is collected using Thurstone scale, Likert scale, Guttman scale, semantic differential, rating scale, multidimensional scaling and so on. However, there are many other ways of collecting qualitative information from the respondents, e.g. alternate response items, sentence completion items, ranking items, open ended questions, multiple choice questions, observation checklists, situation tests, and projective techniques.\nFour levels of measurement (nominal, ordinal, interval, and ratio) are possible to obtain from various scales of measurement that are explained here.\nNominal level of measurement: What is your gender? Answer: Male = 1, Female = 2; Are you a student? Answer: Yes = 1, No = 2. This type of assigning number to responses is called nominal level of measurement. Here the number is used just to identify the response, not to quantify it. In this level, true zero point is absent and the responses cannot be ordered based on practical values. Therefore, mathematical operations are not allowed for this nominal values. Permissible statistical operations are: frequency, percentage, chi-square test etc.\nOrdinal level of measurement: What is your age group? Answer: Adolescence = 1, Young = 2, Old = 3; How much do you believe that ghost exists? Not at all = 0, Low = 1, Medium = 2, High = 3. Here, the categories can be ordered but the intervals are not equal and true zero point is absent. Therefore, mathematical operations are still not allowed. Permissible statistical operations are: frequency, percentage, chi-square test, correlation coefficient based on rankings etc.\nInterval level of measurement: What is the outside temperature now? Answer: 25°C, What is your monthly income group? Answer: BDT 20,000 or less = 1, BDT &gt;20,000 to 40,000 = 2, BDT &gt;40,000 to 60,000 = 3, BDT &gt;60,000 = 4. Here, the intervals are equal but true zero point is absent, i.e. zero does not mean absence of attribute. Example, 0°C does not mean absence of temperature. permissible statistical operations are: mean, standard deviation, Pearson product moment correlation coefficient, t-test, F-test etc.\nRatio level of measurement: What is your age? Answer: 25 years; What is your weight? Answer 60 kg; What is your family size? Answer: 6. This is the highest level of measurement which has a true zero point and intervals are equal. All types of mathematical and statistical operations are allowed.\nDevelopment of test for measurement\n1. Multiple Choice Questions (MCQ): More than one, usually four to five, probable options are provided for answers. One option is typically true and other alternatives are distractors which are prepared with care so that the distractors are related to the correct option but not correct. Good distractors should receive at least 2% of the responses. Item analysis (difficulty index and discrimination index) based on pretest is done to select the items for the tests. The best 27% and worst 27% of the items are retained for the questionnaire. The formula is similar to the Kudar-Richardson (KR-20) coefficient of split-half reliability.\n\nPossible range of the difficulty index is 0.00 to 0.99, and that the discrimination index is -1.00 to +1.00. Items to be retained for the test have difficulty index from 0.50 and 0.70 and discrimination index from 0.20 to 0.29 (twenties). The difficulty index is inversely correlated with the discrimination index.\n2. Alternate response items: The responses include only two possible options and either one is intended to be selected by the respondents. Example: Where would you prefer to live? Options: a) City, b) Rural area.\n3. Open ended questions without any given answers. Example: What would be the consequences if population growth is allowed to continue unabated?\n4. Sentence completion items: Unfinished statements. Example: Our ancestors had large families because _________________.\n5. Ranking items: Respondents are asked to rank the items according to their importance, values or preferences.\n6. Pairwise ranking: Respondents are asked to rank two items from a list of more than two items in all possible combinations. The scores are noted in a matrix format. It is particularly useful for less educated people. It reduces the cognitive burden of the respondents. To interpret the results, you can count the number of times each item is preferred.\n\n\n\n\n\n\n\n\n\n\n\n\n\nApple (A)\nBanana (B)\nCherry (C)\nDate (D)\nRanks (R)\n\n\nApple (A)\n–\nA\nA\nA\nR1: Apple: 3\n\n\nBanana (B)\n–\n–\nB\nB\nR2: Banana: 2\n\n\nCherry (C)\n–\n–\n–\nC\nR3: Cherry: 1\n\n\nDate (D)\n–\n–\n–\n–\nR4: Date: 0\n\n\n\n7. Projective technique: Respondents are asked to draw something (e.g. happy and unhappy family) or write a paragraph telling their thoughts before/after a given situation. This technique unknowingly reveals their attitudes, values and beliefs.\n8. Multidimensional scaling: Respondents are asked to point their position in a quadrant in a two dimensional aspects.\n\n9. Rating scale: Respondents are asked to rate the statements against scores, percentage, or descriptions. Example: To what extent did the cyclone Remal affect your standing crops? Options: Low | Medium | High."
  },
  {
    "objectID": "courses/research-methodology.html#development-of-scales",
    "href": "courses/research-methodology.html#development-of-scales",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "In behavioral research, a researcher may not find any existing scales to measure attitudes, feelings, perceptions, aspiration, fatalism, interest, perception, personality traits or behavior of respondents. In this case, they must develop a scale that can measure such psychological constructs with reliability and validity. This is done to quantify the mysterious mental world of an individual. Four major ways to develop scales are:\n\nThurstone (differential or equal appearing interval) scales,\nLikert (summated) scales,\nGuttman (cumulative) scales, and\nSemantic differential scales (asking participants to place a mark along a line between two opposite adjectives, e.g. good —— bad).\n\nLikert scales\nLikert scales are widely used to measure attitudes and perceptions in social research. In a summated rating scale, individuals express their agreement or disagreement with each statement, and their scores for all statements are summed to determine their overall characteristic. Likert scales, developed in 1932, use a familiar five-point bipolar response format. These scales typically ask respondents to indicate their level of agreement or disagreement, approval or disapproval, or belief in the truth or falsehood of statements. The key aspect of a Likert scale is having at least five response categories to ensure ordinal data can be treated as interval data.\nHere are some examples of Likert scale responses:\n\nFrequency: Never – Seldom – Sometimes – Often – Always\nAgreement: Strongly Agree – Agree – About 50/50 – Disagree – Strongly Disagree – Don’t Know\nApproval: Strongly Approve – Approve – Need more information – Disapprove – Strongly Disapprove\nOpposition: Strongly Opposed – Definitely Opposed – A bit of both – Definitely Unopposed – Strongly Unopposed\n\nWhile “Don’t Know” is optional, responses like “About 50/50,” “Need more information,” or “A bit of both” are preferable. Seven-point scales can be created by adding “very” to the extremes, enhancing reliability. It’s best to use a wide scale, as responses can always be condensed during analysis.\nSteps to Construct a Likert Scale\n\nAssemble Statements: Gather a large number (e.g., 50) of clear, favorable, or unfavorable statements relevant to the attitude under study.\nAdminister to Subjects: Present these statements to a representative group.\nScore Responses: Score the responses for positive and negative statements according to the rules.\nTotal Scores: Sum the scores for all statements to get each individual’s total score.\nAnalyze Scores: Evaluate the reliability of the items for the study using the following formula:\n\n\nA t-value of 1.75 or higher indicates significant differentiation between high and low groups. Statements with t-values equal to or greater than 1.75 are selected for the final investigation.\nCriteria for Constructing Statements\nWhen creating statements for Likert or Thurstone scales, follow these criteria set by Edwards (1957):\n\nAvoid past-tense statements; focus on the present.\nAvoid factual statements or those that could be interpreted as such.\nAvoid statements that can be interpreted in multiple ways.\nEnsure statements are relevant to the psychological object under study.\nAvoid statements that nearly everyone or no one would endorse.\nSelect statements covering the entire range of the affective scale.\nUse simple, clear, and direct language.\nKeep statements short, ideally under 20 words.\nEach statement should contain a single thought.\nAvoid universal terms like “all,” “always,” “none,” and “never.”\nUse words like “only,” “just,” and “merely” sparingly.\nPrefer simple sentences over compound or complex ones.\nAvoid complex vocabulary that may not be understood by respondents.\nAvoid double negatives."
  },
  {
    "objectID": "courses/research-methodology.html#population-and-sampling",
    "href": "courses/research-methodology.html#population-and-sampling",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "In research across social, agricultural, and natural sciences, the concepts of population and sample are fundamental for gathering data and drawing conclusions. Understanding the distinction between population and sample is crucial because it influences research design, data collection, and the validity of conclusions. A well-chosen sample allows researchers to make inferences about the population without studying every member, which is often impractical.\nPopulation: A population is the entire group of individuals, objects, or phenomena that a researcher aims to study. It encompasses all members that fit the criteria for inclusion in the research.\n\nSocial Science: The population might be all teenagers in a country when studying social media usage.\nAgricultural Science: It could be all the corn plants in a specific region when assessing crop yield.\nNatural Science: It might include all the lakes in a region when examining water quality.\n\nExample:\n\nSocial Science: If studying the voting behavior of adults in the United States, the population would be all eligible voters in the country.\nAgricultural Science: For a study on pest resistance in wheat, the population would be all wheat plants in the area of interest.\nNatural Science: In researching the migration patterns of monarch butterflies, the population would be all monarch butterflies across North America.\n\nSample: A sample is a subset of the population selected for actual study. It is meant to be representative of the population to ensure that the results can be generalized.\n\nSocial Science: Researchers might select a sample using random or stratified sampling to ensure diversity and representativeness.\nAgricultural Science: Sampling might involve selecting specific plots of land or groups of plants to study soil health or crop yields.\nNatural Science: Researchers might sample a specific number of lakes or animal individuals to study broader environmental patterns.\n\nExample:\n\nSocial Science: To study voting behavior, researchers might survey 1,000 voters from various demographics and regions.\nAgricultural Science: For a study on wheat pest resistance, a sample could be 50 wheat fields randomly selected across the region.\nNatural Science: In a study of water quality, the sample might include 30 lakes from different parts of the region."
  },
  {
    "objectID": "courses/research-methodology.html#sampling-techniques",
    "href": "courses/research-methodology.html#sampling-techniques",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "The process of obtaining representative samples from a population (sampling frame) is called sampling. Without proper sampling, conclusions will not be valid for the population. In a research report, the population and sample size must be mentioned along with their formula, confidence interval, margin of error and reference. For more details on sample size, you can read this article – Determining Adequate Sample Size for Social Survey Research. For determining the required sample size you can use this web applet – https://kamrulext.shinyapps.io/sample/. However, sample size is seen smaller than the required number due to limited time and resources. In this case, the confidence interval and margin of error will tell the reader about the strength of the conclusion.\nBroadly sampling techniques are of two types: probabilistic and non-probabilistic sampling. Different types of sampling techniques are discussed here.\nProbability/Probabilistic sampling: It is based on the randomization or random selection where every unit of a population has the same chance of being selected as a sample. Randomization can be ensured in five ways:\n\nSimple random sampling: Select randomly (using lottery or computer software) required number of samples from a population.\nSystematic random sampling: Select every 5th, 10th or any other systematic unit as sample from an ordered list of population.\nStratified sampling: The population is divided into homogeneous strata and samples are then drawn from each of the strata proportionately to the sizes of the different strata or disproportionately. Randomization can be done using simple or systematic way. Thus, it can be stratified proportionate/disproportionate simple random/systematic random sampling.\nMulti-stage or double sampling: The population is divided into primary sampling units from where required number of units are selected. Each of the selected primary units are again divided into secondary sampling units from where required number of units are again selected. This can go even tertiary or further levels. In this way, we can select divisions, districts, sub-districts, unions, villages and households. In one or more of these stages, there may have several strata. In this way, we can form several types of sampling techniques:\n\nMulti-stage stratified proportionate\n\nsimple random sampling\nsystematic random sampling\n\nMulti-stage stratified disproportionate\n\nsimple random sampling\nsystematic random sampling\n\n\nCluster or area sampling: Cluster sampling is a probability sampling method used to improve efficiency when dealing with large, dispersed populations. In this approach, the population is divided into clusters, often based on geographical areas or other natural groupings (e.g. fishermen, crop farmers, and dairy farmers). Some entire clusters are then randomly selected, and all individuals within these chosen clusters are included in the sample. This method reduces costs and simplifies data collection, as it focuses on specific locations or groups rather than individuals scattered across a wide area. However, cluster sampling can introduce cluster bias if the selected clusters do not accurately represent the diversity of the entire population.\n\nNon-probability/non-probabilistic sampling: Randomization may not be suitable in many cases, for example, in psychological or medical studies. We cannot willingly put a school-going kid as a drop out or we cannot inoculate a pathogen in a healthy body that will be a violation of research ethics. Therefore, non-probability sampling is suitable in this case where each of the units does not have the same opportunity for being selected as a sample. This type of sampling helps exploration of a subject matter but lacks generalization ability.\n\nAccidental/convenience/haphazard sampling: Sample is taken based on the who-comes-first basis. For example, interviewing some people from a tea stall, interviewing households located near the roadside.\nPurposive sampling: A case is selected based on the judgment of the researcher that the case is assumed to serve as a useful sample.\nSnowball/network/chain/reputational sampling: First arbitrarily a sample is chosen and subsequent samples are drawn by requesting the first sample about potential further samples. This process goes until sufficient samples are selected. This process mimics the snowball that increases in size when rolling.\nQuota sampling: Quota sampling is a non-probability sampling method used in research to ensure that specific subgroups are adequately represented within the sample. Researchers divide the population into exclusive subgroups (e.g., age, gender, income level) and then determine a target quota for each subgroup. Participants are selected non-randomly until these quotas are met. This method helps in reflecting the characteristics of the overall population within the sample. Although it ensures diversity and can be more practical and cost-effective than random sampling, quota sampling may introduce selection bias as the sample might not be truly representative of the population.\n\nDifference between quota and cluster sampling\n\n\n\n\n\n\n\n\nAspect\nQuota Sampling\nCluster Sampling\n\n\n\n\nType\nNon-probability sampling\nProbability sampling\n\n\nPurpose\nEnsure representation of specific subgroups\nImprove efficiency and cost-effectiveness for large, dispersed populations\n\n\nMethod\nPopulation divided into subgroups based on certain characteristics; non-random selection until quotas (specific number of samples) are met\nPopulation divided into clusters; entire clusters are randomly selected and all individuals within selected clusters are included\n\n\nSelection Basis\nNon-random\nRandom\n\n\nAdvantage\nEnsures diversity and representation of key subgroups\nMore practical and cost-effective for large populations\n\n\nDisadvantage\nCan introduce selection bias and may not be truly representative of the population\nMay introduce cluster bias if selected clusters are not representative of the population"
  },
  {
    "objectID": "courses/research-methodology.html#data-collection-tools",
    "href": "courses/research-methodology.html#data-collection-tools",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Interview schedule and questionnaire are widely used for collecting social data. Many people unknowingly use these two terms interchangeably, but they are not the same although the both are printed set of questions. These can be structured (all questions are set in advance), non-structured (questions are framed during interview by expert interviewers), or semi-structured (some questions are prepared in advance and some are not). The differences are given here.\n\nInterview Schedule: Used in qualitative research or in-depth studies where understanding context, emotions, and detailed responses are important. For instance, a researcher studying job satisfaction might use an interview schedule to gather detailed insights from employees.\nQuestionnaire: Used in large-scale surveys or quantitative research where standardized data is needed. For example, a market research firm might use a questionnaire to gather feedback from thousands of customers about a new product. When a questionnaire is sent by mail, it is called mailed questionnaire. The examination question is also an example of questionnaire.\n\n\n\n\n\n\n\n\n\nAspect\nInterview Schedule\nQuestionnaire\n\n\n\n\nDefinition\nA structured set of questions used by an interviewer to guide a face-to-face or telephone interview.\nTo be structured, the interview schedule should be self-directed, i.e. instructions to fill the schedule should be clearly included. This also applies for the questionnaire.\nA written set of questions provided to respondents to fill out on their own\n\n\nMethod of Data Collection\nAdministered by an interviewer\nSelf-administered by the respondent\n\n\nQuestion type\nOpen type can be included.\nMostly closed form questions, i.e., answers are mentioned in the questionnaire in different forms (MCQ, or rating statements).\n\n\nPronouns used in the questions\nI, we, us, our\nYou, your, yours\n\n\nInteraction Level\nHigh interaction between interviewer and respondent\nNo interaction; respondent completes it independently\n\n\nFlexibility\nAllows for probing and follow-up questions\nLimited to predefined questions; no room for probing\n\n\nClarification\nInterviewer can clarify questions if needed\nNo immediate clarification; instructions must be clear\n\n\nResponse Rate\nGenerally higher due to personal interaction\nCan vary; often lower due to lack of personal engagement\n\n\nCost\nHigher, due to interviewer training and time\nLower, as it doesn’t require interviewer involvement\n\n\nAnonymity\nLess anonymity, as interaction is personal\nGreater anonymity, which may encourage honest responses\n\n\nBias\nPotential for interviewer bias\nReduced bias, as there is no interviewer influence\n\n\nData Consistency\nMay vary due to different interviewers’ styles\nMore consistent, as all respondents receive the same questions in the same format\n\n\n\nSteps in preparing a questionnaire or interview schedule\nPreparing a good questionnaire or interview schedule involves several key steps to ensure that it effectively gathers the necessary information while being clear and easy for respondents to complete. Here are the steps:\n\nDefine the Objectives: Clearly articulate the purpose of the questionnaire and what you hope to achieve. Define the specific information you need to collect.\nIdentify the Target Audience: Determine who will be responding to the questionnaire. Understand their background, language, and any specific characteristics relevant to the study.\nChoose the Mode of Administration: Decide whether the questionnaire will be administered online, by mail, in person, or over the phone. Each mode has its own implications for design and delivery.\nDevelop a List of Information Needed: Break down your objectives into specific information requirements. This will guide the content and structure of your questions.\nDraft Questions: Write clear, concise questions. Avoid leading, ambiguous, or complex questions. Use simple language and be specific.\nChoose the Question Type: Decide on the types of questions to use (e.g., multiple-choice, Likert scale, open-ended, dichotomous). Mix question types as appropriate to gather a range of data.\nSequence the Questions: Organize questions logically, starting with easy, engaging questions to build interest. Group similar topics together and place sensitive or difficult questions towards the end.\nUse Clear Instructions: Provide clear instructions on how to answer the questions, especially if the format changes (e.g., from multiple-choice to open-ended).\nPilot Test the Questionnaire: Test the questionnaire with a small, representative sample of your target audience. Gather feedback on question clarity, length, and overall usability.\nRevise and Refine: Use the feedback from the pilot test to make necessary adjustments. Clarify any confusing questions, remove redundant ones, and ensure that the questionnaire flows smoothly.\nPre-Test Again (if necessary): Conduct another round of testing if significant changes were made after the initial pilot test. This ensures that the revised questionnaire is effective.\nFinalize the Questionnaire: Review the final version to ensure it meets all objectives, is free of errors, and is ready for distribution.\n\nPrinciples of question phrasing and wording\nEffective question phrasing and wording are crucial for gathering accurate and meaningful responses in a questionnaire. Here are the key principles to consider:\n\nClarity and Simplicity: Use simple, clear language that is easy to understand. Avoid jargon, technical terms, and complex sentence structures.\nSpecificity: Be specific in your questions to avoid ambiguity. Make sure respondents know exactly what is being asked.\nRelevance: Ensure that every question is relevant to the research objectives. Avoid asking unnecessary or irrelevant questions.\nNeutrality: Phrase questions in a neutral manner to avoid leading or biasing respondents. Ensure that questions do not suggest a “correct” answer.\nSingle-Concept: Each question should address only one concept or issue at a time to avoid confusion (avoid double-barreled questions).\nBalanced Response Options: Provide balanced response options for closed-ended questions, including a range of choices that cover all possible answers (e.g., agree/disagree scales).\nAvoid Negative Wording: Avoid using negative wording, which can be confusing. Instead of asking “Don’t you think…?” ask “Do you think…?” .\nAvoid Double Negatives: Double negatives can confuse respondents. For example, “Do you not agree that…?” should be rephrased for clarity.\nConsistent Terminology: Use consistent terminology throughout the questionnaire to avoid confusion.\nCultural Sensitivity: Be mindful of cultural differences and avoid terms or phrases that might be offensive or misunderstood by different demographic groups.\nPre-test Questions: Pre-test questions with a small, representative sample to identify any issues with phrasing or wording before full deployment.\nLogical Flow: Arrange questions in a logical order that flows naturally from one topic to the next.\n\nExamples\n\nUnclear: “How often do you exercise?”\nClear: “How many days per week do you exercise for at least 30 minutes?”\nAmbiguous: “Do you support government programs?”\nSpecific: “Do you support the government providing free healthcare to all citizens?”\nLeading: “Don’t you agree that reducing taxes is beneficial?”\nNeutral: “What is your opinion on reducing taxes?”\nDouble-barreled: “How satisfied are you with your job and salary?”\nSingle-concept: “How satisfied are you with your job?” and “How satisfied are you with your salary?”\n\nBy adhering to these principles, you can create questions that are clear, precise, and unbiased, leading to more reliable and valid data collection.\nSteps of Interview\nBefore starting the interview the date, time and place of meeting should be agreed by both the interviewer and interviewee. While meeting, the following steps should be properly maintained. Remember: Never guess any answer and never miss any questions.\n\nBuild Rapport: Start with a brief introduction and small talk to make the participant comfortable.\nExplain Purpose and Confidentiality: Reiterate the interview’s purpose, how the data will be used, and assure confidentiality.\nInformed Consent: Ensure participants provide informed consent before the interview.\nCultural Sensitivity: Be aware of and respect cultural differences and sensitivities during the interview.\nUse Open-Ended Questions: Begin with broad questions and gradually narrow down to more specific topics.\nActive Listening: Listen attentively, show interest, and avoid interrupting the participant.\nProbing: Use follow-up questions and probes to elicit more detailed responses (e.g., “Can you tell me more about that?”).\nNeutrality: Maintain a neutral demeanor, avoiding any reactions that might influence the participant’s responses.\nRecord Responses: Use audio recording (with permission) and take notes to capture the interview accurately.\nSummarize: Briefly summarize key points discussed to ensure understanding and accuracy.\nFinal Questions: Ask if the participant has anything else to add or if they have any questions for you.\nThank the Participant: Express gratitude for their time and insights.\n\n\n\nA comparison between Rapid Rural Appraisal (RRA) and Participatory Rural Appraisal (PRA):\n\n\n\n\n\n\n\n\nAspect\nRapid Rural Appraisal (RRA)\nParticipatory Rural Appraisal (PRA)\n\n\n\n\nDefinition\nRRA is a set of techniques for quickly and systematically collecting data and gaining insights from rural communities.\nPRA is an approach that involves local people in the analysis and planning of their own development activities.\n\n\nPurpose\nTo gather quick, reliable data for decision-making by outsiders (researchers, development practitioners).\nTo empower local communities to analyze their own situation and make informed decisions about their development.\n\n\nApproach\nExtractive – information is gathered by outsiders.\nParticipatory – information is generated and analyzed by the community members themselves.\n\n\nDuration\nShort-term, often a few days to a couple of weeks.\nLonger-term, often several weeks to months, depending on the scope of participation and activities.\n\n\nRole of Outsiders\nOutsiders are the main analysts and decision-makers.\nOutsiders act as facilitators, guiding the process but not leading it.\n\n\nCommunity Involvement\nLimited to providing information.\nExtensive, with community members actively participating in the entire process.\n\n\nTechniques Used\nSemi-structured interviews, transect walks, direct observation, secondary data review, focus group discussions.\nMapping, modeling, ranking, scoring, seasonal calendars, Venn diagrams, participatory mapping, and other visual tools.\n\n\nData Collection\nRapid, relying on a mix of qualitative and quantitative methods.\nIn-depth, primarily qualitative, using visual and interactive methods.\n\n\nAnalysis\nConducted primarily by outsiders.\nConducted jointly by community members and facilitators.\n\n\nFocus\nOften problem-oriented, focusing on specific issues or areas.\nHolistic, encompassing a wide range of community issues and perspectives.\n\n\nOutcome\nReports and recommendations for development projects.\nCommunity-driven plans and actions for local development.\n\n\nEmpowerment\nLimited, as the focus is on data extraction.\nHigh, as the process enhances local capacity and self-reliance.\n\n\nSustainability\nOften project-based with limited long-term sustainability.\nAims for sustainable development through local ownership and capacity building.\n\n\nExample\nAn NGO conducts an RRA to quickly assess the agricultural needs of a village to design an intervention program.\nA development agency facilitates a PRA process in a village where community members create detailed maps and plans for managing their natural resources."
  },
  {
    "objectID": "courses/research-methodology.html#reliability-and-validity-of-research-instruments",
    "href": "courses/research-methodology.html#reliability-and-validity-of-research-instruments",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "A researcher needs to answer two questions:\n\nIs the research data reliable? Or will the instrument consistently produce similar scores on repeated measurements?\nDoes the instrument accurately reflect the research objective? Or does the instrument measure what it is designed to measure?\n\nTo answer these questions, the researcher must first ensure that the research instrument is reliable and valid. It is important to remember that a test can be reliable but not valid and vice versa.\nThe reliability refers to the degree to which a research instrument or procedure measures consistently over time. It refers to the consistency or reproducibility of scores. The reliability of a test can be improved by including more items of equal quality as the other items. Carefully designed test directions will also improve the testing instrument’s reliability.\nThe validity of a research instrument refers to the degree to which it accurately measures what it is intended to measure. It indicates the extent to which the instrument’s results are truthful and reflect the real characteristics or phenomena under study.\n\nHigh bias, high variance: neither reliable nor valid\nHigh variance, low bias: not reliable but valid\nLow variance, high bias: reliable but not valid\nLow variance, low bias: reliable and valid\nEstimation of reliability\nEstimating the reliability of a research instrument involves several methods, each with its own formula. Here are some common methods:\n1. Test-retest reliability (temporal stability): This method measures the stability of the instrument over time (typically 15 days gap) by administering the same test to the same group of respondents at two different points in time. The reliability coefficient is the correlation between the two sets of scores. Reliability is satisfied if r &gt; 0.7.\n2. Parallel-forms reliability (equivalent or stability over item samples): This method involves creating two different forms of the same test that are equivalent in content and difficulty. Both forms are administered to the same group, and the scores are used to calculate the correlation coefficient to express the reliability. Reliability is satisfied if r &gt; 0.7.\n3. Inter-rater reliability (stability over scores): This method assesses the consistency of scores assigned by different raters. It is often used in qualitative research or subjective assessments. The reliability is calculated by correlating the scores given by different raters. Cohen’s Kappa values less than or equal to 0 indicate no agreement, 0.01–0.20 denote none to slight agreement, 0.21–0.40 indicate fair agreement, 0.41–0.60 signify moderate agreement, 0.61–0.80 represent substantial agreement, and 0.81–1.00 denote almost perfect agreement.\n\n4. Split-half or Internal consistency reliability (stability of items):\nThis method evaluates the homogeneity or consistency of results across items within a test. In this method, the test items are divided into two (odd numbered and even numbered). If the correlation coefficient between these two sets &gt; 0.7, the scale is said to be reliable. Besides, we can measure the internal consistency of the scale is measured using Cronbach’s alpha, which assesses the average correlation among all items. However, split-half reliability is measured by correlation coefficient, Spearman-Brown Prophecy or Kudar-Richardson formula where at least 0.7 value is considered as reliable.\n\n\n\n\n\n\n\n\n\n\nEstimation of Validity\nContent validity: Content validity ensures the instrument covers the entire range of the concept being measured. It is usually evaluated qualitatively by experts in the field, who review the instrument’s items for relevance and completeness. There is no specific formula for content validity, as it relies on expert judgment rather than statistical analysis.\nFace validity: Face validity refers to the extent to which an instrument appears to measure what it is supposed to measure, based on subjective judgment by a colleague or expert in the field. Like content validity, it does not have a specific formula and relies on expert or user feedback.\nCriterion-related validity: Criterion validity evaluates how well one measure predicts an outcome based on another measure (the criterion). It includes predictive validity and concurrent validity. Predictive validity correlates actual job success (job performance) with previous activities (exam performance). Concurrent validity correlates new test scores (viva-voce) with previous standard test scores (paper-based exam). Pearson correlation coefficient is calculated to measure the validity.\nConstruct validity: A construct is a theoretical concept that is being measured or assessed. It represents an abstract idea or characteristic that cannot be directly observed but can be inferred from observable behaviors, responses, or indicators. Examples of constructs include intelligence, satisfaction, anxiety, and motivation. Construct validity assesses whether the instrument truly measures the theoretical construct it is intended to measure. It involves both convergent and discriminant validity, often evaluated through factor analysis. Convergent validity assesses whether two measures that are supposed to be related are actually related. Discriminant validity checks that measures supposed to be unrelated are indeed unrelated.\n\nEigenvalues and eigenvectors: These are computed to determine the principal components.\nFactor loadings: Represent the correlations between observed variables and the latent factors.\n\nAverage variance extracted (AVE) is calculated to estimate the convergent and discriminant validity with the following formula:\n\nAn Average Variance Extracted (AVE) value of 0.50 or higher is generally considered acceptable for demonstrating the validity of a research instrument. This threshold indicates that the construct explains at least 50% of the variance in its indicators, suggesting adequate convergent validity. For discriminant validity, the AVE of a construct (e.g. customer satisfaction) should be greater than the highest squared correlation with any other construct (e.g. customer loyalty) in the model. This means that the AVE for each construct should exceed the squared inter-construct correlations, indicating that the construct shares more variance with its own indicators than with other constructs. The goal is to demonstrate that each construct measures a unique aspect of the theoretical framework, therefore, they should not be closely related."
  },
  {
    "objectID": "courses/research-methodology.html#data-and-its-types",
    "href": "courses/research-methodology.html#data-and-its-types",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Data refers to raw, unprocessed facts and figures without context, such as numbers, text, or images. It is the basic input that can be collected and stored but lacks meaning on its own. Information, on the other hand, is data that has been processed, organized, or structured to provide context and meaning. It is useful for decision-making, understanding, and knowledge creation. For instance, a list of numbers is data, but when these numbers are analyzed and interpreted as sales figures over a month, they become information that can inform business strategies. Research data refers to the collected observations or measurements gathered during a research study. It serves as the foundation for analysis and conclusions, supporting the validation of hypotheses, theories, or findings.\nTypes of data\n\nQualitative data – express description. Example: It is beneficial, the flower color is red, the family is a large family.\nQuantitative data – numerical information\n\nDiscrete data – It is counted, can take only full integers. Example: Number of farmers, family size, number of crops.\nContinuous data – It is measured, can take any values including decimals). Example: Height, weight, distance.\n\n\n\n\nQualitative data\n\nIt’s color is black.\nIt has long hair.\nIt has lots of energy.\n\n\n\n\nQuantitative data\n\nDiscrete:\n\nIt has 4 legs.\nIt has 2 eyes.\n\nContinuous:\n\nIt weighs 7 kg.\nIt is 30 cm tall."
  },
  {
    "objectID": "courses/research-methodology.html#variables",
    "href": "courses/research-methodology.html#variables",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Features refer to the distinctive characteristics or properties of the subjects under study. In social science, a feature could be the educational level of individuals in a population. In agricultural science, it might be the soil type in different farming regions. In natural science, a feature could be the temperature range of a habitat.\nVariables are elements that can change or be changed within an experiment or study. A variable in a research study is any factor, trait, or condition that can exist in differing amounts or types. They are often categorized as independent, dependent, or controlled. In social science, an independent variable could be the teaching method used in schools, while the dependent variable might be student performance. In agricultural science, the amount of fertilizer applied (independent variable) could affect crop yield (dependent variable). In natural science, sunlight exposure (independent variable) might impact plant growth (dependent variable).\nControlled variables, also known as constants, are kept unchanged to ensure that the effect on the dependent variable is due to the manipulation of the independent variable. For example, in a study on the effect of fertilizer on plant growth, the fertilizer amount is the independent variable, plant growth is the dependent variable, and factors like soil type and watering frequency are controlled variables.\nTypes of variables\nVariables that can only exist in two opposite states are known as attributes. Variables refer to the quantifiable traits or properties of the subjects being examined. Besides dependent and independent variables, there are some other types of variables. Similar to the types of data, variables are classified as qualitative, quantitative (discrete or continuous). The variables having only two values, e.g. male/female, yes/no, are termed as dichotomous or binary variable. The major jargon of variables are listed here:\n\nDependent/target/response/criterion/predicted variable: the phenomenon hypothesized to be the outcome, effect, consequence or output of some input variables. e.g. knowledge on IPM\nIndependent/regressor/input/stimulus/determinant/predictor variable: the phenomenon hypothesized to be the input or antecedent of the effect or outcome. e.g. educational status\nIntervening/ hidden variable: immeasurable variable which is hypothesized to exist and can help to explain the relationship between the dependent and independent variables. e.g. intelligence\nExtraneous variable: those measurable independent variables that are not related to the purpose of the study but may affect the dependent variable. e.g. cosmopolitanism\nEndogenous variable: An independent variable is called an endogenous variable when it can be be affected by the dependent variable. In this case, the endogenous variable shows correlation with the residuals.\nInstrumental variable: an alternative variable used instead of another variable that shows endogeneity issues with the dependent variable in a regression analysis."
  },
  {
    "objectID": "courses/research-methodology.html#hypothesis",
    "href": "courses/research-methodology.html#hypothesis",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "A hypothesis is a intelligent guess or testable statement predicting the relationship between variables. It proposes a potential outcome based on prior knowledge or theories.\nFor example, a researcher might hypothesize that “Applying nitrogen fertilizer will increase the yield of maize crops.” This hypothesis can be tested through experiments by applying nitrogen fertilizer to some maize crops (the experimental group) and not to others (the control group), and then measuring the yield differences. The results will either support or refute the hypothesis, contributing to the understanding of the effects of nitrogen fertilizer on maize production.\nIn social science, a researcher might hypothesize that “Students who participate in extracurricular activities have higher academic performance than those who do not.” This hypothesis can be tested by collecting data on students’ involvement in extracurricular activities and their academic grades, then analyzing the correlation between these variables. The findings will either support or refute the hypothesis, thereby contributing to the understanding of the impact of extracurricular activities on academic success.\n\n\n\nStates the Expected Relationship Between Variables: Clearly defines the interaction or association between the variables involved.\nTestable with Operationally Defined Variables: Specifies variables in measurable terms, ensuring they can be empirically tested.\nClear Implications for Testing Stated Relations: Provides explicit directions for how the relationship between variables can be examined and tested.\nExplanatory Power: Offers insights or explanations about the phenomena, contributing to the understanding of the subject matter.\nConsistency with Existing Knowledge: Aligns with and builds upon the current body of knowledge and theories in the field.\nSimplicity and Conciseness: Articulates the hypothesis in a straightforward and succinct manner.\nAppropriate Scope: Balances specificity and generality; not too broad to be untestable nor too narrow to be insignificant.\n\n\n\n\n\nProvides a Tentative Explanation of Phenomena: A hypothesis offers an initial explanation for observed phenomena, serving as a starting point for further investigation. By proposing potential relationships or effects, it facilitates the extension of knowledge in a specific area.\nOffers a Directly Testable Relational Statement: A hypothesis presents a clear, testable statement that defines the relationship between variables. This allows researchers to design experiments and studies that can empirically test these relationships, providing concrete evidence for or against the hypothesis.\nGuides the Research: Hypotheses give direction to the research process by focusing on specific variables and their potential interactions. This helps in formulating research questions, selecting methodologies, and determining the scope of the study.\nFramework for Reporting Conclusions: Hypotheses provide a structured framework for interpreting and reporting the results of a study. Researchers can compare their findings against the hypothesis to draw conclusions, making the research process more systematic and coherent.\nFirst Level of Theory Development: Hypothesis formulation is a crucial step in theory development. By proposing testable statements, hypotheses lay the groundwork for building broader theoretical frameworks that explain and predict phenomena in a given field.\nObjectivity in Testing: Hypotheses allow for objective testing that is independent of personal values and opinions. They can be empirically evaluated and shown to be supported or not supported based on evidence, ensuring that conclusions are based on data rather than subjective beliefs.\nEnhances Critical Thinking and Innovation: Formulating and testing hypotheses encourage critical thinking and innovation. Researchers must think creatively and analytically to develop plausible hypotheses and design experiments to test them, fostering scientific advancement.\n\n\n\n\nHypotheses can be classified into two main types:\n\nNull Hypothesis\nResearch Hypothesis\n\nNull Hypothesis: A null hypothesis is a formal negative statement predicting no relationship between two or more variables, typically denoted by H0​ or HN​. For example, H0​​: There is no relationship between anxiety and IQ levels of children. The null hypothesis states the opposite of what the researcher expects or predicts. The final conclusion of the study will either retain the null hypothesis or reject it in favor of an alternative hypothesis. Not rejecting H0​​ does not necessarily mean that H0​​ is true; it may simply indicate insufficient evidence against it.\nResearch Hypothesis: A research hypothesis is a formal affirmative statement predicting a relationship between two or more variables, also known as the scientific or alternative hypothesis, denoted by Ha​. For example, Ha: There is a relationship between anxiety and IQ levels of children. This hypothesis often arises from prior literature or studies and is established when the null hypothesis is rejected. The research hypothesis is usually the desired conclusion of the investigator.\nHypotheses can further be categorized as directional or non-directional.\nDirectional Hypothesis:\n\nSpecifies the expected direction of the findings.\nOften used to examine relationships among variables rather than comparing groups.\nExample: “Children with high IQ will exhibit more anxiety than children with low IQ.”\n\nNon-directional Hypothesis:\n\nDoes not specify the expected direction of the findings.\nUsed when the researcher is unsure of what to predict based on past literature.\nExample: “There is a difference in the anxiety levels of children with high IQ and those with low IQ.”\n\n\n\n\nHypothesis testing is a method for making rational decisions about the presence of effects based on empirical data. Hypotheses are tested using observed facts. When the test statistic falls within the acceptance region, the hypothesis is accepted; if it falls within the rejection region, the hypothesis is rejected. Before testing a hypothesis, we must select a significance level based on which we will reject the null hypothesis. Then we have select appropriate statistical tests based on the distribution and nature of the variables of interest.\nThe decision regarding hypothesis testing is based on the calculated value of the statistic in relation to the null hypothesis. The strength of the sample data is evaluated to determine if the null hypothesis should be rejected. If the calculated value exceeds the critical table value, the result is significant, and the null hypothesis is rejected. Conversely, if the calculated value is less than the table value, the result is non-significant, and the null hypothesis is not rejected.\nEven if hypotheses are not confirmed, they hold value (Kerlinger, 1956). Negative findings are as significant as positive ones, as they reduce ignorance and can highlight new hypotheses and avenues for research. Hypotheses cannot be conclusively proved or disproved but can be supported or not supported.\nErrors in Hypothesis Testing\nAt the conclusion of hypothesis testing, decisions can be correct or erroneous. Errors in hypothesis testing are categorized into two types:\n\nType I Error: Rejecting a null hypothesis when it is true.\nType II Error: Not rejecting a null hypothesis when it is false.\n\n\n\n\nDecision\nH0​ is True\nH0​ is False\n\n\n\n\nReject\nType I Error\nCorrect Decision\n\n\nAccept\nCorrect Decision\nType II Error\n\n\n\nSignificance Levels and Sample Size\nSetting a higher level of significance reduces the risk of Type I error but increases the risk of Type II error. The researcher must set the level of significance based on the relative severity of making a Type I or Type II error. Increasing the sample size can reduce the probability of both types of errors.\n\n\n\n\n\n\n\n\nAspect\nType I Error\nType II Error\n\n\nDefinition\nRejecting a true null hypothesis (false positive).\nFailing to reject a false null hypothesis (false negative).\n\n\nExample\nConcluding a new drug is effective when it is not.\nConcluding a new drug is ineffective when it is effective.\n\n\nSymbol\nα (alpha)\nβ (beta)\n\n\nConsequence\nTaking action based on incorrect assumptions.\nMissing out on a potentially beneficial action or intervention.\n\n\nRisk Perception\nOften seen as less dangerous because it leads to further scrutiny and testing.\nOften seen as more dangerous because it can result in missed opportunities and prolonged issues.\n\n\nReal-world Impact\nApproving a harmful or ineffective treatment.\nNot approving a beneficial treatment, leading to continued suffering.\n\n\nDanger Comparison\nType I errors can be mitigated by further research and validation.\nType II errors can prolong the problem, leading to significant negative consequences.\n\n\nMinimization option\nBy increasing alpha level.\nBy increasing sample size.\n\n\n\nType II error is often more dangerous. This type of error occurs when a researcher fails to reject a false null hypothesis, meaning that a potentially effective agricultural practice or intervention is dismissed as ineffective. Imagine a study evaluating the effectiveness of a new drought-resistant crop variety. The null hypothesis (H0) is that the new crop variety is no more effective than the existing varieties in terms of drought resistance. A Type II error in this context would mean concluding that the new crop variety is not more effective in resisting drought when it actually is. Farmers continue using the less effective existing varieties, leading to lower yields and higher susceptibility to drought conditions. This results in economic losses due to reduced productivity. The broader agricultural sector might miss out on improvements in crop yields and resilience, which could have strengthened food security and reduced economic losses during drought periods. Lower crop yields during droughts can lead to food shortages, impacting food security for communities that rely heavily on agriculture.\nOne-tailed vs two-tailed test\nIn hypothesis testing, the concepts of one-tailed and two-tailed tests are crucial. A one-tailed test is used when the rejection area is located in only one tail of the distribution, either on the left or right side. Conversely, a two-tailed test is used when the rejection areas are on both tails of the distribution. The choice between a one-tailed and a two-tailed test depends on how the null hypothesis is formulated. A one-tailed test is appropriate for testing a directional hypothesis, which indicates a specific direction of difference, such as superiority or inferiority between two groups. For example, a one-tailed hypothesis could be: H0​: Athletes do not have higher IQs than non-athletes. You can see the dynamics of probability (p-value: area below the quantile) and quantile (x) from this link https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html.\n\n\n\n\n\n\n\n\n\nFeature\nOne-Tailed Test\nTwo-Tailed Test\n\n\nDefinition\nExamines the direction of an effect\nExamines if there is any significant difference in either direction\n\n\nUse\nApplied when the hypothesis predicts a specific direction of the effect\nApplied when the hypothesis does not predict the direction of the effect\n\n\nHypotheses Example\nH0​: The new fertilizer does not increase yield (yield ≤ current yield)\nHa​: The new fertilizer increases yield (yield &gt; current yield)\nH0​​: The new teaching method does not affect performance (performance = current method)\nHa​​: The new teaching method affects performance (performance ≠ current method)\n\n\nTesting Direction\nOnly one direction (greater than or less than)\nBoth directions (either greater or less)\n\n\nPower\nMore powerful for detecting an effect in a specific direction\nMore conservative, allows for detection of effects in both directions"
  },
  {
    "objectID": "courses/research-methodology.html#data-processing-and-analysis",
    "href": "courses/research-methodology.html#data-processing-and-analysis",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Data editing is done in two steps: field editing and central editing. Field editing is done at the field immediately after interviews to clarify any queries, complete answers, input missed answers, or correct wrongly written notes if necessary. Central editing is done after completing all the data collection during data entry and compilation if any inconsistency is noticed. The collected data is entered in an Excel sheet or in a paper-based master sheet. This process is called data tabulation. During data tabulation numerical values are assigned to qualitative data, which is called coding. The coding helps apply mathematical operations and counting easier. For example, 1 for yes, 2 for no; 1 for male, 2 for female; 1 for low, 2 for medium, 3 for high.\nCategorization means classification of data based on their class intervals. This reduces the number of levels in discrete or continuous data. Categorization must fulfill two conditions: mutually exclusive and completely exhaustive. Mutually exclusive means an observation will fall one and only once category. Completely exhaustive means all the categories will accommodate all the data and no observations will remain unclassified.\nExample of categorization: Data was collected on a group of 50 farmers’ age in years. The 50 observations are classified as follows where each farmer falls in only one of the category (mutually exclusive) and all 50 farmers are classified (completely exhaustive).\n\n\n\nCategory\nRange (years)\nFrequency\n\n\nYoung\nUp to 35\n15\n\n\nMiddle-aged\n&gt; 35 to 50\n25\n\n\nOld\n&gt; 50\n10\n\n\n\n\n\n\nSelection of suitable statistical tests and analyses depends on the purpose, data types and distribution properties. A brief overview of the selection criteria and related assumptions are stated here.\nPurpose 1: Exploration of data or cases or respondents is done using descriptive statistics, e.g. mean, standard deviation, standard error, coefficient of variation, frequency or percentage distribution, bar graph, line graph, boxplot, and group boxplot.\nPurpose 2: Difference testing between groups:\nDependent variable ——&gt; continuous\n\ngroup = 2: t-test (data normal, equal variance)\ngroup = 2: Welch test (data normal, unequal variance)\ngroup = 2: Wilcoxon test (non-parametric test, data not normal, n&lt;10)\ngroup &gt; 2: ANOVA (data normal, equal variance)\ngroup &gt; 2: Kruskal-Wallis test (data not normal\n\nDependent variable ——&gt; categorical\n\nChi-square test: cell frequency &gt; 5 for &gt; 20% cells\nFisher’s exact test: cell frequency assumptions not required\n\nPurpose 3: Testing relationship between variables is can be done by either parametric or non-parametric tests. Parametric analysis requires the data follows specific distribution otherwise the result will not be reliable. Non-parametric tests do not require such assumptions.\nPearson correlation coefficient (r) is calculated as a parametric test. In this case, the assumptions are: data should be normally distributed and both variables are continuous or equal interval (not ordinal, not nominal). If the data does not meet these criteria or the data is rank data, Spearman rank correlation (ρ) is calculated. However, for categorical variables Chi-square or Fisher’s exact test is used.\n\nChi-square test: data categorical, cell frequency &gt;5 for &gt;20% cells\nFisher’s exact test: data categorical, cell frequency &lt;5 for &gt;20% cells\nCramer’s V is calculated to understand the strength of association in Chi-square or Fisher’s exact test\nCramer’s V is a scaled version (values ranges 0 – 1) similar to r values\n\nPurpose 4: Causal factor identification:\n\nSimple regression: one independent variable\nMultiple regression: multiple independent variable\nPoisson regression: dependent variable is frequency\nCensored dependent variable: Tobit regression, censored Poisson regression\nStepwise regression: variable selection\nLogistic regression (logit/probit): dependent variable binary\nMultinomial logit: dependent variable has &gt; 2 categories\nPolynomial regression: relationship is not linear\nMachine learning: decision making for mainly categorical variables\n\n\n\n\n\n1. R\n\nOverview: R is a free, open-source programming language and software environment used for statistical computing and graphics. It is highly extensible, with a vast repository of packages available via CRAN (Comprehensive R Archive Network).\nStrengths:\n\nFlexibility: R excels in statistical analysis and graphical models. Users can perform a wide range of statistical tests and create publication-quality plots with packages like ggplot2.\nCommunity Support: Being open-source, R benefits from a large, active community of users and developers, constantly contributing new packages and updates.\nReproducibility: Scripts and markdown documents ensure that analyses can be replicated and shared easily.\n\nWeaknesses:\n\nLearning Curve: R can be challenging for beginners due to its steep learning curve and the necessity of understanding programming concepts.\nPerformance: For very large datasets, R can be slower compared to some other statistical software.\n\n\n2. Python\n\nOverview: Python is a versatile, high-level programming language known for its simplicity and readability. In the context of statistics, libraries like Pandas, NumPy, SciPy, and statsmodels, along with visualization tools like Matplotlib and Seaborn, make Python a powerful tool.\nStrengths:\n\nEase of Use: Python’s syntax is intuitive and user-friendly, making it accessible for beginners and those familiar with programming.\nIntegration: Python integrates well with other programming languages and software, making it suitable for a wide range of applications beyond statistics, such as machine learning (via scikit-learn) and data science.\nCommunity and Resources: Python has a robust community and extensive documentation, tutorials, and forums to support users.\n\nWeaknesses:\n\nStatistical Packages: While Python is strong overall, its statistical libraries are not as specialized or comprehensive as those available in R.\nPerformance: Like R, Python can struggle with performance issues when handling extremely large datasets without the use of specialized libraries or frameworks.\n\n\n3. SPSS (Statistical Package for the Social Sciences)\n\nOverview: SPSS is a software package used for interactive, or batched, statistical analysis. It is widely used in social sciences, health sciences, and market research.\nStrengths:\n\nUser-Friendly Interface: SPSS is known for its easy-to-use, menu-driven interface, which is ideal for users without a programming background.\nComprehensive Features: SPSS offers a wide range of statistical procedures, from basic descriptive statistics to complex multivariate analyses.\nIntegration: It integrates well with other IBM products and can handle large datasets efficiently.\n\nWeaknesses:\n\nCost: SPSS is a commercial product with a significant cost, which can be a barrier for some users.\nFlexibility: While powerful, SPSS is less flexible than programming languages like R and Python, particularly for custom analyses.\n\n\n4. STATA\n\nOverview: STATA is a powerful statistical software used for data management, statistical analysis, and graphical representation. It is popular in economics, sociology, and political science.\nStrengths:\n\nEase of Learning: STATA’s command syntax is straightforward and easier to learn for those new to programming.\nComprehensive Data Management: STATA excels in handling large datasets and complex data management tasks.\nAdvanced Statistical Procedures: It offers advanced statistical techniques, particularly in econometrics.\n\nWeaknesses:\n\nCost: Like SPSS, STATA is a commercial software with substantial licensing fees.\nLess Community Support: While it has a dedicated user base, STATA’s community is smaller compared to R and Python.\n\n\n5. SAS (Statistical Analysis System)\n\nOverview: SAS is a software suite developed for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\nStrengths:\n\nIndustry Standard: Widely used in corporate and governmental settings for its reliability and robustness in handling complex analyses.\nComprehensive Capabilities: SAS provides a broad range of statistical procedures and has strong data handling capabilities.\nSupport and Documentation: SAS offers extensive support and thorough documentation, which is beneficial for enterprise environments.\n\nWeaknesses:\n\nCost: SAS is one of the most expensive statistical software packages, limiting its accessibility to larger organizations.\nSteep Learning Curve: Despite its power, SAS can be difficult to learn, especially for users without a strong background in statistics or programming.\n\n\nEach of these statistical software tools has its unique strengths and weaknesses. R and Python offer flexibility and powerful community support, making them ideal for academic and research settings. SPSS and STATA are user-friendly and highly efficient for social sciences and econometrics, respectively, though their cost can be prohibitive. SAS, while expensive and complex, is a powerhouse in enterprise environments for handling large-scale, complex analyses. The choice of software often depends on the specific needs, budget, and expertise of the user.\n\n\n\nAfter analyzing the data, results are presented using texts, tables and figures without repetition. It means that if any information is presented in a table that should not be presented in a figure in the same report. However, in the texts can repeat the information for the sake of interpretation. Results include texts, tables, figures and their related interpretation. In the discussion step, the results are explained as why they are so (reasons) and how they are related to other studies and real word. When we ask the question ‘SO WHAT?’ to the results and discussion, we get conclusions. Conclusion includes implications of the results to the study areas and population. Finally, suggestions are included in a report in the form of recommendations. One golden rule is that all the sections in a report – introduction, methods, results, discussion and conclusions – are closely linked and consistent. Never add conclusions and recommendations those are not part of your research. In other words, never add any conclusions and recommendations those are not backed up by your data.\nGuidelines of including tables in a report: Table title should be placed on the top of the table. A table must be referred in the text before the table.\n\nNumbering and Titles: Number tables sequentially (e.g., Table 1, Table 2) and provide a clear, descriptive title for each table.\nSelf-Contained: Ensure tables are self-explanatory, containing all necessary information to understand the data without referring to the text.\nHeaders: Label all columns (caption) and rows (stub) clearly with descriptive headers and indicate units of measurement.\nFootnotes: Use footnotes to explain abbreviations, symbols, and any additional information necessary for understanding the table.\nConsistency: Maintain a consistent format and style throughout all tables, including font size and type, borders, and alignment.\nData Presentation: Present data accurately and succinctly, avoiding unnecessary details. Align numerical data to the right for readability.\nReferencing: Reference all tables in the text and discuss key findings presented in the tables within the report.\nJournal Guidelines: Adhere to specific guidelines provided by the journal or institution, which may include formatting rules, size limits, and submission requirements.\nSimplicity: Keep tables simple and uncluttered, avoiding excessive use of lines and shading.\nSoftware Tools: Use appropriate software tools to create tables, ensuring they are professionally formatted and easy to read.\nAvoid ditto marks for the identical texts in the next rows.\nKeep contrasting columns side by side.\nAdd the sum/total column at the right-most column or bottom-most row.\nKeep the table in a single page, if not possible repeat the header row in the following pages.\n\nGuidelines of including figures in a report: Figure title should be placed at the bottom of the figure. A figure must be referred in the text before the figure.\n\nNumbering and Titles: Number figures sequentially (e.g., Figure 1, Figure 2) and provide a concise, descriptive caption beneath each figure.\nReferencing: Refer to all figures in the main text, and ensure the discussion highlights the key information each figure presents.\nClarity and Quality: Ensure figures are of high resolution and clarity. Avoid unnecessary details that may clutter the figure. Use standard fonts and sizes.\nSelf-Contained: Each figure should be self-explanatory. Include legends and labels to explain symbols, lines, or colors used in the figure.\nUnits and Scales: Clearly indicate units of measurement and scales on axes. Use appropriate and consistent scales.\nConsistency: Maintain a consistent style and format across all figures. This includes color schemes, font sizes, and line thicknesses.\nRelevance: Only include figures that are relevant and add value to the report. Each figure should contribute to the understanding of the data and support the main findings.\nPlacement: Place figures close to where they are referenced in the text to aid readability and flow.\nEthical Considerations: Ensure figures are not manipulated in a way that misrepresents the data. Credit the source if you use figures from other works.\nSupplementary Material: If figures are complex or numerous, consider placing detailed versions in supplementary material while keeping simplified versions in the main report.\n\nTypes of charts, graphs and diagrams used as figures in a report\nIn a thesis or scientific article, various types of figures are used to present data and illustrate key concepts. Here are some common types:\n\nGraphs and Charts:\n\nLine Graphs: Show trends over time or continuous data.\nBar Charts: Compare quantities across different categories.\nPie Charts: Display proportions of a whole.\nScatter Plots: Illustrate relationships between two variables.\nHistograms: Show frequency distributions of continuous data.\n\nDiagrams and Schematics:\n\nFlowcharts: Outline processes or workflows.\nCircuit Diagrams: Represent electrical circuits.\nBlock Diagrams: Illustrate system structures or processes.\n\nPhotographs:\n\nShow real-life images of specimens, equipment, or experimental setups.\n\nMaps:\n\nProvide geographical context or spatial distribution of data.\n\nMolecular Structures:\n\nDisplay chemical structures or biological macromolecules.\n\nHeatmaps:\n\nShow intensity of data points over a two-dimensional space, often used in genomics and other biological data.\n\nBox Plots:\n\nSummarize a set of data showing its distribution, median, and outliers.\n\n3D Plots:\n\nRepresent three-dimensional data for better spatial understanding.\n\nInfographics:\n\nCombine images, charts, and minimal text to explain a concept or process in an engaging way.\n\n\nEach type of figure serves a specific purpose, helping to clarify complex information and support the narrative of the scientific work."
  },
  {
    "objectID": "courses/research-methodology.html#format-of-a-scientific-report",
    "href": "courses/research-methodology.html#format-of-a-scientific-report",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Scientific report is a document that contains the justification, investigation techniques and implications of a scientific inquiry. Examples: thesis, scientific articles/notes/commentary in journals. Scientific reports are reviewed by experts before publication to ensure that the policy makers and users will get reliable information about the topic of interest. Style and format of a scientific report can widely vary depending on the academic degrees, institutions, publishers and disciplines. The general format of a scientific reports contains the following five chapters (body of the report) and a few extra sections without sections numbers (abstract, acknowledgments and references, appendix/annex for interview schedule, additional related information):\n\n Abstract\nIntroduction\nMethodology\nResults\nDiscussion\nConclusions\nAcknowledgments\nReferences\nAppendix (if any)\n\nIn a thesis there is a section called preliminaries that contain\n\nCover page (Research title, name of the researcher, degree and institution, submission date)\nDeclaration (The thesis contents and supervisors’ approval)\nAcknowledgments (Thanks to those who are not author but helped in the research work)\nContents (List of chapters, sections, subsections; List of tables; List of figures)\nAbstract (Summary of the study, 200 – 300 words)\n\nThere is a separate chapter in thesis after the introduction, which is ‘Review of Literature’. This is embedded in the introduction of other types of scientific reports.\nIntroduction of a scientific report\nIntroduction means introducing the research problem and objectives. There are some well accepted and widely practices principles of writing an introduction. In a thesis, subheadings of the introduction are mentioned but in scientific journals those are implicit (hidden). Contents of a good introduction are:\n1. General background or context\n\nStart with general or international perspectives and end with specific or local perspectives towards your research topic.\n\n2. Research problem\n\n   Importance of the topic\n   Existing research/literature about the topic\n   Research gap\n   Conceptual framework\n\n3. Research objectives\n\nResearch questions or objectives specific to the study. These must be SMART (specific, measurable, achievable, rational and timely) and sufficient to fill the research gap.\n\n4. Research contributions\n\nAnticipated research contribution to the community, policymakers, science and existing body of knowledge.\n\nReview of literature of a thesis\nOutput of review of literature is to design a conceptual framework of a research work. It contains related theories and existing research outputs in relation to the study variables.\nMethodology\n\nLocation of the study where the respondents and problems are located.\nMethodological framework\nSampling frame (part of population) and techniques\nVariables and their measurements\nData collection methods\nData processing and analytical frameworks\nSoftware used for the analysis.\n\nResults/findings and discussions\nResults/findings are the output of research. This section objectively present results/findings using text, figures and tables. What has been found is the result and why this has been found is the discussion.\nConclusions\nConclusions are the implications of the results. So what is explained in this section with a generalization over the population based on the sample study. What could be the impact of the findings are concisely written in conclusion without any references. Recommendations are merged with conclusions. Recommendations include what to do to overcome the problems and what should be included in further research."
  },
  {
    "objectID": "courses/research-methodology.html#citations-references-and-footnotes",
    "href": "courses/research-methodology.html#citations-references-and-footnotes",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "There are numerous styles of citations, footnotes, references and bibliography. Examples\nAPA: American Psychological Association (APA), mostly followed by social sciences. Please visit the linkfor detail APA updates and quick guides.\nHarvard: Author-date style, mostly followed by Elsevier, Springer and many other journals.\nEEE: Electronic and Electrical Engineering and computer science (EEE), numbered citations\nCitations are placed in the texts (body of the article). Example: Climatic impacts on agriculture are inevitable. El Niño Southern Oscillation itself, through droughts and floods, can cause 15 to 35% variation in global yield in wheat, oilseeds and coarse grains (Howden et al. 2007). Habiba et al. (2012) and Roy et al. (2018) conducted research in the drought-prone areas of Bangladesh.\nReferences are the detailed sources placed at the end of the report. These are alphabetically arranged, except EEE where references are arranged a sequence as they in the text. Example:\n\nHabiba, U., Shaw, R., & Takeuchi, Y. (2012). Farmer’s perception and adaptation practices to cope with drought: perspectives from northwestern Bangladesh. International Journal of Disaster Risk Reduction, 1, 72–84. https://doi.org/10.1016/j. ijdrr.2012.05.004\nHowden, S.M., Soussana, J.F., Tubiello, F.N., Chhetri, N., Dunlop, M., & Meinke, H. (2007). Adapting agriculture to climate change. PNAS 104, 19691–19696. https://doi.org/10.1073/pnas.0701890104\nRoy, D, Kowsari, M.S., Nath, T.D., Taiyebi, K.A., & Rashid, M.M. (2018). Smallholder farmers’ perception to climate change impact on crop production: case from drought prone areas of Bangladesh. International Journal of Agricultural Technology, 14, 1813–1828\n\nNote: References must contain only those sources that have been used in the text. References must include all the sources used in the text. Bibliography may contain additional related sources.\nUse Mendeley(free), Zotero(free), or Endnote(paid) reference manager for managing and changing styles. Create an account at Mendeley website and watch in YouTube for more information.\nSources and components of literature and information\n\nBooks: author(s), year, title, publisher, place\nJournal articles: author(s), year, title, journal, volume (issue): page, doi (digital object identifier) number\nNewspaper articles: author(s), year, title, newspaper, publication date\nConference proceedings: authors(s), year, title, conference, date, page\nChapter of an edited book: author(s), year, chapter title, editors, book title, page, publisher, place\nWebpage: authors(s), year, title, access date, web address\nAnonymous (n.d.) when author and year are unavailable.\nAnonymous (2021) when only author is unavailable.\nHasan et al. (n.d.) when only date is unavailable.\n\n‘et al.’ is a Latin phrase et alia (and others), which is usually used when there are more than two authors.\nA footnote is a note placed at the bottom of a page in a document that provides additional information or citations related to the text on that page. Footnotes are used to:\n\nCite sources of information, data, or quotations used in the text.\nProvide additional context or explanations that would be too lengthy or distracting to include in the main text.\nInclude comments or references that are relevant to the text but do not fit smoothly into the main narrative.\n\nFootnotes are typically indicated by a superscript number or symbol in the text, which corresponds to a matching number or symbol at the bottom of the page where the footnote is provided. Here is an example of how footnotes are used:\nIn the main text: “According to recent studies, the population of tigers in the wild has decreased significantly over the past decade.^1”\nAt the bottom of the page: “1. Smith, John. The Decline of Tigers in the Wild. Wildlife Conservation Journal, 2023, pp. 45-50.”\nFootnotes help keep the main text concise and focused while still providing necessary citations and additional information for readers who want to delve deeper into the topic."
  },
  {
    "objectID": "courses/research-methodology.html#research-plagiarism-and-ai-artificial-intelligence",
    "href": "courses/research-methodology.html#research-plagiarism-and-ai-artificial-intelligence",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Plagiarism is the inclusion of someone else ideas, text and results without acknowledgment is unethical and punishable academic crime. Use quote or paraphrased statements with proper citations of the original authors (even for your own previous works) to avoid plagiarism. The Turnitin software is used to detect plagiarism. Our PSTU has its subscription and you can use it with the help from the library.\nIn the age of the 4th Industrial Revolution, we must be familiar with AI. At the same time we must be aware of its dangerous sides. It is a wolf in sheep’s cloths. So, my suggestion is to use AI cautiously and creatively. Some AI tools include ChatGPT(general purpose tool), Elicit(review of literature tool), Quillbot(paraphrasing tool). If you use AI in your research, you need to acknowledge it by mentioning why you have used it. For example AI tools have been used to improve grammar and paraphrasing.\nWe have also to be technologically sound in the academic and research world. Proficiency in MS Word, Excel, PowerPoint, R Program, Canva, Slido, Miroboard, Kahoot, Zoom, Google Meet, Microsoft Team, Google Drive, ResearchGate, Google Scholar, Scopus Researcher ID, Web of Science, ORCIDand the similar software and online platforms are some additional capital assets for a researcher."
  },
  {
    "objectID": "courses/research-methodology.html#references",
    "href": "courses/research-methodology.html#references",
    "title": "Understanding Research Methodology: A Comprehensive Guide",
    "section": "",
    "text": "Kothari, C. R. (2013). Research Methodology: Methods and Techniques. New Delhi: New Age International.\nEdwards, A. L. (1957). Techniques of Attitude Scale Construction. Appleton-Century-Crofts.\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2010). Multivariate Data Analysis. 7th ed. Prentice Hall.\n\n\n\n\nHasan, M. K. (2024). Understanding Research Methodology: A Comprehensive Guide. Accessed [date of access] from www.ruenresearch.com/courses/research-methodology"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#why-r",
    "href": "courses/presentations/data-analysis-presentation.html#why-r",
    "title": "Data Analysis Using R",
    "section": "Why R?",
    "text": "Why R?\n\nR is a free software environment for statistical computing and graphics.\nIt is widely used among statisticians and data miners for developing statistical software and data analysis.\nR is highly extensible and has a large number of packages for various statistical techniques.\nR is command driven, so its potentiality is literally unlimited.\nCode sharing is easy and reproducibility is ensured."
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#installation",
    "href": "courses/presentations/data-analysis-presentation.html#installation",
    "title": "Data Analysis Using R",
    "section": "Installation",
    "text": "Installation\n\nDownload R and RStudio from R and RStudio.\nInstall R and RStudio, which is an integrated development environment (IDE) for R.\nOpen RStudio and set up your working directory using and other options in the Tools &gt; Global Options…\nExplore and play with RStudio interface, including the console, script editor, environment pane, and plots pane.\nInstruction link"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#what-will-we-cover",
    "href": "courses/presentations/data-analysis-presentation.html#what-will-we-cover",
    "title": "Data Analysis Using R",
    "section": "What will we cover?",
    "text": "What will we cover?\n\nResearch design and data collection\nData entry and cleaning\nBasic R syntax and data types\nData structures: vectors, matrices, lists, and data frames\nImporting and exporting data\nData manipulation with dplyr and tidyr\nData visualization with ggplot2\nStatistical analysis: chi-square test, t-tests, ANOVA, correlation, regression"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#basic-r-syntax",
    "href": "courses/presentations/data-analysis-presentation.html#basic-r-syntax",
    "title": "Data Analysis Using R",
    "section": "Basic R Syntax",
    "text": "Basic R Syntax\n\nCodeOutput\n\n\n# This is a comment\nx &lt;- 5  # Assigning a value to a variable x\n3 -&gt; y  # Another way to assign a value to y\nz = 8 # Simple way of assigning a value to z\nx # Print the value of x\ny # Print the value of y\nz # Print the value of z\nx + y  # Perform addition\nx * y  # Perform multiplication\nx / y  # Perform division\nx^2  # Square of x\nsqrt(x)  # Square root of x\nx &gt; y  # Check if x is greater than y\nx == y  # Check if x is equal to y\nx != y  # Check if x is not equal to y\ny %/% x  # Integer division\ny %% x  # Modulus operation\n\n\n\n\n[1] 5\n\n\n[1] 3\n\n\n[1] 8\n\n\n[1] 8\n\n\n[1] 15\n\n\n[1] 1.666667\n\n\n[1] 25\n\n\n[1] 2.236068\n\n\n[1] TRUE\n\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n[1] 0\n\n\n[1] 3"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#basic-r-syntax-1",
    "href": "courses/presentations/data-analysis-presentation.html#basic-r-syntax-1",
    "title": "Data Analysis Using R",
    "section": "Basic R Syntax",
    "text": "Basic R Syntax\n\nCodeOutput\n\n\nlog(1000) # Natural logarithm of 1000\nlog10(1000) # Base 10 logarithm of 1000\nsin(pi/2) # Sine of pi/2\n\ndegree = 45\nradian = degree * (pi / 180) # Convert degrees to radians\ntan(radian) # Tangent of the angle in radians\n\n\n\n\n[1] 6.907755\n\n\n[1] 3\n\n\n[1] 1\n\n\n[1] 1"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#data-structures",
    "href": "courses/presentations/data-analysis-presentation.html#data-structures",
    "title": "Data Analysis Using R",
    "section": "Data Structures",
    "text": "Data Structures\n\nData codesIndex codesAccessed elements\n\n\n# Vectors\nv = c(1, 2, 3, 4, 5)  # Create a numeric vector\nv_char = c(\"a\", \"b\", \"c\")  # Create a character vector\n# Matrices\nm = matrix(1:9, nrow=3, ncol=3)  # Create a 3x3 matrix\nm2 = matrix(1:12, nrow=3, ncol=4)  # Create a 3x4 matrix\n# Lists\nlist = list(name=\"John\", age=30, scores=c(90, 85, 88))  # Create a list\n# Data Frames by combing vectors\nname = c(\"Alice\", \"Bob\", \"Charlie\")\nage = c(25, 30, 35)\nscores = c(90, 85, 88)\ndf = data.frame(name, age, scores)  # Create a data frame\n\n\nv = c(1, 2, 3, 4, 5) # Accessing elements in vectors\nv[1]  # First element\nv[2:4]  # Elements from index 2 to 4\n\nm = matrix(1:9, nrow=3, ncol=3) # Accessing elements in matrices\nm[1, 2]  # Element in first row, second column\nm[2, ]  # Second row\nm[, 3]  # Third column\n# Accessing elements in lists\nlist = list(name=\"John\", age=30, scores=c(90, 85, 88))\nlist$name  # Access the 'name' element\nlist[[2]]  # Access the second element (age)\n# Accessing elements in data frames\ndf = data.frame(name=c(\"Alice\", \"Bob\", \"Charlie\"), age=c(25, 30, 35), scores=c(90, 85, 88))\ndf$name  # Access the 'name' column\ndf[1, ]  # Access the first row\ndf[2, \"age\"]  # Access the 'age' of the second row\n\n\n\n\n[1] 1\n\n\n[1] 2 3 4\n\n\n[1] 4\n\n\n[1] 2 5 8\n\n\n[1] 7 8 9\n\n\n[1] \"John\"\n\n\n[1] 30\n\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\n   name age scores\n1 Alice  25     90\n\n\n[1] 30"
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#accessing-elements-in-data-frames-using-dplyr",
    "href": "courses/presentations/data-analysis-presentation.html#accessing-elements-in-data-frames-using-dplyr",
    "title": "Data Analysis Using R",
    "section": "Accessing elements in data frames using dplyr",
    "text": "Accessing elements in data frames using dplyr\n\nFirst you need to install the library by install.packages(\"dplyr\").\nThen you can load the library using library(dplyr).\nYou can also manage libraries using the Packages options in Tools in RStudio.\nThe easiest way to managme packages using ‘pacman’ package.\nInstall it using install.packages(\"pacman\").\nThen you can load other packages using pacman::p_load(dplyr, ggplot2, tidyr, car, jtools, kableExtra)."
  },
  {
    "objectID": "courses/presentations/data-analysis-presentation.html#accessing-elements-in-data-frames-using-dplyr-1",
    "href": "courses/presentations/data-analysis-presentation.html#accessing-elements-in-data-frames-using-dplyr-1",
    "title": "Data Analysis Using R",
    "section": "Accessing elements in data frames using dplyr",
    "text": "Accessing elements in data frames using dplyr\n\nCodeOutput\n\n\nlibrary(dplyr)\ndf = data.frame(name=c(\"Alice\", \"Bob\", \"Charlie\"), \n                age=c(25, 30, 35), \n                scores=c(90, 85, 88))\n\ndf %&gt;% \n  filter(age &gt; 28)  # Filter rows where age is greater than 28\n\ndf %&gt;%\n  select(name, scores)  # Select specific columns\n\n\n\n\n     name age scores\n1     Bob  30     85\n2 Charlie  35     88\n\n\n     name scores\n1   Alice     90\n2     Bob     85\n3 Charlie     88\n\n\n\n\n\n\n\n\nRUEN Research"
  },
  {
    "objectID": "courses/lessons-data-analysis-r/Descriptives-chi-sqaure.html",
    "href": "courses/lessons-data-analysis-r/Descriptives-chi-sqaure.html",
    "title": "R markdown, Basic functions, Package (tidyverse) installation, Visualization using ggplot()",
    "section": "",
    "text": "Reading data sets\nBefore reading a dataset from a folder we need to make sure the dataset is in the working directory. We can set working directory using different methods:\n\nSession &gt; Set Working Directory …..\nTyping command\nFiles tab in the Environment Pane\n\n\ndata = readxl::read_excel('DataSets.xlsx', sheet = 'wrangling', range = 'B7:N507')\nhead(data)\n\n# A tibble: 6 × 13\n    sr.   jan   feb   mar   apr   may   jun   jul   aug   sep   oct   nov   dec\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     0     1     1     0     0     0     0     0     0     0     0     0\n2     2     0     1     1     1     0     0     0     0     0     0     0     0\n3     3     1     0     1     0     1     0     0     0     0     0     0     0\n4     4     0     0     1     1     0     1     0     0     1     0     0     0\n5     5     1     0     0     0     0     0     0     0     0     1     0     0\n6     6     0     1     0     0     0     0     0     0     0     0     0     0\n\n\n\ntail(data)\n\n# A tibble: 6 × 13\n    sr.   jan   feb   mar   apr   may   jun   jul   aug   sep   oct   nov   dec\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   495     0     1     0     1     0     0     1     0     0     0     0     0\n2   496     0     1     1     0     0     0     0     0     0     0     0     0\n3   497     1     0     1     1     1     0     0     0     0     0     0     0\n4   498     0     0     0     1     1     0     1     0     0     0     0     0\n5   499     1     0     0     0     0     0     0     0     0     0     0     1\n6   500     0     0     0     1     0     0     0     0     0     0     0     0\n\n\n\nstr(data)\n\ntibble [500 × 13] (S3: tbl_df/tbl/data.frame)\n $ sr.: num [1:500] 1 2 3 4 5 6 7 8 9 10 ...\n $ jan: num [1:500] 0 0 1 0 1 0 0 0 1 0 ...\n $ feb: num [1:500] 1 1 0 0 0 1 1 0 0 0 ...\n $ mar: num [1:500] 1 1 1 1 0 0 0 0 1 1 ...\n $ apr: num [1:500] 0 1 0 1 0 0 1 0 0 1 ...\n $ may: num [1:500] 0 0 1 0 0 0 1 0 1 0 ...\n $ jun: num [1:500] 0 0 0 1 0 0 0 0 1 1 ...\n $ jul: num [1:500] 0 0 0 0 0 0 0 0 0 0 ...\n $ aug: num [1:500] 0 0 0 0 0 0 0 1 0 0 ...\n $ sep: num [1:500] 0 0 0 1 0 0 0 0 0 0 ...\n $ oct: num [1:500] 0 0 0 0 1 0 0 0 0 0 ...\n $ nov: num [1:500] 0 0 0 0 0 0 0 1 0 0 ...\n $ dec: num [1:500] 0 0 0 0 0 0 0 0 0 0 ...\n\n\n\nsummary(data)\n\n      sr.             jan             feb             mar       \n Min.   :  1.0   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:125.8   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n Median :250.5   Median :0.000   Median :0.000   Median :0.000  \n Mean   :250.5   Mean   :0.286   Mean   :0.354   Mean   :0.392  \n 3rd Qu.:375.2   3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000  \n Max.   :500.0   Max.   :1.000   Max.   :1.000   Max.   :1.000  \n      apr             may             jun             jul            aug      \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.00   Min.   :0.00  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.00   1st Qu.:0.00  \n Median :0.000   Median :0.000   Median :0.000   Median :0.00   Median :0.00  \n Mean   :0.358   Mean   :0.224   Mean   :0.218   Mean   :0.18   Mean   :0.09  \n 3rd Qu.:1.000   3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:0.00   3rd Qu.:0.00  \n Max.   :1.000   Max.   :1.000   Max.   :1.000   Max.   :1.00   Max.   :1.00  \n      sep             oct            nov             dec       \n Min.   :0.000   Min.   :0.00   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:0.00   1st Qu.:0.000   1st Qu.:0.000  \n Median :0.000   Median :0.00   Median :0.000   Median :0.000  \n Mean   :0.082   Mean   :0.08   Mean   :0.076   Mean   :0.042  \n 3rd Qu.:0.000   3rd Qu.:0.00   3rd Qu.:0.000   3rd Qu.:0.000  \n Max.   :1.000   Max.   :1.00   Max.   :1.000   Max.   :1.000  \n\n\nWe will subset the dataset by excluding the Sr. variable (1st variable)\n\ndata1 = data[ , -1] \nhead(data1)\n\n# A tibble: 6 × 12\n    jan   feb   mar   apr   may   jun   jul   aug   sep   oct   nov   dec\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     1     1     0     0     0     0     0     0     0     0     0\n2     0     1     1     1     0     0     0     0     0     0     0     0\n3     1     0     1     0     1     0     0     0     0     0     0     0\n4     0     0     1     1     0     1     0     0     1     0     0     0\n5     1     0     0     0     0     0     0     0     0     1     0     0\n6     0     1     0     0     0     0     0     0     0     0     0     0\n\n\nCalculate the column frequencies (summation) using colSums()\n\ncolSums(data1)\n\njan feb mar apr may jun jul aug sep oct nov dec \n143 177 196 179 112 109  90  45  41  40  38  21 \n\n\nCreate an object to store the value of summation.\n\nsums = as.data.frame(colSums(data1))\nstr(sums)\n\n'data.frame':   12 obs. of  1 variable:\n $ colSums(data1): num  143 177 196 179 112 109 90 45 41 40 ...\n\n\n\nrownames(sums)\n\n [1] \"jan\" \"feb\" \"mar\" \"apr\" \"may\" \"jun\" \"jul\" \"aug\" \"sep\" \"oct\" \"nov\" \"dec\"\n\n\nPiping operation: %&gt;% (ctrl+shif+M). It uses the previous output as an input for the next calculation.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.5.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nsums1 = sums %&gt;% rownames_to_column('Months') \nsums1\n\n   Months colSums(data1)\n1     jan            143\n2     feb            177\n3     mar            196\n4     apr            179\n5     may            112\n6     jun            109\n7     jul             90\n8     aug             45\n9     sep             41\n10    oct             40\n11    nov             38\n12    dec             21\n\n\n\nsums2 = sums1 %&gt;% rename(Frequency = `colSums(data1)`)\nsums2\n\n   Months Frequency\n1     jan       143\n2     feb       177\n3     mar       196\n4     apr       179\n5     may       112\n6     jun       109\n7     jul        90\n8     aug        45\n9     sep        41\n10    oct        40\n11    nov        38\n12    dec        21\n\n\nChange order of the x-axis items\n\nsums2$Months = factor(sums2$Months, \n                      levels = c('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec' ), \n                      labels = c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec' ))\n\n\nggplot(sums2)+\n  aes(x = Months, y = Frequency, color = Months, fill = Months)+\n  geom_col()+\n  theme_bw()+\n  labs(title = 'Months of money shortage',\n       x = '',\n       y = 'Number of respondents')+\n  theme(legend.position = '')\n\n\n\n\n\nggsave('My plot.png', dpi = 600, height = 4, width = 8, units = 'in')\n\n\nshortage = as.data.frame(rowSums(data1))\nshort = shortage %&gt;% rename(frequency = `rowSums(data1)`)\nstr(short)\n\n'data.frame':   500 obs. of  1 variable:\n $ frequency: num  2 3 3 4 2 1 3 2 4 3 ...\n\n\n\nhead(short)\n\n  frequency\n1         2\n2         3\n3         3\n4         4\n5         2\n6         1\n\n\nSome descriptives of this frequency variable.\n\nsummary(short)\n\n   frequency    \n Min.   :0.000  \n 1st Qu.:1.000  \n Median :2.000  \n Mean   :2.382  \n 3rd Qu.:3.000  \n Max.   :9.000  \n\n\nTo see the frequency, we need categorical (factor) variable. Convert the numeric variable to factor variable.\n\nshort$frequency = as.factor(short$frequency)\nsummary(short)\n\n   frequency  \n 2      :141  \n 3      :113  \n 1      :110  \n 4      : 69  \n 0      : 34  \n 5      : 21  \n (Other): 12  \n\n\n\nCategorize the respondents based on their number of months with money shortage:\nTwo principles: Mutually exclusive, Completely exhaustive\nLow = up to 3 months of money shortage\nMedium = 4 to 5 months of money shortage\nHigh = &gt;5 months of money shortage\nFor categorization we need numeric (continuous or discrete) variables.\nRight = TRUE, means right value closed, right value is included in the category\n\n\nshort$frequency = as.numeric(short$frequency)\ncategory = cut(short$frequency, breaks = c(-Inf, 3, 5, Inf), \n    labels = c('Low', 'Medium', 'High'),\n    right = TRUE)\ntable(category)\n\ncategory\n   Low Medium   High \n   285    182     33 \n\n\ncalculate the percentage\n\ntable(category)*100/500\n\ncategory\n   Low Medium   High \n  57.0   36.4    6.6"
  },
  {
    "objectID": "courses/data-analysis.html",
    "href": "courses/data-analysis.html",
    "title": "Data Analysis Using R",
    "section": "",
    "text": "This page briefly introduces with R and guides you through basic to advanced level of data analysis and visualizations. It covers data set creation, manipulation, saving, and opening. Implementation of descriptive statistics (e.g. mean, standard deviation, standard error, frequency and percentage distribution) and inferential statistics (e.g. chi-square test, independent two sample t-test, paired t-test, Welch test, Wilcoxon test, one-way ANOVA, two-way ANOVA, three-way ANOVA, post-hoc analysis with Tukey and Bonferroni adjustment, Kruskal Wallis test, Pearson and Spearman correlation, and multiple regression with detecting and solving multicollinearity, heteroskedasticity and endogeneity problems). Along with these analysis, related visualization techniques are explained in this manual. It is strongly advised to practice the R codes mentioned in this manual while reading it.\n\n\nR is usually known as an open source data analysis tool though it can do much more than just data analysis, such as website design, article writing, thesis preparation, book compilation, and web applet creation. R is a programming language that uses our personal computer resources to translate R programming language into useful outputs. Installation of R is very easy. However, the functionality of R is popularly enhanced by RStudio. Therefore, you need to download and install both R and RStudio for completing this manual smoothly. You can download R and R studio from https://posit.co/download/rstudio-desktop/. It should be noted that RStudio is a software that enables us to write and execute R script. You can also integrate R with Jupyter Notebook in visual Studio Code. After writing the codes and markdown in Jupyter Notebook, export as HTML, open in RStudio, publish in RPubs. Alternatively, you can convert .ipynb (jupyter notebook) to .Rmd (R Makrdown) in R using rmarkdown::convert_ipynb(input, output) function.\nIn jupyter lab or notebook, you may need additional installation to activate co-pilot (pip install notebook-intelligence in command prompt terminal). Details are here. Custom shortcut can also be added by editing the .json file. Details are here. You need to add the following codes in the User Preference Pane. Be careful for not messing the original file. Paste this codes before the last curly bracket and put comma after the ] (square bracket) of the previous line.\n{\n    \"shortcuts\": [\n         {\n            \"command\": \"notebook:replace-selection\",\n            \"selector\": \".jp-Notebook\",\n            \"keys\": [\"Ctrl Shift M\"],\n            \"args\": {\"text\": '%&gt;% '}\n        }\n    ]\n}\nThis course has 30 hours of active face-to-face classroom activities and 50 hours of independent study, homework, assignment and presentation.\nR is widely used for data science for several reasons, such as:\n\nIt is a free software.\nYou can do whatever you want to do regarding data analysis.\nIt’s publication ready visualization power is unique.\nProgram script can be saved, shared, and reuse easily for further reference.\nHighly dedicated, motivated and enthusiastic user forum and helpful online free learning materials provide confidence among users.\n\n\n\n\n\nPresentation\n\n\n\nData sets\n\nDownload CSV Dataset\nDownload txt Dataset\nDownload xlsx Dataset\nDownload Full Excel Dataset\n\n\nPlease double check and ensure your datasets and R script/markdown files are in the same folder/directory. Set the working the directory to the folder containing the script and dataset.\n\n\n\n\n1\n\n\n\nR – Installation of R and RStudio, setting the working directory [YouTube]\n\n\n\n\n\n\n\n\n\n2\n\nR projects, scripts, objects, variables, dataset, list [Supplements] |\n\n\n\n3\n\nR markdown, Basic functions, Package (tidyverse) installation, Visualization using ggplot() [YouTube]\n\n\n\n4\n\nDataset manipulation, Pareto chart, Chi-square test [YouTube] [Supplements]\n\n\n\n5\n\nDescriptive statistics and chi-square test revisit [YouTube]\n\n\n\n6\n\nCLT, Standard error, Confidence interval\n\n\n\n7\n\nOne sample t-test, z-test\n\n\n\n8\n\nTwo sample t-test (independent and paired)\n\n\n\n9\n\nMann-Whitney test (independent) and Wilcoxon test (dependent)\n\n\n\n10\n\nANOVA (one-way, two-way, three-way for CRD, RCBD and Factorial design)\n\n\n\n11\n\nANOVA (mixed effects for split-plot design) [Supplement]\n\n\n\n12\n\nResidual and Post-hoc analysis\n\n\n\n13\n\nKruskal-Wallis test\n\n\n\n14\n\nCorrelation (Spearman and Pearson)\n\n\n\n15\n\nLinear simple and multiple regression\n\n\n\n16\n\nLogistic regression\n\n\n\n\n\n\n\n\nClass test 1: Lesson 1-6: Bring your calculator for practical task\nClass test 2: Lesson 7-10: Bring your calculator for practical task\nAssignment: Lesson 11-16: Homework/group project, presentation\nFinal exam: Lesson 1-16: Concept, selection of test, requirement of assumptions, cleaning the outputs of analysis for report, writing interpretation of given analysis results\nExample questions: To be discussed in the makeup class.\n\n\n\n\n\nBasics of R Programming Language for Data Wrangling and Analysis – [Class notes, will be supplied]\nYouTube video links may be accessed from the course contents.\nWebsite for learning: https://www.geeksforgeeks.org/r-tutorial/?ref=outind\nLearning Statistics with R – Danielle Navarro [Datasets]\nR Graphics Cookbook – Winston Chang [Printed version]"
  },
  {
    "objectID": "courses/data-analysis.html#installation-of-r-and-rstudio",
    "href": "courses/data-analysis.html#installation-of-r-and-rstudio",
    "title": "Data Analysis Using R",
    "section": "",
    "text": "R is usually known as an open source data analysis tool though it can do much more than just data analysis, such as website design, article writing, thesis preparation, book compilation, and web applet creation. R is a programming language that uses our personal computer resources to translate R programming language into useful outputs. Installation of R is very easy. However, the functionality of R is popularly enhanced by RStudio. Therefore, you need to download and install both R and RStudio for completing this manual smoothly. You can download R and R studio from https://posit.co/download/rstudio-desktop/. It should be noted that RStudio is a software that enables us to write and execute R script. You can also integrate R with Jupyter Notebook in visual Studio Code. After writing the codes and markdown in Jupyter Notebook, export as HTML, open in RStudio, publish in RPubs. Alternatively, you can convert .ipynb (jupyter notebook) to .Rmd (R Makrdown) in R using rmarkdown::convert_ipynb(input, output) function.\nIn jupyter lab or notebook, you may need additional installation to activate co-pilot (pip install notebook-intelligence in command prompt terminal). Details are here. Custom shortcut can also be added by editing the .json file. Details are here. You need to add the following codes in the User Preference Pane. Be careful for not messing the original file. Paste this codes before the last curly bracket and put comma after the ] (square bracket) of the previous line.\n{\n    \"shortcuts\": [\n         {\n            \"command\": \"notebook:replace-selection\",\n            \"selector\": \".jp-Notebook\",\n            \"keys\": [\"Ctrl Shift M\"],\n            \"args\": {\"text\": '%&gt;% '}\n        }\n    ]\n}\nThis course has 30 hours of active face-to-face classroom activities and 50 hours of independent study, homework, assignment and presentation.\nR is widely used for data science for several reasons, such as:\n\nIt is a free software.\nYou can do whatever you want to do regarding data analysis.\nIt’s publication ready visualization power is unique.\nProgram script can be saved, shared, and reuse easily for further reference.\nHighly dedicated, motivated and enthusiastic user forum and helpful online free learning materials provide confidence among users."
  },
  {
    "objectID": "courses/data-analysis.html#course-contents",
    "href": "courses/data-analysis.html#course-contents",
    "title": "Data Analysis Using R",
    "section": "",
    "text": "Presentation\n\n\n\nData sets\n\nDownload CSV Dataset\nDownload txt Dataset\nDownload xlsx Dataset\nDownload Full Excel Dataset\n\n\nPlease double check and ensure your datasets and R script/markdown files are in the same folder/directory. Set the working the directory to the folder containing the script and dataset.\n\n\n\n\n1\n\n\n\nR – Installation of R and RStudio, setting the working directory [YouTube]\n\n\n\n\n\n\n\n\n\n2\n\nR projects, scripts, objects, variables, dataset, list [Supplements] |\n\n\n\n3\n\nR markdown, Basic functions, Package (tidyverse) installation, Visualization using ggplot() [YouTube]\n\n\n\n4\n\nDataset manipulation, Pareto chart, Chi-square test [YouTube] [Supplements]\n\n\n\n5\n\nDescriptive statistics and chi-square test revisit [YouTube]\n\n\n\n6\n\nCLT, Standard error, Confidence interval\n\n\n\n7\n\nOne sample t-test, z-test\n\n\n\n8\n\nTwo sample t-test (independent and paired)\n\n\n\n9\n\nMann-Whitney test (independent) and Wilcoxon test (dependent)\n\n\n\n10\n\nANOVA (one-way, two-way, three-way for CRD, RCBD and Factorial design)\n\n\n\n11\n\nANOVA (mixed effects for split-plot design) [Supplement]\n\n\n\n12\n\nResidual and Post-hoc analysis\n\n\n\n13\n\nKruskal-Wallis test\n\n\n\n14\n\nCorrelation (Spearman and Pearson)\n\n\n\n15\n\nLinear simple and multiple regression\n\n\n\n16\n\nLogistic regression"
  },
  {
    "objectID": "courses/data-analysis.html#assessment",
    "href": "courses/data-analysis.html#assessment",
    "title": "Data Analysis Using R",
    "section": "",
    "text": "Class test 1: Lesson 1-6: Bring your calculator for practical task\nClass test 2: Lesson 7-10: Bring your calculator for practical task\nAssignment: Lesson 11-16: Homework/group project, presentation\nFinal exam: Lesson 1-16: Concept, selection of test, requirement of assumptions, cleaning the outputs of analysis for report, writing interpretation of given analysis results\nExample questions: To be discussed in the makeup class."
  },
  {
    "objectID": "courses/data-analysis.html#resources",
    "href": "courses/data-analysis.html#resources",
    "title": "Data Analysis Using R",
    "section": "",
    "text": "Basics of R Programming Language for Data Wrangling and Analysis – [Class notes, will be supplied]\nYouTube video links may be accessed from the course contents.\nWebsite for learning: https://www.geeksforgeeks.org/r-tutorial/?ref=outind\nLearning Statistics with R – Danielle Navarro [Datasets]\nR Graphics Cookbook – Winston Chang [Printed version]"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Python or R is an open-source programming language. Python is one of the most widely used general programming language and R is one of the most widely used programming language for research and data analysis.\nCode sharing, customizing, reusability of codes, and literally endless possibility of expansion of data science ideas are some of the benefits of command-driven programs over menu-driven programs.\nR seems to know anything that a data analyst needs to analyze. The code is highly customizable. One problem can be solved in different ways. Output formatting can be styled using tidyverse and kableExtra. However, I think R has attracted many data analyst for its amazing publication ready data-based figures usually done with ggplot2.\n\n\n\nInstall R Download R\nInstall Python Download Python\nInstall pip (package installer for python) using cmd (command prompt): python -m pip install --upgrade pip\nFor python3, update pip if needed: python3 -m pip install --upgrade pip\nInstall Jupyter using cmd: python -m pip install jupyter\nYou can also install Jupyter Lab using cmd: python -m pip install jupyterlab\nYou can launch Jupyter Notebook or Lab using cmd: jupyter notebook or jupyter lab\n\n\n\n\n\nIn cmd: jupyter notebook\n\n\n\n\n\nInstall VS Code Download VS Code\nInstall R extension in VS code: R Extension for Visual Studio Code (REditorSupport)\nInstall radian in cmd for syntax highlighting: pip3 install -U radian\nOpen R from Start Menu\nInstall packages in R (not RStudio) without compilation: install.packages(c('languageserver', 'httpgd', 'repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))\nInstall R kernel for jupyter in R (not RStudio): devtools::install_github('IRkernel/IRkernel')\nLink R kernel to jupyter notebook in R (not RStudio): IRkernel::installspec(user = FALSE)\nOpen a file in VS code with .ipynb extension and select kernel (jupyer kernel &gt; R kernel)\n\nNow your jupyter notebook is ready for both python and R. You can also create and work with R file with .R extension in vs code. At the end, you can export the .ipynb (jupyter notebook) file as html by pressing ctrl+shift+P and search for ‘jupyter: export to html’. Open this html file in RStudio to publish. In addition, you can read some tips on working with markdown in Markdown cheat sheet."
  },
  {
    "objectID": "blogs.html#installation-of-r-and-integration-with-jupyter-notebook-in-visual-studio-code",
    "href": "blogs.html#installation-of-r-and-integration-with-jupyter-notebook-in-visual-studio-code",
    "title": "Blog Posts",
    "section": "",
    "text": "Install R Download R\nInstall Python Download Python\nInstall pip (package installer for python) using cmd (command prompt): python -m pip install --upgrade pip\nFor python3, update pip if needed: python3 -m pip install --upgrade pip\nInstall Jupyter using cmd: python -m pip install jupyter\nYou can also install Jupyter Lab using cmd: python -m pip install jupyterlab\nYou can launch Jupyter Notebook or Lab using cmd: jupyter notebook or jupyter lab"
  },
  {
    "objectID": "blogs.html#run-jupyter-notebook-in-browser",
    "href": "blogs.html#run-jupyter-notebook-in-browser",
    "title": "Blog Posts",
    "section": "",
    "text": "In cmd: jupyter notebook"
  },
  {
    "objectID": "blogs.html#run-r-with-jupyter-notebook-in-vs-code-visual-studio-code",
    "href": "blogs.html#run-r-with-jupyter-notebook-in-vs-code-visual-studio-code",
    "title": "Blog Posts",
    "section": "",
    "text": "Install VS Code Download VS Code\nInstall R extension in VS code: R Extension for Visual Studio Code (REditorSupport)\nInstall radian in cmd for syntax highlighting: pip3 install -U radian\nOpen R from Start Menu\nInstall packages in R (not RStudio) without compilation: install.packages(c('languageserver', 'httpgd', 'repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ', 'devtools', 'uuid', 'digest'))\nInstall R kernel for jupyter in R (not RStudio): devtools::install_github('IRkernel/IRkernel')\nLink R kernel to jupyter notebook in R (not RStudio): IRkernel::installspec(user = FALSE)\nOpen a file in VS code with .ipynb extension and select kernel (jupyer kernel &gt; R kernel)\n\nNow your jupyter notebook is ready for both python and R. You can also create and work with R file with .R extension in vs code. At the end, you can export the .ipynb (jupyter notebook) file as html by pressing ctrl+shift+P and search for ‘jupyter: export to html’. Open this html file in RStudio to publish. In addition, you can read some tips on working with markdown in Markdown cheat sheet."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "📞 Phone: +880 1891 565856\n📧 Email: kamrulext@pstu.ac.bd\n🌐 Website: www.ruenresearch.com\n📍Postal Address:\nDepartment of Agricultural Extension and Rural Development\nPatuakhali Science and Technology University\nDumki, Patuakhali 8660, Bangladesh"
  },
  {
    "objectID": "courses/machine-learning.html",
    "href": "courses/machine-learning.html",
    "title": "Machine Learning Using R",
    "section": "",
    "text": "This course is an advanced course designed by the integration of Research Methodology and Applied Statistics Using R. Complete the courses on your own to understand this machine learning course to a greater extent.\n\n\n\n\n\n\nHypothesis is a scientific guess which has not tested yet. Hypothesis test is done to make decision on the nature of a variable or statistics. Before performing a test we need to make a null hypothesis (H0).\nExample:\n\nH0: Two means are equal (t-test)\nH0: All means are equal (ANOVA)\nH0: Two variances are equal (Levene test)\nH0: Correlation coefficient = 0\nH0: Regression coefficient = 0\nH0: No association between the variables (chi-square test)\nH0: No effects/relationships (correlation, regression, ANOVA)\nH0: Data is normally distributed (Shapiro test)\n\nLook at this applet https://kamrulext.shinyapps.io/p-values/\n\np-values indicate evidence to support the H0.\np-value shows level of error (type I: true negative) in repeated trials.\nFYI, type II error = false positive (fails to reject a false H0)\np-values remain in the rejection area.\np-value is the area under the curve left side of the z-value.\nEach of the p-values correspond to a z-value.\nZ-value =&gt; quartile value, variable value, observed value, x-axis value, test statistics (t, r, F, beta).\nH0 is born to be rejected, so, we can never accept it. We can only reject or not reject it.\nSo, in a simple sense, small p-value (&lt;= 0.05) means insufficient evidence to support the H0 (rejected).\nLarge p-value (&gt;0.05) means sufficient evidence to support the H0 (cannot be rejected).\nHere is the twist: H0 rejected =&gt; significant relationship or effect because H0 was saying no relationship\nFurther twist: Test statistic (t, r, F, beta) value increases p-value decreases.\nHigher test statistic gives small p-values =&gt; significant result.\nTest statistic equivalent to (Coefficient)/(Standard error)\n\n\n\n\n\nDownload CSV Dataset\nDownload txt Dataset\nDownload xlsx Dataset\n\n\n\n\nLesson\nTopic\nDuration\nLinks\n\n\n\n\n1\n\n1 hr\nInstruction, YouTube\n\n\n2\n\n3 hr 30 min\nYouTube, R Codes Used in the Class\n\n\n3\n\n3 hr 30 min\nR Codes Used in the Class\n\n\n4\n\n3 hr\nR Codes Used in the Class\n\n\n5\n\n2 hr\nR Codes Used in the Class\n\n\n6\n\n1 hr\nR Codes Used in the Class\n\n\n7\n\n1 hr\nR Codes Used in the Class\n\n\n8\n\n0.5 hr\nR Codes Used in the Class\n\n\n9\n\n1 hr\nR Codes Used in the Class\n\n\n10\n\n1 hr\nR Codes Used in the Class\n\n\n11\n\n0.5 hr\nR Codes Used in the Class\n\n\n12\n\n2 hr\nR Codes Used in the Class\n\n\n13\n\n3 hr\nR Codes Used in the Class\n\n\n14\n\n3 hr\nR Codes Used in the Class\n\n\n15\n\n1 hr\nR Codes Used in the Class\n\n\n16\n\n1 hr\nR Codes Used in the Class\n\n\n17\n\n1 hr\nR Codes Used in the Class\n\n\n18\n\n1 hr\nR Codes Used in the Class\n\n\n19\n\n2 hr\nR Codes Used in the Class\n\n\n20\n\n2 hr\nR Codes Used in the Class\n\n\n21\n\n2 hr\nR Codes Used in the Class\n\n\n22\n\n2 hr\nR Codes Used in the Class"
  },
  {
    "objectID": "courses/machine-learning.html#hypothesis-test",
    "href": "courses/machine-learning.html#hypothesis-test",
    "title": "Machine Learning Using R",
    "section": "",
    "text": "Hypothesis is a scientific guess which has not tested yet. Hypothesis test is done to make decision on the nature of a variable or statistics. Before performing a test we need to make a null hypothesis (H0).\nExample:\n\nH0: Two means are equal (t-test)\nH0: All means are equal (ANOVA)\nH0: Two variances are equal (Levene test)\nH0: Correlation coefficient = 0\nH0: Regression coefficient = 0\nH0: No association between the variables (chi-square test)\nH0: No effects/relationships (correlation, regression, ANOVA)\nH0: Data is normally distributed (Shapiro test)\n\nLook at this applet https://kamrulext.shinyapps.io/p-values/\n\np-values indicate evidence to support the H0.\np-value shows level of error (type I: true negative) in repeated trials.\nFYI, type II error = false positive (fails to reject a false H0)\np-values remain in the rejection area.\np-value is the area under the curve left side of the z-value.\nEach of the p-values correspond to a z-value.\nZ-value =&gt; quartile value, variable value, observed value, x-axis value, test statistics (t, r, F, beta).\nH0 is born to be rejected, so, we can never accept it. We can only reject or not reject it.\nSo, in a simple sense, small p-value (&lt;= 0.05) means insufficient evidence to support the H0 (rejected).\nLarge p-value (&gt;0.05) means sufficient evidence to support the H0 (cannot be rejected).\nHere is the twist: H0 rejected =&gt; significant relationship or effect because H0 was saying no relationship\nFurther twist: Test statistic (t, r, F, beta) value increases p-value decreases.\nHigher test statistic gives small p-values =&gt; significant result.\nTest statistic equivalent to (Coefficient)/(Standard error)"
  },
  {
    "objectID": "courses/machine-learning.html#schedule-and-data-sets",
    "href": "courses/machine-learning.html#schedule-and-data-sets",
    "title": "Machine Learning Using R",
    "section": "",
    "text": "Download CSV Dataset\nDownload txt Dataset\nDownload xlsx Dataset\n\n\n\n\nLesson\nTopic\nDuration\nLinks\n\n\n\n\n1\n\n1 hr\nInstruction, YouTube\n\n\n2\n\n3 hr 30 min\nYouTube, R Codes Used in the Class\n\n\n3\n\n3 hr 30 min\nR Codes Used in the Class\n\n\n4\n\n3 hr\nR Codes Used in the Class\n\n\n5\n\n2 hr\nR Codes Used in the Class\n\n\n6\n\n1 hr\nR Codes Used in the Class\n\n\n7\n\n1 hr\nR Codes Used in the Class\n\n\n8\n\n0.5 hr\nR Codes Used in the Class\n\n\n9\n\n1 hr\nR Codes Used in the Class\n\n\n10\n\n1 hr\nR Codes Used in the Class\n\n\n11\n\n0.5 hr\nR Codes Used in the Class\n\n\n12\n\n2 hr\nR Codes Used in the Class\n\n\n13\n\n3 hr\nR Codes Used in the Class\n\n\n14\n\n3 hr\nR Codes Used in the Class\n\n\n15\n\n1 hr\nR Codes Used in the Class\n\n\n16\n\n1 hr\nR Codes Used in the Class\n\n\n17\n\n1 hr\nR Codes Used in the Class\n\n\n18\n\n1 hr\nR Codes Used in the Class\n\n\n19\n\n2 hr\nR Codes Used in the Class\n\n\n20\n\n2 hr\nR Codes Used in the Class\n\n\n21\n\n2 hr\nR Codes Used in the Class\n\n\n22\n\n2 hr\nR Codes Used in the Class"
  },
  {
    "objectID": "custom-panel.html",
    "href": "custom-panel.html",
    "title": "Custom Sidebar Panel",
    "section": "",
    "text": "Extra Message\n📢 “This is a dynamic sidebar panel!”\n\n🔗 Important Link\n\n📅 Latest update: r Sys.Date()\n\n🎨 Custom HTML/CSS allowed too."
  },
  {
    "objectID": "privacy-policy.html",
    "href": "privacy-policy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Last updated: September 10, 2024\nThis Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\nWe use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy.\n\n\n\n\nThe words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\n\n\n\nFor the purposes of this Privacy Policy:\n\nAccount means a unique account created for You to access our Service or parts of our Service.\nAffiliate means an entity that controls, is controlled by or is under common control with a party, where “control” means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.\nCompany (referred to as either “the Company”, “We”, “Us” or “Our” in this Agreement) refers to RUEN Research.\nCookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.\nCountry refers to: Bangladesh\nDevice means any device that can access the Service such as a computer, a cellphone or a digital tablet.\nPersonal Data is any information that relates to an identified or identifiable individual.\nService refers to the Website.\nService Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.\nUsage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).\nWebsite refers to RUEN Research, accessible from https://ruenresearch.com/\nYou means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.\n\n\n\n\n\n\n\n\n\nWhile using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\n\nEmail address\nFirst name and last name\nPhone number\nAddress, State, Province, ZIP/Postal code, City\nUsage Data\n\n\n\n\nUsage Data is collected automatically when using the Service.\nUsage Data may include information such as Your Device’s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\n\n\n\n\nWe use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\n\nCookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.\nWeb Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).\n\nCookies can be “Persistent” or “Session” Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. You can learn more about cookies on TermsFeed website article.\nWe use both Session and Persistent Cookies for the purposes set out below:\n\nNecessary / Essential Cookies\nType: Session Cookies\nAdministered by: Us\nPurpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\nCookies Policy / Notice Acceptance Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies identify if users have accepted the use of cookies on the Website.\nFunctionality Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\n\nFor more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\n\n\n\nThe Company may use Personal Data for the following purposes:\n\nTo provide and maintain our Service, including to monitor the usage of our Service.\nTo manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.\nFor the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.\nTo contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application’s push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.\nTo provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.\nTo manage Your requests: To attend and manage Your requests to Us.\nFor business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.\nFor other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.\n\nWe may share Your personal information in the following situations:\n\nWith Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.\nFor business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.\nWith Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.\nWith business partners: We may share Your information with Our business partners to offer You certain products, services or promotions.\nWith other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.\nWith Your consent: We may disclose Your personal information for any other purpose with Your consent.\n\n\n\n\nThe Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\n\n\n\nYour information, including Personal Data, is processed at the Company’s operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\nYour consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\nThe Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\n\n\n\nYou have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.\nOur Service may give You the ability to delete certain information about You from within the Service.\nYou may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.\nPlease note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.\n\n\n\n\n\nIf the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\n\n\n\nUnder certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\n\n\n\nThe Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\n\nComply with a legal obligation\nProtect and defend the rights or property of the Company\nPrevent or investigate possible wrongdoing in connection with the Service\nProtect the personal safety of Users of the Service or the public\nProtect against legal liability\n\n\n\n\n\nThe security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.\n\n\n\n\nOur Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.\nIf We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent’s consent before We collect and use that information.\n\n\n\nOur Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party’s site. We strongly advise You to review the Privacy Policy of every site You visit.\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\n\n\n\nWe may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\nWe will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the “Last updated” date at the top of this Privacy Policy.\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\n\n\n\nIf you have any questions about this Privacy Policy, You can contact us: Contact"
  },
  {
    "objectID": "privacy-policy.html#interpretation-and-definitions",
    "href": "privacy-policy.html#interpretation-and-definitions",
    "title": "Privacy Policy",
    "section": "",
    "text": "The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\n\n\n\nFor the purposes of this Privacy Policy:\n\nAccount means a unique account created for You to access our Service or parts of our Service.\nAffiliate means an entity that controls, is controlled by or is under common control with a party, where “control” means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.\nCompany (referred to as either “the Company”, “We”, “Us” or “Our” in this Agreement) refers to RUEN Research.\nCookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.\nCountry refers to: Bangladesh\nDevice means any device that can access the Service such as a computer, a cellphone or a digital tablet.\nPersonal Data is any information that relates to an identified or identifiable individual.\nService refers to the Website.\nService Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.\nUsage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).\nWebsite refers to RUEN Research, accessible from https://ruenresearch.com/\nYou means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable."
  },
  {
    "objectID": "privacy-policy.html#collecting-and-using-your-personal-data",
    "href": "privacy-policy.html#collecting-and-using-your-personal-data",
    "title": "Privacy Policy",
    "section": "",
    "text": "While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\n\nEmail address\nFirst name and last name\nPhone number\nAddress, State, Province, ZIP/Postal code, City\nUsage Data\n\n\n\n\nUsage Data is collected automatically when using the Service.\nUsage Data may include information such as Your Device’s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\nWhen You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\nWe may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\n\n\n\n\nWe use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\n\nCookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.\nWeb Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).\n\nCookies can be “Persistent” or “Session” Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. You can learn more about cookies on TermsFeed website article.\nWe use both Session and Persistent Cookies for the purposes set out below:\n\nNecessary / Essential Cookies\nType: Session Cookies\nAdministered by: Us\nPurpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\nCookies Policy / Notice Acceptance Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies identify if users have accepted the use of cookies on the Website.\nFunctionality Cookies\nType: Persistent Cookies\nAdministered by: Us\nPurpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\n\nFor more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\n\n\n\nThe Company may use Personal Data for the following purposes:\n\nTo provide and maintain our Service, including to monitor the usage of our Service.\nTo manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.\nFor the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.\nTo contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application’s push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.\nTo provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.\nTo manage Your requests: To attend and manage Your requests to Us.\nFor business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.\nFor other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.\n\nWe may share Your personal information in the following situations:\n\nWith Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.\nFor business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.\nWith Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.\nWith business partners: We may share Your information with Our business partners to offer You certain products, services or promotions.\nWith other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.\nWith Your consent: We may disclose Your personal information for any other purpose with Your consent.\n\n\n\n\nThe Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\nThe Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\n\n\n\nYour information, including Personal Data, is processed at the Company’s operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\nYour consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\nThe Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\n\n\n\nYou have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.\nOur Service may give You the ability to delete certain information about You from within the Service.\nYou may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.\nPlease note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.\n\n\n\n\n\nIf the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\n\n\n\nUnder certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\n\n\n\nThe Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\n\nComply with a legal obligation\nProtect and defend the rights or property of the Company\nPrevent or investigate possible wrongdoing in connection with the Service\nProtect the personal safety of Users of the Service or the public\nProtect against legal liability\n\n\n\n\n\nThe security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security."
  },
  {
    "objectID": "privacy-policy.html#childrens-privacy",
    "href": "privacy-policy.html#childrens-privacy",
    "title": "Privacy Policy",
    "section": "",
    "text": "Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.\nIf We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent’s consent before We collect and use that information."
  },
  {
    "objectID": "privacy-policy.html#links-to-other-websites",
    "href": "privacy-policy.html#links-to-other-websites",
    "title": "Privacy Policy",
    "section": "",
    "text": "Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party’s site. We strongly advise You to review the Privacy Policy of every site You visit.\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services."
  },
  {
    "objectID": "privacy-policy.html#changes-to-this-privacy-policy",
    "href": "privacy-policy.html#changes-to-this-privacy-policy",
    "title": "Privacy Policy",
    "section": "",
    "text": "We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\nWe will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the “Last updated” date at the top of this Privacy Policy.\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page."
  },
  {
    "objectID": "privacy-policy.html#contact-us",
    "href": "privacy-policy.html#contact-us",
    "title": "Privacy Policy",
    "section": "",
    "text": "If you have any questions about this Privacy Policy, You can contact us: Contact"
  }
]